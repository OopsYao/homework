\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}

    \problem
    \begin{question}
        Let us recall that, we say a function $f(t)$, $t\in(a,b)$ is
        of bounded variation if there exists a finite number $C$ such
        that for any partition $\pi_n=\{t_i\}^{n}_{i=0}$ of $(a,b)$
        \[\sum^{n-1}_{i=0}|f(t_{i+1})-f(t_i)|<C.\]
        Prove that a Wiener process (almost surely) does not have a
        bounded variation.  Hint: argue by contradiction.  Suppose
        that it has a bounded variation, then you can show that its
        quadratic variation converges to zero almost surely, which is impossible.
    \end{question}
    \begin{proof}
        We argue by contradiction\sidenote{The opposite of statement
        that a.s. does not have a bounded variation should be that
        the probability of having a bounded variation is not zero.
        But here we assumes the probability is 1, i.e., a.s.
        having a bounded variation, which is not the same thing.
        And I believe the latter is the convention when we talk about
        the opposite of a.s. not.}. If Wiener process
        $W_t$ a.s. has a bounded variation, i.e.,
        for a.s. $\omega\in\Omega$,
        \[\sum_{i=0}^{n-1}
        |W_{t_{i+1}}(\omega)-W_{t_i}(\omega)|<C(\omega)\]
        it follows that the partial sum of quadratic variation
        satisfies
        \[I_N=\sum_{i=0}^{n-1}
        |W_{t_{i+1}}(\omega)-W_{t_i}(\omega)|^2\leq
        \tau\sum_{i=0}^{n-1}|W_{t_{i+1}}(\omega)-W_{t_i}(\omega)|
        <\tau C(\omega)\]
        where $\tau=\max_i|t_{i+1}-t_i|$ is the mesh of the
        partition.
        Then we send $\tau\to 0^+$ and obtain
        \[\text{a.s.}\quad I_N\to 0\]
        i.e., $[W_t,W_t]=0$ in a.s. sense, which implies
        convergence in probability
        \[\text{s.t.}\quad I_N\to 0\]
        
        And we know that
        \[\text{m.s.}\quad I_N\to T\]
        as the quadratic variation is $T$ on $[0,T]$,
        which yields convergence in probability
        \[\text{s.t.}\quad I_N\to T\]
        as well. And this leads a contradiction because of the
        uniqueness of limit.
    \end{proof}

    \skipproblem

    \problem
    \begin{question}
        We know that $(dW_t)^2=dt$ is (and also should be) understood as the differential form of the quadratic variation $[W_t,W_t]|_T=T$, or precisely the mean square limit as
        \[m.s.-\lim_{n\rightarrow \infty} \sum^{n-1}_{k=0}(W_{t_{k+1}}-W_{t_k})^2=T.\]
        Similarly one should understand $dtdW_t=0$ in previous HW as
        \[m.s.-\lim_{n\rightarrow \infty} \sum^{n-1}_{k=0}(t_{k+1}-t_{k})(W_{t_{k+1}}-W_{t_k})=0.\]

        (1).  How do you understand the following differential identity
        \[d(W_t^3)=3W_t^2dW_t+3W_tdt.\]
        Though you do not that why this holds.

        (2).  Prove your claim in (1).
    \end{question}
    \begin{subproblem}[(\arabic*).]
        \item
        It should be understood as for any arbitrary $T>0$,
        \[\int_0^T\diff(W_t^3)=3\left(\int_0^TW_t^2\diff W_t
        +\int_0^TW_t\diff t\right)\] 
        or for any partition of $[0,T]$,
        \begin{multline*}
            \text{m.s.-}\lim_{\tau\to 0^+}
        \sum_{i=0}^{N-1}
        W_{t_{i+1}}^3-W^3_{t_i}
        =\text{m.s.-}\lim_{\tau\to 0^+}
        \sum_{i=0}^{N-1}
        3W_{t_i}^2(W_{t_{i+1}}-W_{t_i})\\
        +\text{m.s.-}\lim_{\tau\to0^+}
        \sum_{i=0}^{N-1}3W_{t^*}(t_{i+1}-t_i)
        \end{multline*}
        where $t^*_i$ of the Riemann partial sum is arbitrary
        in $[t_i,t_{i+1}]$.

        \item
        \begin{proof}
            We know that
            \[\begin{aligned}
                \diff (W_t^2)&=2W_t\diff W_t+(\diff W_t)^2\\
                &=2W_t\diff W_t+\diff t
            \end{aligned}\]
            hence
            \[\begin{aligned}
                \diff(W_t^3)&=\diff(W_t^2\cdot W_t)\\
                &=W_t^2\diff W_t+W_t\diff(W_t^2)+\diff(W_t^2)\cdot\diff W_t\\
                &=W_t^2\diff W_t
                +2W_t^2\diff W_t+W_t(\diff W_t)^2
                +2W_t(\diff W_t)^2+(\diff W_t)^3\\
                &=3W_t^2\diff W_t+3W_t(\diff W_t)^2+(\diff W_t)^3
            \end{aligned}\]
            Since
            \[\diff(W^2_t)=\diff t\]
            then we have that
            \[\begin{aligned}
                \diff(W_t^3)=3W_t^2\diff W_t+3W_t\diff t+\diff t\cdot\diff W_t
            \end{aligned}\]
            And in our previous homework we know that
            \[\diff t\cdot\diff W_t=0\]
            Therefore,
            \[\diff(W_t^3)=3W_t^2\diff W_t+3W_t\diff t\]
            \end{proof}
    \end{subproblem}

    \problem
    \begin{question}
        Some textbooks define the Stratonovich integral as
        \[\int_0^t f(s,W_s) \circ dW_s=m.s.-\lim_{n\rightarrow \infty}
        \sum_{i=0}^{n-1}\frac{f(t_i,W_{t_i})+f(t_{i+1},W_{t_{i+1}})}{2}(W_{t_{i+1}}-W_{t_i}),\]
        i.e., choosing the mean value of $f(s,W_s)$ at the end points,
        instead of the value at the middle point.  For $f(s,W_s)=W_s$,
        in this setting, we can easily see that
        \[\int_0^t  W_s \circ dW_s= m.s.-\lim_{n\rightarrow \infty}\sum_{i=0}^{n-1}
        \frac{W_{t_{i+1}}+W_{t_i}}{2}(W_{t_{i+1}}-W_{t_i})
        = m.s.-\lim_{n\rightarrow \infty}\frac{1}{2}\sum_{i=0}^{n-1}(W^2_{t_{i+1}}-W^2_{t_i})
        =\frac{W_t^2}{2}.\]
        What if you choose the value of
        $\lambda f(t_i,W_{t_i})+(1-\lambda)f(t_{i+1},W_{t_{i+1}}),\lambda\in[0,1]$.
        Find the Stratonovich integral
        \[\int_0^t  W_s \circ d_\lambda W_s.\]
        Compare it with the previous general Stratonovich integral. 
    \end{question}
    Motivated by the symmetry, we denote
    \newcommand{\sumhead}{\sum_{i=0}^{n-1}}
    \newcommand{\limhead}{\lim_{n\to\infty}}
    \newcommand{\wi}{W_{t_i}}
    \newcommand{\wii}{W_{t_{i+1}}}
    \newcommand{\deltaw}{\wii-\wi}
    \[\begin{aligned}
        I_n:=\sumhead\left(\lambda\wi+(1-\lambda)\wii\right)(\deltaw)\\
        J_n:=\sumhead\left((1-\lambda)\wi+\lambda\wii\right)(\deltaw)
    \end{aligned}\]
    then it is easy to find that
    \[\begin{aligned}
        I_n+J_n&=\sumhead(\wi+\wii)(\deltaw)\\
        &=\sumhead(\wii^2-\wi^2)\\
        &=W_t^2\to W_t^2\quad\text{m.s.}\\
        I_n-J_n&=\sumhead\left((2\lambda-1)\wi+(1-2\lambda)\wii\right)
                         (\deltaw)\\
               &=(1-2\lambda)\sumhead(\deltaw)^2
               \to (1-2\lambda)t\quad\text{m.s.}
    \end{aligned}\]
    Therefore by the additivity of m.s. convergence, we have
    that
    \[\text{m.s.}\quad I_n\to\frac{W_t^2+(1-2\lambda)t}{2}
    =\frac{W_t^2}{2}+\left(\frac{1}{2}-\lambda\right)t\]
    in other words, the integral
    \[\int_0^tW_s\diamond W_s
    =\frac{W_t^2}{2}+\left(\frac{1}{2}-\lambda\right)t\]
    which is the same as choosing the value of $\lambda$-point.

    \problem
    \begin{question}
        Denote
        \[X_t:=\int_0^t W_sdW_s, \forall t>0,\]
        and then we already know that
        \[X_t=\frac{W_t^2}{2}-\frac{t}{2}\]
        hence can find its expectation and variance.

        Now, find $E(X_t)$ and Var$(X_t)$ by the its mean square limit property,
        not using the explicit formula above.
    \end{question}
    \label{var and e}
    \newcommand{\wj}{W_{t_j}}
    \newcommand{\wjj}{W_{t_{j+1}}}
    \newcommand{\deltawj}{\wjj-\wj}
    Denote
    \[I_n:=\sumhead\wi(\deltaw)\]
    where $0=t_0<t_1<\cdots<t_n=t$ is a partition of $[0,t]$,
    then we know that
    \[\text{m.s.}\quad I_n\to X_t:=\int_0^tW_s\diff W_s
    \quad(n\to\infty)\]
    i.e.,
    \[E(|I_n-X_t|^2)\to 0\]
    hence\sidenote{
        \label{side:norm limit}
        The first one can be proved by Jensen's inequality,
        \[(E(I_n-X_t))^2\leq E(|I_n-X_t|^2)\]
        and Minkovski inequality yields the second one
        as
        \[\begin{aligned}
            \sqrt{E(I_n^2)}&\leq\sqrt{E(|I_n-X_t|^2)}+\sqrt{E(X_t^2)}\\
            \sqrt{E(X_t)}&\leq\sqrt{E(|I_n-X_t|^2)}+\sqrt{E(I_n^2)}\\
        \end{aligned}\]
        which implies
        \[\left|\sqrt{E(I_n^2)}-\sqrt{E(X_t^2)}\right|
        \leq\sqrt{E(|I_n-X_t|^2)}\]
        thus
        \[\sqrt{E(I_n^2)}\to\sqrt{E(X_t^2)}\]
    }
    \[\begin{aligned}
        E(I_n)&\to E(X_t)\\
        E(|I_n|^2)&\to E(X_t^2)
    \end{aligned}\]

    And it easy to see that
    \[E(I_n)=\sumhead E(\wi)\cdot E(\deltaw)=0\]
    thus
    \[E(X_t)=0\]
    As for $I_n^2$,
    \[I_n^2=\sumhead\wi^2(\deltaw)^2
    +\sum_{i<j}\wi\wj(\deltaw)(\deltawj)\]
    do note that $\deltawj$ in the last term is independent
    of $\wi\wj(\deltaw)$ when $i<j$, as the latter is before
    time $t_j$, hence the expectation can be self cancelled
    as $E(\deltawj)=0$,
    \[\begin{aligned}
        E(I_n^2)
        &=\begin{aligned}[t]
         &\sumhead E(\wi^2)\cdot E(|\deltaw|^2)\\
         &+\sum_{i<j}E\left(\wi\wj(\deltaw)\right)\cdot E(\deltawj)
         \end{aligned}\\
        &=\sumhead t_i(t_{i+1}-t_i)
        \to\int_0^ts\diff s=\frac{t^2}{2}
    \end{aligned}\]
    where the limit is obtained by the definition of Riemann integral.
    Therefore
    \[\var(X_t)=E(X_t^2)=\frac{t^2}{2}\]

    \problem
    \begin{question}
        In order to prove the following connection between It\^o and Stratonovich integrals
        \[\int_0^t f(s,W_s) \circ dW_s=\int_0^t f(s,W_s) dW_s+\frac{1}{2}\int_0^t \frac{\partial f(s,W_s)}{\partial W_s} ds,\]
        it is necessary to show that the all the Higher Order Terms converges to zero.  Denote
        \[\Delta W^{i+1}_{i}=W_{t_{i+1}}-W_{t_i}, \Delta W^{i+\frac{1}{2}}_{i}=W_{t_{i+\frac{1}{2}}}-W_{t_i}.\]
        Prove that
        \begin{enumerate}[label=(\alph*)]
        \item $m.s.-\lim_{n\rightarrow \infty}\sum_{i=0}^{n-1} \frac{\partial^2 f(t_i,W_{t_i})}{\partial t^2}(\frac{\Delta t}{2})^2\Delta W^{i+1}_{i}=0$;
        \item $m.s.-\lim_{n\rightarrow \infty}\sum_{i=0}^{n-1} \frac{\partial^2 f(t_i,W_{t_i})}{\partial t\partial W_t}\frac{\Delta t}{2}(\Delta W^{i+\frac{1}{2}}_{i}) \Delta W^{i+1}_{i}=0$;
        \item $m.s.-\lim_{n\rightarrow \infty}\sum_{i=0}^{n-1} \frac{\partial^2 f(t_i,W_{t_i})}{\partial W^2_t}\big(\Delta W^{i+\frac{1}{2}}_{i}\big)^2\Delta W^{i+1}_{i}=0$;
        \item  $m.s.-\lim_{n\rightarrow \infty}\sum_{i=0}^{n-1} \frac{\partial^{k+l} f(t_i,W_{t_i})}{\partial^k t \partial^l W_t}(\frac{\Delta t}{2})^k\big(\Delta W^{i+\frac{1}{2}}_{i}\big)^l\Delta W^{i+1}_{i}=0$, $\forall k,l\geq 2$
        ---this is optional and only for strongly motivated students.
        \end{enumerate}
    \end{question}
    \newcommand{\ptt}[1]{%
        \frac{\partial^2f(t_{#1},W_{t_{#1}})}{\partial t^2}
    }
    \newcommand{\dw}[1]{\Delta W_{#1}^{#1+1}}
    \newcommand{\dws}[1]{\Delta W_{#1}^{#1+\frac{1}{2}}}
    \begin{subproblem}[(\alph*)]
        \item
        \begin{proof}
            Denote
            \[I_n=\sum_{i=0}^{n-1}\ptt i\left(\frac{\Delta t}{2}\right)^2
            \dw i\]
            then all we need is to show that
            \[E(I_n^2)\to 0\]
            and
            \begin{multline}
                \label{eq: In square of a}
                I_n^2=
                \left(\frac{\Delta t}{2}\right)^4
                \left\{
                \sum_{i=0}^{n-1}\left(\ptt i\right)^2
                \left(\dw i\right)^2\right.\\
                \left.+\sum_{i<j}\ptt i\ptt j\dw i\dw j
                \right\}
            \end{multline}
            Note that in the last term $\dw j$ is only relavent
            to the filtration from $t_j$ to $t_{j+1}$ while
            others are before $t_j$, hence
            \[\begin{aligned}
             &E\left(\ptt i\ptt j\dw i\dw j\right)\\
            =&E\left(\ptt i\ptt j\dw i\right)\cdot
            E\left(\dw j\right)\\
            =&0\\
            \end{aligned}\]
            as $E\left(\dw j\right)=0$.

            For the same reason, in the first term of \cref{eq: In square of a},
            $\dw i$ is independent of $\ptt i$, thus
            \[\begin{aligned}
                &E\left(\left(\ptt i\right)^2\left(\dw i\right)^2\right)\\
                =&E\left[\left(\ptt i\right)^2\right]
                 \cdot E\left[\left(\dw i\right)^2\right]\\
            \end{aligned}\]
            \newcommand{\integrand}[1][]{%
                \if\relax\detokenize{#1}\relax
                E\left[
                \left(\frac{\partial^2f(t,W_t)}{\partial t^2}\right)^2
                \right]
                \else
                E\left[\left(\ptt #1\right)^2\right]
                \fi
            }
            It follows that
            \[E(I_n^2)=
            \left(\frac{\Delta t}{2}\right)^4
            \sum_{i=0}^{n-1}
            \integrand[i]\Delta t
            \]
            where the right part\sidenote{Here we assume the
            integral is finite.}
            \[\sum_{i=0}^{n-1}\integrand[i]\Delta t
            \to\int_0^T\integrand\diff t<\infty\]
            therefore
            \[E(I_n^2)\to 0\]
            as $\Delta t\to 0^+$, i.e.,
            \[\text{m.s.-}\lim_{n\to\infty}I_n=0\]
        
        \end{proof}

        \item
        \begin{proof}
        Since (b), (c), (d) all take the form of
        \newcommand{\gt}[1]{g(#1,W_{#1})}
        \newcommand{\increment}{\left(\frac{\Delta t}{2}\right)}
        \[\gt{t_i}\increment^k\left(\dws i\right)^l\dw i\]
        where $\gt{t_i}$ equals
        \[\begin{aligned}
            \frac{\partial^2f(t_i,W_{t_i})}{\partial t\partial W_t},
            \frac{\partial^2f(t_i,W_{t_i})}{\partial W_t^2},
            \frac{\partial^{k+l}f(t_i,W_{t_i})}{\partial^kt\partial^l W_t}\\
        \end{aligned}\]
        accordingly,
        then we can handle it at once.
        Denote
        \[I_n=\sum_{i=0}^{n-1}
        \increment^k M_i\]
        where
        \[M_i=
        \gt{t_i}\left(\dws i\right)^l\dw i\]
        then
        \[
            I_n^2=\increment^{2k}
            \sum_{i,j}M_iM_j
        \]

        For the term $M_iM_j$, in light of H\"older's inequality,
        we have that
        \[E(M_iM_j)\leq E(|M_iM_j|)\leq \sqrt{E(M_i^2)}\cdot\sqrt{E(M_j^2)}\]
        It follows that
        \begin{equation}
            \label{eq:EIn2<=}
            \begin{aligned}
            E(I_n^2)&\leq
            \increment^{2k}
            \sum_{i,j}\sqrt{E(M_i^2)}\cdot\sqrt{E(M_j^2)}\\
        &=\increment^{2k}\left(\sum_{i=0}^{n-1}\sqrt{E(M_i^2)}\right)^2\\
        \end{aligned}\end{equation}
        which is the key to this problem.
        Since $\gt{t_i}$ is adapted to the filtration up to time $t_i$
        of which $\dws i,\dw i$ is independent, we have that
        \begin{equation}
            \label{eq:EMi2}
            E(M_i^2)=E[(\gt{t_i})^2]\cdot
            E\left[\left(\dws i\right)^{2l}\left(\dw i\right)^2\right]
        \end{equation}
        And it is not difficult to see that\sidenote{
            Details are put in appendix: \nameref{sec:proof for expectation}.
        }
        the expectation only depends
        on the gap $\Delta t/2$ since we choose an equi-distant partition,
        i.e.,
        \[
            E\left[
            \left(\dws i\right)^{2l}
            \left(\dw i\right)^2
            \right]
            =p\left(\frac{\Delta t}{2}\right)
        \]
        where the specific form of $p(\Delta t/2)$
        we will see later.

        It follows that \cref{eq:EMi2} becomes
        \[E(M_i^2)=p\left(\frac{\Delta t}{2}\right)
        \cdot E[(\gt{t_i})^2]\]
        thus \cref{eq:EIn2<=} reads
        \[\begin{aligned}
            E(I_n^2)&\leq
            \increment^{2k}p\increment\left(\sum_{i=0}^{n-1}
            \sqrt{E[(\gt{t_i})^2]}\right)^2\\
            &=\increment^{2k-2}p\increment
            \left(\frac12\sum_{i=0}^{n-1}\sqrt{E[(\gt{t_i})^2]}\Delta t\right)^2
        \end{aligned}\]

        So if we assume that
        \[\int_0^t\sqrt{E[(\gt{s})^2]}\diff s<\infty\]
        then
        \[
            \left(\frac12\sum_{i=0}^{n-1}\sqrt{E(\gt{t_i})}\Delta t\right)^2
            \to
            \frac14\left(\int_0^T\sqrt{E(\gt{t})}\diff t\right)^2
            <\infty
        \]
        as the mesh $\Delta t\to 0^+$.
        It follows that all we need is to show
        \[\varphi(\Delta t)=\increment^{2k-2}p\increment\to 0\quad(\Delta t\to 0^+)\]

        In fact we have that (see appendix: \nameref{sec:deduction of p} for details)
        \[p\increment=(2l+2)\cdot(2l-1)!!\increment^{l+1}\]
        hence
        \[\varphi(\Delta t)
        =(2l+2)\cdot(2l-1)!!\increment^{2k+l-1}\]
        therefore
        \[2k+l>1\]
        is a sufficient condition.
        Here in problem (b) we have that
        \[k=l=1\]
        therefore
        \[\varphi(\Delta t)\to 0\]
        in sight of the assumption
        \[
            \int_0^t
            \sqrt{E\left[\left(\frac{\partial^2f(s,W_{s})}{\partial t\partial W_s}\right)^2\right]}
            \diff s
            <\infty
        \]
        thus
        \[E(I_n^2)\to 0\]
        \end{proof}

        \item
        \begin{proof}
        Since
        \[k=0,l=2\]
        then
        \[2k+l=2>1\]
        thus the proof is done on the assumption that
        \[
            \int_0^t
            \sqrt{E\left[\left(\frac{\partial^2f(s,W_{s})}{\partial W_s^2}\right)^2\right]}
            \diff s
            <\infty
        \]
        \end{proof}

        \item
        \begin{proof}
        Since
        \[k,l\geq 2\]
        then
        \[2k+l\geq 6>1\]
        thus the proof is done on the assumption that
        \[
            \int_0^t
            \sqrt{E\left[\left(\frac{\partial^{k+l}f(s,W_{s})}{\partial^k s\partial^l W_s}\right)^2\right]}
            \diff s
            <\infty
        \]
        \end{proof}


    \end{subproblem}

    \problem
    \begin{question}
        Prove that the It\^o integral $\int_0^t f(s,W_s)dW_s$ is $\mathcal F_t$--predictable, i.e.,
        \[E\Big(\int_0^t f(u,W_u)dW_u |\mathcal F_s\Big)=\int_0^s f(u,W_u)dW_u.\]
        Hint: the partition property. 
    \end{question}
    \begin{proof}
        \newcommand{\conde}[1]{E\left(\left. #1\right|\mathcal F_s\right)}
        By partition property, we have that
        \[\int_0^tf(u,W_u)\diff W_u=\int_0^sf(u,W_u)\diff W_u
        +\int_s^tf(u,W_u)\diff W_u\]
        And it is apparent that the first term is
        $\mathcal F_s$~-predictable, i.e.,
        \[\conde{\int_0^sf(u,W_u)\diff W_u}=\int_0^sf(u,W_u)\diff W_u\]
        

        In light of Fubini theorem\sidenote{Here we assume that $f(u,W_u)$
        satisfies all the necessary conditions.}, we have that
        \[\conde{\int_s^tf(u,W_u)\diff W_u}
            =\int_s^t\conde{f(u,W_u)\diff W_u}\]
        Since $f(u,W_u)$ and $\diff W_u$ are independent, we obtain
        \[\conde{f(u,W_u)\diff W_u}=\conde{f(u,W_u)}\cdot\conde{\diff W_u}
        =0\]
        as $\diff W_u$ is also independent of $\mathcal F_s$ which implies
        $\conde{\diff W_u}=E(\diff W_u)=0$. It follows that
        \[\conde{\int_s^tf(u,W_u)\diff W_u}=0\]

        Therefore,
        \[\begin{aligned}
        \conde{\int_0^tf(u,W_u)\diff W_u}
        &=\conde{\int_0^sf(u,W_u)\diff W_u}\\
        &=\int_0^sf(u,W_u)\diff W_u
        \end{aligned}\]
    \end{proof}


    \problem
    \begin{question}
        Let $\{X_n(t)\}$ be a sequence of square integrable
        and adapted/non-anticipating stochastic processes and
        it converges to $X(t)$ in the following sense
        \[\lim_{n\rightarrow \infty}E\Big(\int_a^b |X_n-X|^2 dt\Big)=0.\]
        Prove that in mean square as $n\rightarrow\infty$
        \[\int_a^b X_n(t)dW_t\rightarrow \int_a^b X(t)dW_t.\]
        Hint: Use Isometry identity. 
    \end{question}
    \begin{proof}
        By isometry identity
        \[\begin{aligned}
            E\left(\int_a^b|X_n-X|^2\diff t\right)
            &=E\left(\left|\int_a^b(X_n-X)\diff W_t\right|^2\right)\\
            &=E\left(\left|
            \int_a^bX_n\diff W_t-\int_a^bX\diff W_t
            \right|^2\right)
        \end{aligned}\]
        hence
        \[\lim_{n\to\infty}E\left(
            \int_a^b|X_n-X|^2\diff t\right)=0\]
        means
        \[\lim_{n\to\infty}E\left(\left|
            \int_a^bX_n\diff W_t-\int_a^bX\diff W_t
            \right|^2\right)=0\]
        i.e.,
        \[\text{m.s.}\quad
        \int_a^bX_n\diff W_t\to\int_a^bX\diff W_t\]
    \end{proof}

    \problem
    \begin{question}
        Find
        \[E\Big(\int_0^TW_sdW_s\Big)\text{~and~}\text{Var}\Big(\int_0^TW_sdW_s\Big).\]
        Verify the Isometry property 
        \[E\Big(\Big(\int_0^TW_sdW_s\Big)^2\Big)=E\Big(\int_0^TW^2_sdW_s\Big).\]
    \end{question}
    In problem \ref{var and e} we know that
    \[\begin{aligned}
        E\left(\int_0^TW_s\diff W_s\right)&=0\\
        \var\left(\int_0^TW_s\diff W_s\right)&=\frac{t^2}{2}
    \end{aligned}\]
    hence
    \[E\left(\left(\int_0^TW_s\diff W_s\right)^2\right)=\frac{t^2}{2}\]
    On the other hand, by isometry identity,
    \[\begin{aligned}
        E\left(\left(\int_0^TW_s\diff W_s\right)^2\right)
        &=E\left(\int_0^TW_s^2\diff s\right)\\
        &=\int_0^TE(W_s^2)\diff s\\
        &=\int_0^Ts\diff s\\
        &=\frac{t^2}{2}
    \end{aligned}\]
    which coincides with result we obtained above.

    \problem
    \begin{question}
        Find the expectation and variance of the following integrals
        (if necessary you need to specify the condition on the parameters)
        \[(1).  X=\int_1^T \frac{1}{\sqrt s}dW_s; \quad \quad (2).  Y= \int_0^T s^\alpha dW_s.\]
    \end{question}
    \begin{subproblem}[(\arabic*).]
        \item
        Since It\^o integral has zero-mean, then
        we know that
        \[E(X)=0\]
        and in light of isometry identity,
        \[E(X^2)=E\left(\int_1^T\left(\frac{1}{\sqrt s}\right)^2
        \diff s\right)=\ln T\]
        therefore,
        \[\var(X)=E(X^2)=\ln T\]

        \item
        Similarly, we have that
        \[E(Y)=0\]
        and by implies identity,
        \[E(Y^2)=E\left(\int_0^Ts^{2\alpha}\diff s\right)
        =\frac{T^{2\alpha+1}}{2\alpha+1}\]
        if $\alpha>0$.

        When $\alpha\leq 0$, however,
        the problem of improper integral arises\sidenote{It is
        about whether the It\^o integral can be well defined.}.
        Recall how we define the It\^o integral.
        If there exists some random variable $Y$ s.t.
        \begin{equation}
            \label{eq:Ito def}
            E(|I_n-Y|^2)\to 0
        \end{equation}
        where
        \[I_n=\sum_{i=0}^{n-1}t_i^\alpha(\deltaw)\]
        for an arbitrary partition,
        then we say
        \[\int_0^Ts^\alpha\diff W_s:=Y\]
        And \cref{eq:Ito def} yields
        \[E(I_n^2)\to E(Y^2)\]
        as convergence in norm yields the convergence of norm\sidenote{
            See sidenote \ref{side:norm limit} for details.
        }.
        And in the deduction of isometry identity, we know that
        \[E(I_n^2)=\sum_{i=0}^{n-1}E(t_i^{2\alpha})(t_{i+1}-t_i)
        =\sum_{i=0}^{n-1}t_i^{2\alpha}\Delta t_i\]
        which is a partitial sum of Riemann integral
        \begin{equation}
            \label{eq:Riemann int}
            \int_0^Ts^{2\alpha}\diff s
        \end{equation}
        It follows that Riemann integral above exists\sidenote{
            I admit there is a fatal flaw that the sample point of
            It\^o is not arbitrary, so the existence of Riemann
            integral is not promised.

            But all I want is to
            illustrate the relationship between the existence of It\^o integral
            and square Riemann integral, and this is a good start point
            in my opinion -- as it can lead to some useful results,
            though may not so rigorous.
        }
        and
        \[E(Y^2)=\int_0^Ts^{2\alpha}\diff s\]
        That being said, the It\^o integral does not exists
        without the existence of the Riemann integral.
        In other words, a well defined It\^o integral
        $\int_0^Ts^\alpha\diff W_s$
        leads to
        \[E\left(\int_0^Ts^{2\alpha}\diff s\right)<\infty\]

        From calculus we know that \cref{eq:Riemann int} does
        not exists if $2\alpha+1\leq 0$.
        So in conclusion,
        when\sidenote{If $-1/2<\alpha\leq 0$, it is an improper
        integral indeed as the integrand $s^{\alpha}$ is unbounded on $[0,T]$.
        } $\alpha>-1/2$,
        \[\begin{aligned}
            E(Y)&=0\\
            \var(Y)&=\frac{T^{2\alpha+1}}{2\alpha+1}
        \end{aligned}\]
        when $\alpha\leq -1/2$, $Y$ is not well defined, then
        we can not talk about its expectation and variation either.
    \end{subproblem}

    \problem
    \begin{question}
        Find constant $\alpha$ such
        \[m.s.-\lim_{t\rightarrow 0}\frac{1}{t^\alpha}\int_0^t sdW_s=0\]
    \end{question}
    By isometry identity, we have that
    \[\begin{aligned}
        E\left(
            \left|\frac{\int_0^ts\diff W_s}{t^\alpha}\right|^2
        \right)&=\frac{E(|\int_0^ts\diff W_s|^2)}{t^{2\alpha}}\\
        &=\frac{E(\int_0^ts^2\diff s)}{t^{2\alpha}}\\
        &=\frac{t^{3-2\alpha}}{3}\to
        \begin{cases}
            0,&3-2\alpha>0\\
            \frac{1}{3},&3-2\alpha=0\\
            \infty,&3-2\alpha<0
        \end{cases}
    \end{aligned}\]
    hence if $\alpha<3/2$ we have that
    \[E\left(
            \left|\frac{\int_0^ts\diff W_s}{t^\alpha}\right|^2
        \right)\to 0\]
    i.e.,
    \[\text{m.s.-}\lim_{t\to0}\frac{1}{t^\alpha}\int_0^ts\diff W_s=0\]

    \problem
    \begin{question}
        Let $f(t)$ be a function integrable in $(a,b)$.  Find
        \[\text{Cov}\Big(W_t,\int_0^t f(s)dW_s\Big).\] 
    \end{question}
    It easy to see that
    \[W_t=\int_0^t\diff W_s\]
    as $\sum_{i=0}^{n-1}(\deltaw)=W_t$,
    then we know that
    \[E\left(\int_0^t\diff W_s\cdot\int_0^tf(s)\diff W_s\right)
    =E\left(\int_0^tf(s)\diff t\right)=\int_0^tf(s)\diff s\]
    as $f(t)$ is integrable in $(0,t)$.
    Since It\^o integral has zero mean, we have that
    \[\begin{aligned}
        \cov\left(\int_0^t\diff W_s,\int_0^tf(s)\diff W_s\right)
        =E\left(\int_0^t\diff W_s\cdot\int_0^tf(s)\diff W_s\right)
        =\int_0^tf(s)\diff s
    \end{aligned}\]
    i.e.,
    \[\cov\left(W_t,\int_0^tf(s)\diff W_s\right)=
    \int_0^tf(s)\diff s\]

    \problem
    \label{dZt}
    \begin{question}
        Let $Z_t$ be the integrated Brownian Motion
        \[Z_t=\int_0^t W_sds.\]
        Show that
        \[dZ_t=W_tdt.\]
        Hint:  Just use the definition of $dZ_t$ and the apply
        the intermediate value theorem and the continuity of $W_s$ as a function of $s$.
    \end{question}
    \begin{proof}
        By partition property,
        \[Z_{t+\diff t}=\int_0^{t+\diff t}W_s\diff s
        =Z_t+\int_t^{t+\diff t}W_s\diff s\]
        And in sight of intermediate value theorem, there exists some
        $\xi\in[t,t+\diff]$, s.t.
        \[\int_t^{t+\diff t}W_s\diff s
        =W_\xi\diff t\]
        Since $\diff t$ is sufficiently small and $W_t$ is continuous,
        we have that
        \[W_\xi=W_t\]
        thus by definition,
        \[\diff Z_t=Z_{t+\diff t}-Z_t=W_t\diff t\]
    \end{proof}

    \problem
    \label{pr:over dt}
    \begin{question}
        Let $A_t$ be the average of the Brownian motion over $(0,t)$
        \[A_t=\frac{1}{t}Z_t=\frac{1}{t}\int_0^t W_sds.\]
        Show that
        \[dA_t=\frac{1}{t}\Big(W_t-\frac{1}{t}Z_t\Big)dt.\]
        Hint: here and in the sequel you might want to use the fact that $W_t$
        is continuous in $t$.
    \end{question}
    \begin{proof}
        Since $W_t$ is continuous w.r.t. $t$, then the
        integral $\int_0^tW_s\diff s$ is continuous too.
        Hence $A_t$ is continuous. And by basic rules of
        differential we leart in class,
        \[\diff Z_t=\diff(tA_t)=t\diff A_t+A_t\diff t\]
        as $t$ is determinisitic.

        From problem \ref{dZt} we know that
        \[\diff Z_t=W_t\diff t\]
        therefore,
        \[W_t\diff t=t\diff A_t+A_t\diff t\]
        i.e.,
        \[\diff A_t=\frac{1}{t}(W_t-A_t)\diff t
        =\frac{1}{t}\left(W_t-\frac{1}{t}Z_t\right)\diff t\]
    \end{proof}

    \problem
    \begin{question}
        Let $G_t$ be the average of the geometric Brownian motion over $(0,t)$
        \[G_t= \frac{1}{t}\int_0^t e^{W_s}ds.\]
        Find $dG_t$.
    \end{question}
    \begin{proof}
        Similar to problem \ref{dZt},
        we obtain
        \[\diff\int_0^t\e^{W_s}\diff s=\e^{W_t}\diff t\]
        as $\e^{W_s}$ is continuous over $s$.
        Then based on the result of problem \ref{pr:over dt},
        we have that (by substitution of $\e^{W_t}$ for $W_t$)
        \[\diff G_t=\frac1t(\e^{W_t}-G_t)\diff t\]
    \end{proof}

    \problem
    \begin{question}
        Find the following differentials by definition
        \[d(e^{W_t}); d((t+W_t)^k); d\Big(\frac{1}{t^\alpha}\int_0^te^{W_s}ds\Big)\]
    \end{question}
    By definition,
    \[\begin{aligned}
        \diff\e^{W_t}&=\e^{W_{t+\diff t}}-\e^{W_t}\\
        &=\e^{W_t+\diff W_t}-\e^{W_t}\\
        &=\e^{W_t}(\e^{\diff W_t}-1)
    \end{aligned}\]
    And we have that
    \[\e^{\diff W_t}-1=\diff W_t+\frac{(\diff W_t)^2}{2!}
    +\frac{(\diff W_t)^3}{3!}+\cdots\]
    Note that $(\diff W_t)^2=\diff t$ and for higher terms
    \begin{equation}
        \label{eq:high term 0}
        (\diff W_t)^k=(\diff W_t)^{k-2}(\diff W_t)^2
        =(\diff W_t)^{k-2}\cdot\diff t=0\quad(k\geq 3)
    \end{equation}
    as $t$ is determinisitic. It follows that
    \[\e^{\diff W_t}-1=\diff W_t+\frac{\diff t}{2}\]
    thus
    \[\diff\e^{W_t}=\e^{W_t}\left(\diff W_t+\frac{\diff t}{2}\right)\]

    As for $(t+W_t)^k$, we take $t+W_t$ as a group, and denote
    \[X_t:=t+W_t\]
    then
    \[\begin{aligned}
        \diff X_t^k&=(X_t+\diff X_t)^k-X_t^k\\
        &=\begin{aligned}[t]
        &kX_t^{k-1}\diff X_t+\frac{k(k-1)}{2!}X_t^{k-2}(\diff X_t)^2\\
        &+\frac{k(k-1)(k-2)}{3!}X_t^{k-3}(\diff X_t)^3+\cdots
        \end{aligned}
    \end{aligned}\]
    as
    \begin{multline*}
        (x+\Delta x)^k-x^k=kx^{k-1}\Delta x
        +\frac{k(k-1)x^{k-2}}{2!}(\Delta x)^2\\
        +\frac{k(k-1)(k-2)x^{k-3}}{3!}(\Delta x)^3+\cdots
    \end{multline*}
    And we have the fact that
    \[(\diff X_t)^2=(\diff t)^2+(\diff W_t)^2+2\diff W_t\cdot\diff t
    =\diff t\]
    For the same reason as \cref{eq:high term 0}, terms with index
    greater than 3 is 0, thus,
    \[\begin{aligned}
        \diff X_t^k&=kX_t^{k-1}\diff X_t+\frac{k(k-1)}{2}X_t^{k-2}\diff t\\
        &=kX_t^{k-2}\left(\left(X_t+\frac{k-1}{2}\right)\diff t+X_t\diff W_t\right)
    \end{aligned}\]
    i.e.,
    \[\diff \left((t+W_t)^k\right)=
    k(t+W_t)^{k-2}
    \left(\left(t+W_t+\frac{k-1}{2}\right)\diff t
    +(t+W_t)\diff W_t\right)
    \]

    For $\int_0^t\e^{W_s}\diff s/t^\alpha$, denote
    \[A_t:=\frac{\int_0^t\e^{W_s}\diff s}{t^\alpha}\]
    then
    \[\begin{aligned}
        \diff(t^\alpha A_t)=A_t\diff t^\alpha+t^\alpha\diff A_t
        +\diff t^\alpha\cdot\diff A_t
        =\alpha t^{\alpha-1}A_t\diff t+t^\alpha\diff A_t
    \end{aligned}\]
    as
    \[\diff t^\alpha\cdot\diff A_t=0\]
    Also note that (similar to problem \ref{dZt})
    \[\diff (t^\alpha A_t)=\diff\left(\int_0^t\e^{W_s}\diff s\right)
    =\e^{W_t}\diff t\]
    hence
    \[\e^{W_t}\diff t=\alpha t^{\alpha-1}A_t\diff t+t^\alpha\diff A_t\]
    It follows that
    \[\begin{aligned}
        \diff A_t&=t^{-\alpha}\e^{W_t}\diff t-\frac{\alpha}{t}A_t\diff t\\
        &=\frac{1}{t^\alpha}\left(
            \e^{W_t}-\frac{\alpha}{t}\int_0^t\e^{W_s}\diff s
        \right)\diff t
    \end{aligned}\]

    \problem
    \begin{question}
        Let $R_t=\sqrt{X^2_t+Y^2_t}$ be the Bessel process, where $X_t$ and $Y_t$
        are two independent Brownian motions.  Find $dR_t$ and show that $R_t$
        satisfies the following stochastic differential equation
        \[dR_t=dW_t+\frac{1}{2}\frac{dt}{R_t}.\]
    \end{question}
    \begin{proof}
        By definition,
        \[\begin{aligned}
            \diff R_t&=R_{t+\diff t}-R_t\\
            &=\sqrt{(X_t+\diff t)^2+(Y_t+\diff t)^2}-R_t\\
            &=\sqrt{X_t^2+Y_t^2+(\diff X_t)^2+(\diff Y_t)^2
            +2X_t\diff X_t+2Y_t\diff Y_t}-R_t\\
            &=\sqrt{R_t^2+2(X_t\diff X_t+Y_t\diff Y_t+\diff t)}-R_t\\
            &=R_t\left(\sqrt{1+\frac{2M_t}{R_t^2}}-1\right)
        \end{aligned}\]
        where\sidenote{For convenience, assume that
        $R_t\neq 0$.}
        \[M_t=X_t\diff X_t+Y_t\diff Y_t+\diff t\]
        Then we can see that,
        \[\begin{aligned}
            M_t^2&=X_t^2(\diff X_t)^2+Y_t^2(\diff Y_t)+(\diff t)^2
            +2X_tY_t\diff X_t\diff Y_t\\
            &=R_t^2\diff t+2X_tY_t\diff X_t\diff Y_t
        \end{aligned}\]
        as $\diff X_t\diff t=\diff Y_t\diff t=0$.

        Since $X_t$ and $Y_t$ is independent, then we have that
        \[\begin{aligned}
            E\left(\Delta X_{t_i}\Delta X_{t_j}
                   \Delta Y_{t_i}\Delta Y_{t_j}\right)
            &=E\left(\Delta X_{t_i}\Delta X_{t_j}\right)\cdot
              E\left(\Delta Y_{t_i}\Delta Y_{t_j}\right)\\
            &=\begin{cases}
                (\Delta t)^2,&i=j\\
                0,&i\neq j
            \end{cases}
        \end{aligned}\]
        thus
        \[\begin{aligned}
            E\left[\left(\sum_i\Delta X_{t_i}\Delta Y_{t_i}\right)^2\right]
            &=\sum_{i,j}
            E\left(\Delta X_{t_i}\Delta X_{t_j}
                   \Delta Y_{t_i}\Delta Y_{t_j}\right)\\
            &=\sum_{i}(\Delta t)^2\\
            &=t\Delta t\to 0
        \end{aligned}\]
        therefore we can claim that
        \[\diff X_t\diff Y_t=0\]
        hence
        \begin{equation}
            \label{eq:magic equation}
            M_t^2=R_t^2\diff t
        \end{equation}
        thus higher terms $(k\geq 3)$
        \[M_t^k=R_t^2M_t^{k-3}\cdot M_t\diff t
        =R_t^2M_t^{k-3}(X_t\diff X_t\diff t+Y_t\diff Y_t\diff t+(\diff t)^2)
        =0\]

        By Taylor's expansion, we also have the fact that
        \[\sqrt{1+x}-1=\frac{1}{2}x-\frac{1}{2!\cdot 4}x^2+\frac{3}{3!\cdot 8}x^3
        +\cdots\]
        It follows that
        \[\sqrt{1+\frac{2M_t}{R_t^2}}-1
        =\frac{M_t}{R_t^2}-\frac{M_t^2}{2R_t^4}\]
        since the high order terms is proved to be 0.
        i.e.,
        \[\diff R_t=\frac{M_t}{R_t}-\frac{M_t^2}{2R_t^3}
        =\frac{M_t}{R_t}-\frac{\diff t}{2R_t}
        =\frac{X_t\diff X_t+Y_t\diff Y_t}{R_t}+\frac{\diff t}{2R_t}\]

        Finally, we claim that
        \[\frac{X_t\diff X_t+Y_t\diff Y_t}{R_t}=\diff W_t\]
        where $W_t$ is a Wiener process. To see this, note that
        the differential of quadratic variation
        \[\left(\frac{X_t\diff X_t+Y_t\diff Y_t}{R_t}\right)^2
        =\frac{R_t^2\diff t+2X_tY_t\diff X_t\diff Y_t}{R_t^2}=\diff t\]
        and for any $s<t$,
        \[E\left(\left.\frac{X_t\diff X_t}{R_t}\right|\mathcal F_s\right)
        =E\left(\left.\frac{X_t}{R_t}\right|\mathcal F_s\right)
        \cdot E(\diff X_t|\mathcal F_s)=0\]
        as $X_t/R_t$ is independent of $\diff X_t$.
        Similarly, we also have
        \[E\left(\left.\frac{Y_t\diff Y_t}{R_t}\right|\mathcal F_s\right)=0\]
        thus,
        \[E\left(\left.\frac{X_t\diff X_t+Y_t\diff Y_t}{R_t}\right|\mathcal F_s\right)=0\]
        therefore,
        \[\begin{aligned}
            E\left(\left.\int_0^t\frac{X_u\diff X_u+Y_u\diff Y_u}{R_u}\right|\mathcal F_s\right)
            &=\int_0^tE\left(\left.\frac{X_u\diff X_u+Y_u\diff Y_u}{R_u}\right|\mathcal F_s\right)\\
            &=\int_0^sE\left(\left.\frac{X_u\diff X_u+Y_u\diff Y_u}{R_u}\right|\mathcal F_s\right)\\
            &=E\left(\left.\int_0^s\frac{X_u\diff X_u+Y_u\diff Y_u}{R_u}\right|\mathcal F_s\right)\\
        \end{aligned}\]
        It follows that $(X_t\diff X_t+Y_t\diff Y_t)/R_t$ is a differential of a
        martingale, then by L\'evy characterisation of Brownian motion,
        \[\frac{X_t\diff X_t+Y_t\diff Y_t}{R_t}=\diff W_t\]
        where $W_t$ is a Wiener process, which is what we claimed.
        Therefore,
        \[\diff R_t=\diff W_t+\frac{\diff t}{2R_t}\]
    \end{proof}

    \appendix
    \section{Proof For Expectation}
    \label{sec:proof for expectation}
    \begin{proof}
        Note that $\dw i-\dws i$ is independent of $\dws i$ 
        since
        \[\begin{aligned}
            \dw i-\dws i&=W_{t_{i+1}}-W_{t_{i+\frac{1}{2}}}\\
            \dws i&=W_{t_{i+\frac{1}{2}}}-W_{t_i}
        \end{aligned}\]
        Then we have that
        \[\begin{aligned}
            &E\left[
            \left(\dws i\right)^{2l}
            \left(\dw i\right)^2
            \right]\\
            =&E\left[
            \left(\dws i\right)^{2l}
            \left(\dw i-\dws i+\dws i\right)^2
            \right]\\
            =&\begin{aligned}[t]
            &E\left[
            \left(\dws i\right)^{2l}\left(\dw i-\dws i\right)^2
            \right]\\
            &+2E\left[
            \left(\dws i\right)^{2l+1}\left(\dws i-\dw i\right)
            \right]\\
            &+E\left[\left(\dws i\right)^{2l+2}\right]\\
            \end{aligned}\\
        \end{aligned}\]
        In the equation above we have the first term
        \[\begin{aligned}
            E\left[
            \left(\dws i\right)^{2l}\left(\dw i-\dws i\right)^2
            \right]
            =\frac{\Delta t}{2}E\left[\left(\dws i\right)^{2l}\right]
        \end{aligned}\]
        and the middle term
        \[\begin{aligned}
            &E\left[\left(\dws i\right)^{2l+1}\left(\dws i-\dw i\right)\right]\\
            =&
            E\left[\left(\dws i\right)^{2l+1}\right]
            \cdot E\left(\dws i-\dw i\right)\\
            =&0\\
        \end{aligned}\]
        then we have
        \renewcommand{\theequation}{A.\arabic{equation}}
        \begin{equation}
            \label{eq:def of p}
            \begin{aligned}
            &E\left[
            \left(\dws i\right)^{2l}
            \left(\dw i\right)^2
            \right]\\
            =&\frac{\Delta t}{2}E\left[\left(\dws i\right)^{2l}\right]
            +E\left[\left(\dws i\right)^{2l+2}\right]\\
            =&p\left(\frac{\Delta t}{2}\right)
            \end{aligned}
        \end{equation}
        where $p(\Delta t/2)$ is a function of $\Delta t/2$
        as $\dws i\sim\mathcal N(0,\Delta t/2)$.
    \end{proof}

    \section[Deduction Of p(Delta t/2)]{Deduction Of $p(\Delta t/2)$}
    \label{sec:deduction of p}
    Deduction of $E(X^{2l})$ is all we need, where $X\sim\mathcal N(0,\Delta t/2)$.
    For any $l\geq 1$, we have that
    \[\begin{aligned}
        E(X^{2l})&=\int_{-\infty}^\infty
        x^{2l}\frac{\e^{-\frac{x^2}{\Delta t}}}{\sqrt{\pi\Delta t}}
        \diff x\\
        &=2\int_{0}^\infty
        x^{2l}\frac{\e^{-\frac{x^2}{\Delta t}}}{\sqrt{\pi\Delta t}}
        \diff x\\
        &=2\int_{0}^\infty
        \frac{x^{2l}}{\sqrt{\pi\Delta t}}
        \left(-\frac{2x}{\Delta t}\right)^{-1}
        \diff \e^{-\frac{x^2}{\Delta t}}\\
        &=-\frac{\Delta t}{\sqrt{\pi\Delta t}}
        \left(\left.x^{2l-1}\e^{-\frac{x^2}{\Delta t}}\right|^\infty_0
        -\int_0^\infty(2l-1)x^{2l-2}\e^{-\frac{x^2}{\Delta t}}\diff x
        \right)\\
        &=\frac{\Delta t}{2}(2l-1)\cdot
        2\int_0^\infty x^{2l-2}\frac{\e^{-\frac{x^2}{\Delta t}}}{\sqrt{\pi\Delta t}}\diff x\\
        &=\frac{\Delta t}{2}(2l-1)\cdot E(X^{2(l-1)})
    \end{aligned}\]
    It follows that
    \[E(X^{2l})=(2l-1)!!\left(\frac{\Delta t}{2}\right)^l\]
    as
    \[E(X^0)=1\]
    Therefore, in sight of \cref{eq:def of p},
    \[\begin{aligned}
        &E\left[\left(\dws i\right)^{2l}
        \left(\dw i\right)^2\right]\\
        =&(2l-1)!!\left(\frac{\Delta t}{2}\right)^{l+1}
        +(2l+1)!!\left(\frac{\Delta t}{2}\right)^{l+1}\\
        =&(2l+2)\cdot(2l-1)!!\left(\frac{\Delta t}{2}\right)^{l+1}
    \end{aligned}\]