\documentclass{homework}

\title{Homework 6}

\newcommand{\var}{\mathrm{Var}}

\begin{document}
    \maketitle    

    \problem
    \begin{proof}
        We argue by contradiction\sidenote{The opposite of statement
        that a.s. does not have a bounded variation should be that
        the probability of having a bounded variation is not zero.
        But here we assumes the probability is 1, i.e., a.s.
        having a bounded variation, which is not the same thing.
        And I believe the latter is the convention when we talk about
        the opposite of a.s. not.}. If Wiener process
        $W_t$ a.s. has a bounded variation, i.e.,
        for a.s. $\omega\in\Omega$,
        \[\sum_{i=0}^{n-1}
        |W_{t_{i+1}}(\omega)-W_{t_i}(\omega)|<C(\omega)\]
        it follows that the partial sum of quadratic variation
        satisfies
        \[I_N=\sum_{i=0}^{n-1}
        |W_{t_{i+1}}(\omega)-W_{t_i}(\omega)|^2\leq
        \tau\sum_{i=0}^{n-1}|W_{t_{i+1}}(\omega)-W_{t_i}(\omega)|
        <\tau C(\omega)\]
        where $\tau=\max_i|t_{i+1}-t_i|$ is the mesh of the
        partition.
        Then we send $\tau\to 0^+$ and obtain
        \[\text{a.s.}\quad I_N\to 0\]
        i.e., $[W_t,W_t]=0$ in a.s. sense, which implies
        convergence in probability
        \[\text{s.t.}\quad I_N\to 0\]
        
        And we know that
        \[\text{m.s.}\quad I_N\to T\]
        as the quadratic variation is $T$ on $[0,T]$,
        which yields convergence in probability
        \[\text{s.t.}\quad I_N\to T\]
        as well. And this leads a contradiction because of the
        uniqueness of limit.
    \end{proof}

    \problem
    % TODO What do you mean by NOT turn in

    \problem
    \begin{subproblem}[(\arabic*).]
        \item
        It should be understood as for any arbitrary $T>0$,
        \[\int_0^T\diff(W_t^3)=3\left(\int_0^TW_t^2\diff W_t
        +\int_0^TW_t\diff t\right)\] 
        or for any partition of $[0,T]$,
        \begin{multline*}
            \text{m.s.-}\lim_{\tau\to 0^+}
        \sum_{i=0}^{N-1}
        W_{t_{i+1}}^3-W^3_{t_i}
        =\text{m.s.-}\lim_{\tau\to 0^+}
        \sum_{i=0}^{N-1}
        3W_{t_i}^2(W_{t_{i+1}}-W_{t_i})\\
        +\text{m.s.-}\lim_{\tau\to0^+}
        \sum_{i=0}^{N-1}3W_{t^*}(t_{i+1}-t_i)
        \end{multline*}
        where $t^*_i$ of the Riemann partial sum is arbitrary
        in $[t_i,t_{i+1}]$.

        \item
        \begin{proof}
            We know that
            \[\begin{aligned}
                \diff (W_t^2)&=2W_t\diff W_t+(\diff W_t)^2\\
                &=2W_t\diff W_t+\diff t
            \end{aligned}\]
            hence
            \[\begin{aligned}
                \diff(W_t^3)&=\diff(W_t^2\cdot W_t)\\
                &=W_t^2\diff W_t+W_t\diff(W_t^2)+\diff(W_t^2)\cdot\diff W_t\\
                &=W_t^2\diff W_t
                +2W_t^2\diff W_t+W_t(\diff W_t)^2
                +2W_t(\diff W_t)^2+(\diff W_t)^3\\
                &=3W_t^2\diff W_t+3W_t(\diff W_t)^2+(\diff W_t)^3
            \end{aligned}\]
            Since
            \[\diff(W^2_t)=\diff t\]
            then we have that
            \[\begin{aligned}
                \diff(W_t^3)=3W_t^2\diff W_t+3W_t\diff t+\diff t\cdot\diff W_t
            \end{aligned}\]
            And in our previous homework we know that
            \[\diff t\cdot\diff W_t=0\]
            Therefore,
            \[\diff(W_t^3)=3W_t^2\diff W_t+3W_t\diff t\]
            \end{proof}
    \end{subproblem}

    \problem
    Motivated by the symmetry, we denote
    \newcommand{\sumhead}{\sum_{i=0}^{n-1}}
    \newcommand{\limhead}{\lim_{n\to\infty}}
    \newcommand{\wi}{W_{t_i}}
    \newcommand{\wii}{W_{t_{i+1}}}
    \newcommand{\deltaw}{\wii-\wi}
    \[\begin{aligned}
        I_n:=\sumhead\left(\lambda\wi+(1-\lambda)\wii\right)(\deltaw)\\
        J_n:=\sumhead\left((1-\lambda)\wi+\lambda\wii\right)(\deltaw)
    \end{aligned}\]
    then it is easy to find that
    \[\begin{aligned}
        I_n+J_n&=\sumhead(\wi+\wii)(\deltaw)\\
        &=\sumhead(\wii^2-\wi^2)\\
        &=W_t^2\to W_t^2\quad\text{m.s.}\\
        I_n-J_n&=\sumhead\left((2\lambda-1)\wi+(1-2\lambda)\wii\right)
                         (\deltaw)\\
               &=(1-2\lambda)\sumhead(\deltaw)^2
               \to (1-2\lambda)t\quad\text{m.s.}
    \end{aligned}\]
    Therefore by the additivity of m.s. convergence, we have
    that
    \[\text{m.s.}\quad I_n\to\frac{W_t^2+(1-2\lambda)t}{2}
    =\frac{W_t^2}{2}+\left(\frac{1}{2}-\lambda\right)t\]
    in other words, the integral
    \[\int_0^tW_s\diamond W_s
    =\frac{W_t^2}{2}+\left(\frac{1}{2}-\lambda\right)t\]
    which is the same as choosing the value of $\lambda$-point.

    \problem
    \label{var and e}
    \newcommand{\wj}{W_{t_j}}
    \newcommand{\wjj}{W_{t_{j+1}}}
    \newcommand{\deltawj}{\wjj-\wj}
    Denote
    \[I_n:=\sumhead\wi(\deltaw)\]
    where $0=t_0<t_1<\cdots<t_n=t$ is a partition of $[0,t]$,
    then we know that
    \[\text{m.s.}\quad I_n\to X_t:=\int_0^tW_s\diff W_s
    \quad(n\to\infty)\]
    i.e.,
    \[E(|I_n-X_t|^2)\to 0\]
    hence
    % TODO Detail
    \[\begin{aligned}
        E(I_n)&\to E(X_t)\\
        E(|I_n|^2)&\to E(X_t^2)
    \end{aligned}\]

    And it easy to see that
    \[E(I_n)=\sumhead E(\wi)\cdot E(\deltaw)=0\]
    thus
    \[E(X_t)=0\]
    As for $I_n^2$,
    \[I_n^2=\sumhead\wi^2(\deltaw)^2
    +\sum_{i<j}\wi\wj(\deltaw)(\deltawj)\]
    do note that $\deltawj$ in the last term is independent
    of $\wi\wj(\deltaw)$ when $i<j$, as the latter is before
    time $t_j$, hence the expectation can be self cancelled
    as $E(\deltawj)=0$,
    \[\begin{aligned}
        E(I_n^2)
        &=\begin{aligned}[t]
         &\sumhead E(\wi^2)\cdot E(|\deltaw|^2)\\
         &+\sum_{i<j}E\left(\wi\wj(\deltaw)\right)\cdot E(\deltawj)
         \end{aligned}\\
        &=\sumhead t_i(t_{i+1}-t_i)
        \to\int_0^ts\diff s=\frac{t^2}{2}
    \end{aligned}\]
    where the limit is obtained by the definition of Riemann integral.
    Therefore
    \[\var(X_t)=E(X_t^2)=\frac{t^2}{2}\]

    \problem
    % TODO sponge bob abcd

    \problem
    % TODO about filtration

    \problem
    \begin{proof}
        By isometry identity
        \[\begin{aligned}
            E\left(\int_a^b|X_n-X|^2\diff t\right)
            &=E\left(\left|\int_a^b(X_n-X)\diff W_t\right|^2\right)\\
            &=E\left(\left|
            \int_a^bX_n\diff W_t-\int_a^bX\diff W_t
            \right|^2\right)
        \end{aligned}\]
        hence
        \[\lim_{n\to\infty}E\left(
            \int_a^b|X_n-X|^2\diff t\right)=0\]
        means
        \[\lim_{n\to\infty}E\left(\left|
            \int_a^bX_n\diff W_t-\int_a^bX\diff W_t
            \right|^2\right)=0\]
        i.e.,
        \[\text{m.s.}\quad
        \int_a^bX_n\diff W_t\to\int_a^bX\diff W_t\]
    \end{proof}

    \problem
    In problem 5 at page \pageref{var and e} we know that
    \[\begin{aligned}
        E\left(\int_0^TW_s\diff W_s\right)&=0\\
        \var\left(\int_0^TW_s\diff W_s\right)&=\frac{t^2}{2}
    \end{aligned}\]
    hence
    \[E\left(\left(\int_0^TW_s\diff W_s\right)^2\right)=\frac{t^2}{2}\]
    On the other hand, by isometry identity,
    \[\begin{aligned}
        E\left(\left(\int_0^TW_s\diff W_s\right)^2\right)
        &=E\left(\int_0^TW_s^2\diff s\right)\\
        &=\int_0^TE(W_s^2)\diff s\\
        &=\int_0^Ts\diff s\\
        &=\frac{t^2}{2}
    \end{aligned}\]
    which coincides with result we obtained above.
\end{document}