\documentclass{homework}

\title{Homework 6}

\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}

\begin{document}
    \maketitle    

    \problem
    \begin{proof}
        We argue by contradiction\sidenote{The opposite of statement
        that a.s. does not have a bounded variation should be that
        the probability of having a bounded variation is not zero.
        But here we assumes the probability is 1, i.e., a.s.
        having a bounded variation, which is not the same thing.
        And I believe the latter is the convention when we talk about
        the opposite of a.s. not.}. If Wiener process
        $W_t$ a.s. has a bounded variation, i.e.,
        for a.s. $\omega\in\Omega$,
        \[\sum_{i=0}^{n-1}
        |W_{t_{i+1}}(\omega)-W_{t_i}(\omega)|<C(\omega)\]
        it follows that the partial sum of quadratic variation
        satisfies
        \[I_N=\sum_{i=0}^{n-1}
        |W_{t_{i+1}}(\omega)-W_{t_i}(\omega)|^2\leq
        \tau\sum_{i=0}^{n-1}|W_{t_{i+1}}(\omega)-W_{t_i}(\omega)|
        <\tau C(\omega)\]
        where $\tau=\max_i|t_{i+1}-t_i|$ is the mesh of the
        partition.
        Then we send $\tau\to 0^+$ and obtain
        \[\text{a.s.}\quad I_N\to 0\]
        i.e., $[W_t,W_t]=0$ in a.s. sense, which implies
        convergence in probability
        \[\text{s.t.}\quad I_N\to 0\]
        
        And we know that
        \[\text{m.s.}\quad I_N\to T\]
        as the quadratic variation is $T$ on $[0,T]$,
        which yields convergence in probability
        \[\text{s.t.}\quad I_N\to T\]
        as well. And this leads a contradiction because of the
        uniqueness of limit.
    \end{proof}

    \problem
    % TODO What do you mean by NOT turn in

    \problem
    \begin{subproblem}[(\arabic*).]
        \item
        It should be understood as for any arbitrary $T>0$,
        \[\int_0^T\diff(W_t^3)=3\left(\int_0^TW_t^2\diff W_t
        +\int_0^TW_t\diff t\right)\] 
        or for any partition of $[0,T]$,
        \begin{multline*}
            \text{m.s.-}\lim_{\tau\to 0^+}
        \sum_{i=0}^{N-1}
        W_{t_{i+1}}^3-W^3_{t_i}
        =\text{m.s.-}\lim_{\tau\to 0^+}
        \sum_{i=0}^{N-1}
        3W_{t_i}^2(W_{t_{i+1}}-W_{t_i})\\
        +\text{m.s.-}\lim_{\tau\to0^+}
        \sum_{i=0}^{N-1}3W_{t^*}(t_{i+1}-t_i)
        \end{multline*}
        where $t^*_i$ of the Riemann partial sum is arbitrary
        in $[t_i,t_{i+1}]$.

        \item
        \begin{proof}
            We know that
            \[\begin{aligned}
                \diff (W_t^2)&=2W_t\diff W_t+(\diff W_t)^2\\
                &=2W_t\diff W_t+\diff t
            \end{aligned}\]
            hence
            \[\begin{aligned}
                \diff(W_t^3)&=\diff(W_t^2\cdot W_t)\\
                &=W_t^2\diff W_t+W_t\diff(W_t^2)+\diff(W_t^2)\cdot\diff W_t\\
                &=W_t^2\diff W_t
                +2W_t^2\diff W_t+W_t(\diff W_t)^2
                +2W_t(\diff W_t)^2+(\diff W_t)^3\\
                &=3W_t^2\diff W_t+3W_t(\diff W_t)^2+(\diff W_t)^3
            \end{aligned}\]
            Since
            \[\diff(W^2_t)=\diff t\]
            then we have that
            \[\begin{aligned}
                \diff(W_t^3)=3W_t^2\diff W_t+3W_t\diff t+\diff t\cdot\diff W_t
            \end{aligned}\]
            And in our previous homework we know that
            \[\diff t\cdot\diff W_t=0\]
            Therefore,
            \[\diff(W_t^3)=3W_t^2\diff W_t+3W_t\diff t\]
            \end{proof}
    \end{subproblem}

    \problem
    Motivated by the symmetry, we denote
    \newcommand{\sumhead}{\sum_{i=0}^{n-1}}
    \newcommand{\limhead}{\lim_{n\to\infty}}
    \newcommand{\wi}{W_{t_i}}
    \newcommand{\wii}{W_{t_{i+1}}}
    \newcommand{\deltaw}{\wii-\wi}
    \[\begin{aligned}
        I_n:=\sumhead\left(\lambda\wi+(1-\lambda)\wii\right)(\deltaw)\\
        J_n:=\sumhead\left((1-\lambda)\wi+\lambda\wii\right)(\deltaw)
    \end{aligned}\]
    then it is easy to find that
    \[\begin{aligned}
        I_n+J_n&=\sumhead(\wi+\wii)(\deltaw)\\
        &=\sumhead(\wii^2-\wi^2)\\
        &=W_t^2\to W_t^2\quad\text{m.s.}\\
        I_n-J_n&=\sumhead\left((2\lambda-1)\wi+(1-2\lambda)\wii\right)
                         (\deltaw)\\
               &=(1-2\lambda)\sumhead(\deltaw)^2
               \to (1-2\lambda)t\quad\text{m.s.}
    \end{aligned}\]
    Therefore by the additivity of m.s. convergence, we have
    that
    \[\text{m.s.}\quad I_n\to\frac{W_t^2+(1-2\lambda)t}{2}
    =\frac{W_t^2}{2}+\left(\frac{1}{2}-\lambda\right)t\]
    in other words, the integral
    \[\int_0^tW_s\diamond W_s
    =\frac{W_t^2}{2}+\left(\frac{1}{2}-\lambda\right)t\]
    which is the same as choosing the value of $\lambda$-point.

    \problem
    \label{var and e}
    \newcommand{\wj}{W_{t_j}}
    \newcommand{\wjj}{W_{t_{j+1}}}
    \newcommand{\deltawj}{\wjj-\wj}
    Denote
    \[I_n:=\sumhead\wi(\deltaw)\]
    where $0=t_0<t_1<\cdots<t_n=t$ is a partition of $[0,t]$,
    then we know that
    \[\text{m.s.}\quad I_n\to X_t:=\int_0^tW_s\diff W_s
    \quad(n\to\infty)\]
    i.e.,
    \[E(|I_n-X_t|^2)\to 0\]
    hence
    % TODO Detail
    \[\begin{aligned}
        E(I_n)&\to E(X_t)\\
        E(|I_n|^2)&\to E(X_t^2)
    \end{aligned}\]

    And it easy to see that
    \[E(I_n)=\sumhead E(\wi)\cdot E(\deltaw)=0\]
    thus
    \[E(X_t)=0\]
    As for $I_n^2$,
    \[I_n^2=\sumhead\wi^2(\deltaw)^2
    +\sum_{i<j}\wi\wj(\deltaw)(\deltawj)\]
    do note that $\deltawj$ in the last term is independent
    of $\wi\wj(\deltaw)$ when $i<j$, as the latter is before
    time $t_j$, hence the expectation can be self cancelled
    as $E(\deltawj)=0$,
    \[\begin{aligned}
        E(I_n^2)
        &=\begin{aligned}[t]
         &\sumhead E(\wi^2)\cdot E(|\deltaw|^2)\\
         &+\sum_{i<j}E\left(\wi\wj(\deltaw)\right)\cdot E(\deltawj)
         \end{aligned}\\
        &=\sumhead t_i(t_{i+1}-t_i)
        \to\int_0^ts\diff s=\frac{t^2}{2}
    \end{aligned}\]
    where the limit is obtained by the definition of Riemann integral.
    Therefore
    \[\var(X_t)=E(X_t^2)=\frac{t^2}{2}\]

    \problem
    % TODO sponge bob abcd

    \problem
    \begin{proof}
        \newcommand{\conde}[1]{E\left(\left. #1\right|\mathcal F_s\right)}
        By partition property, we have that
        \[\int_0^tf(u,W_u)\diff W_u=\int_0^sf(u,W_u)\diff W_u
        +\int_s^tf(u,W_u)\diff W_u\]
        And it is apparent that the first term is
        $\mathcal F_s$~-predictable, i.e.,
        \[\conde{\int_0^sf(u,W_u)\diff W_u}=\int_0^sf(u,W_u)\diff W_u\]
        

        In light of Fubini theorem\sidenote{Here we assume that $f(u,W_u)$
        satisfies all the necessary conditions.}, we have that
        \[\conde{\int_s^tf(u,W_u)\diff W_u}
            =\int_s^t\conde{f(u,W_u)\diff W_u}\]
        Since $f(u,W_u)$ and $\diff W_u$ are independent, we obtain
        \[\conde{f(u,W_u)\diff W_u}=\conde{f(u,W_u)}\cdot\conde{\diff W_u}
        =0\]
        as $\diff W_u$ is also independent of $\mathcal F_s$ which implies
        $\conde{\diff W_u}=E(\diff W_u)=0$. It follows that
        \[\conde{\int_s^tf(u,W_u)\diff W_u}=0\]

        Therefore,
        \[\begin{aligned}
        \conde{\int_0^tf(u,W_u)\diff W_u}
        &=\conde{\int_0^sf(u,W_u)\diff W_u}\\
        &=\int_0^sf(u,W_u)\diff W_u
        \end{aligned}\]
    \end{proof}


    \problem
    \begin{proof}
        By isometry identity
        \[\begin{aligned}
            E\left(\int_a^b|X_n-X|^2\diff t\right)
            &=E\left(\left|\int_a^b(X_n-X)\diff W_t\right|^2\right)\\
            &=E\left(\left|
            \int_a^bX_n\diff W_t-\int_a^bX\diff W_t
            \right|^2\right)
        \end{aligned}\]
        hence
        \[\lim_{n\to\infty}E\left(
            \int_a^b|X_n-X|^2\diff t\right)=0\]
        means
        \[\lim_{n\to\infty}E\left(\left|
            \int_a^bX_n\diff W_t-\int_a^bX\diff W_t
            \right|^2\right)=0\]
        i.e.,
        \[\text{m.s.}\quad
        \int_a^bX_n\diff W_t\to\int_a^bX\diff W_t\]
    \end{proof}

    \problem
    In problem \ref{var and e} we know that
    \[\begin{aligned}
        E\left(\int_0^TW_s\diff W_s\right)&=0\\
        \var\left(\int_0^TW_s\diff W_s\right)&=\frac{t^2}{2}
    \end{aligned}\]
    hence
    \[E\left(\left(\int_0^TW_s\diff W_s\right)^2\right)=\frac{t^2}{2}\]
    On the other hand, by isometry identity,
    \[\begin{aligned}
        E\left(\left(\int_0^TW_s\diff W_s\right)^2\right)
        &=E\left(\int_0^TW_s^2\diff s\right)\\
        &=\int_0^TE(W_s^2)\diff s\\
        &=\int_0^Ts\diff s\\
        &=\frac{t^2}{2}
    \end{aligned}\]
    which coincides with result we obtained above.

    \problem
    \begin{subproblem}[(\arabic*).]
        \item
        Since It\^o integral has zero-mean, then
        we know that
        \[E(X)=0\]
        and in light of isometry identity,
        \[E(X^2)=E\left(\int_1^T\left(\frac{1}{\sqrt s}\right)^2
        \diff s\right)=\ln T\]
        therefore,
        \[\var(X)=E(X^2)=\ln T\]

        \item
        Similarly, we have that
        \[E(Y)=0\]
        and by implies identity,
        \[E(Y^2)=E\left(\int_0^Ts^{2\alpha}\diff s\right)
        =\frac{T^{2\alpha+1}}{2\alpha+1}\]
        if $\alpha>0$.

        When $\alpha\leq 0$, however,
        the problem of improper integral arises\sidenote{It is
        about whether the It\^o integral can be well defined.}.
        Recall how we define the It\^o integral.
        If there exists some random variable $Y$ s.t.
        \begin{equation}
            \label{eq:Ito def}
            E(|I_n-Y|^2)\to 0
        \end{equation}
        where
        \[I_n=\sum_{i=0}^{n-1}t_i^\alpha(\deltaw)\]
        for an arbitrary partition,
        then we say
        \[\int_0^Ts^\alpha\diff W_s:=Y\]
        And \cref{eq:Ito def} yields
        \[E(I_n^2)\to E(Y^2)\]
        % TODO illustrate
        and in the deduction of isometry identity, we know that
        \[E(I_n^2)=\sum_{i=0}^{n-1}E(t_i^{2\alpha})(t_{i+1}-t_i)
        =\sum_{i=0}^{n-1}t_i^{2\alpha}\Delta t_i\]
        which is a partitial sum of Riemann integral
        \begin{equation}
            \label{eq:Riemann int}
            \int_0^Ts^{2\alpha}\diff s
        \end{equation}
        It follows that Riemann integral above exists\sidenote{
            I admit there is a flaw that the sample point of
            It\^o is not arbitrary, so the existence of Riemann
            integral is not promised.

            But all I want is to
            illustrate the relationship between the existence of It\^o integral
            and square Riemann integral, and this is a good start point
            in my opinion -- as it can lead to some useful results,
            though may not so rigorous.
        }
        and
        \[E(Y^2)=\int_0^Ts^{2\alpha}\diff s\]
        That being said, the It\^o integral does not exists
        without the existence of the Riemann integral.
        In other words, a well defined It\^o integral
        $\int_0^Ts^\alpha\diff W_s$
        leads to
        \[E\left(\int_0^Ts^{2\alpha}\diff s\right)<\infty\]

        From calculus we know that \cref{eq:Riemann int} does
        not exists if $2\alpha+1\leq 0$.
        So in conclusion,
        when\sidenote{If $-1/2<\alpha\leq 0$, it is an improper
        integral indeed as the integrand $s^{\alpha}$ is unbounded on $[0,T]$.
        } $\alpha>-1/2$,
        \[\begin{aligned}
            E(Y)&=0\\
            \var(Y)&=\frac{T^{2\alpha+1}}{2\alpha+1}
        \end{aligned}\]
        when $\alpha\leq -1/2$, $Y$ is not well defined, then
        we can not talk about its expectation and variation either.
    \end{subproblem}

    \problem
    By isometry identity, we have that
    \[\begin{aligned}
        E\left(
            \left|\frac{\int_0^ts\diff W_s}{t^\alpha}\right|^2
        \right)&=\frac{E(|\int_0^ts\diff W_s|^2)}{t^{2\alpha}}\\
        &=\frac{E(\int_0^ts^2\diff s)}{t^{2\alpha}}\\
        &=\frac{t^{3-2\alpha}}{3}\to
        \begin{cases}
            0,&3-2\alpha>0\\
            \frac{1}{3},&3-2\alpha=0\\
            \infty,&3-2\alpha<0
        \end{cases}
    \end{aligned}\]
    hence if $\alpha<3/2$ we have that
    \[E\left(
            \left|\frac{\int_0^ts\diff W_s}{t^\alpha}\right|^2
        \right)\to 0\]
    i.e.,
    \[\text{m.s.-}\lim_{t\to0}\frac{1}{t^\alpha}\int_0^ts\diff W_s=0\]

    \problem
    It easy to see that
    \[W_t=\int_0^t\diff W_s\]
    as $\sum_{i=0}^{n-1}(\deltaw)=W_t$,
    then we know that
    \[E\left(\int_0^t\diff W_s\cdot\int_0^tf(s)\diff W_s\right)
    =E\left(\int_0^tf(s)\diff t\right)=\int_0^tf(s)\diff s\]
    as $f(t)$ is integrable in $(0,t)$.
    Since It\^o integral has zero mean, we have that
    \[\begin{aligned}
        \cov\left(\int_0^t\diff W_s,\int_0^tf(s)\diff W_s\right)
        =E\left(\int_0^t\diff W_s\cdot\int_0^tf(s)\diff W_s\right)
        =\int_0^tf(s)\diff s
    \end{aligned}\]
    i.e.,
    \[\cov\left(W_t,\int_0^tf(s)\diff W_s\right)=
    \int_0^tf(s)\diff s\]

    \problem
    \label{dZt}
    % TODO definition of differential

    \problem
    \begin{proof}
        Since $W_t$ is continuous w.r.t. $t$, then the
        integral $\int_0^tW_s\diff s$ is continuous too.
        Hence $A_t$ is continuous. And by basic rules of
        differential we leart in class,
        \[\diff Z_t=\diff(tA_t)=t\diff A_t+A_t\diff t\]
        as $t$ is determinisitic.

        From problem \ref{dZt} we know that
        \[\diff Z_t=W_t\diff t\]
        therefore,
        \[W_t\diff t=t\diff A_t+A_t\diff t\]
        i.e.,
        \[\diff A_t=\frac{1}{t}(W_t-A_t)\diff t
        =\frac{1}{t}\left(W_t-\frac{1}{t}Z_t\right)\diff t\]
    \end{proof}
\end{document}