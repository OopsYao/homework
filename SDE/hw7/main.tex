\documentclass{homework}

\title{Homework 7}

\newcommand{\trans}{\mathrm T}

\begin{document}
    \maketitle

    \problem
    \begin{proof}
        Denote
        \[f(t,W_t):=\frac{1}{2}W_t^2-\frac{1}{2}t\]
        then it is equivalent to show that
        \[W_t\diff W_t=\diff f(t,W_t)\]
        as
        \[\int_0^T\diff f(s,W_s)=f(T,W_T)-f(0,W_0)=
        \frac{1}{2}W_T^2-\frac{1}{2}T\]
        And we have from It\^o's lemma that
        \[\diff f(t,W_t)=-\frac{\diff t}{2}+W_t\diff W_t
        +\frac{\diff t}{2}=W_t\diff W_t\]
        which is what we need exactly.
    \end{proof}

    \problem
    \begin{proof}
        Denote
        \[f(t,W_t):=S_t\]
        then it easy to see that
        \[\begin{aligned}
            \frac{\partial f}{\partial t}(t,W_t)
            &=\left(\mu-\frac{\sigma^2}{2}\right)f(t,W_t)
            =\left(\mu-\frac{\sigma^2}{2}\right)S_t\\
            \frac{\partial f}{\partial W_t}(t,W_t)
            &=\sigma f(t,W_t)=\sigma S_t\\
            \frac{\partial^2 f}{\partial^2 W_t}(t,W_t)
            &=\sigma^2 f(t,W_t)=\sigma^2 S_t
        \end{aligned}\]
        Then we have from It\^o's lemma that
        \[\begin{aligned}
            \diff S_t&=\diff f(t,W_t)\\
            &=\left(\mu-\frac{\sigma^2}{2}\right)S_t\diff t
            +\sigma S_t\diff W_t+\frac{\sigma^2 S_t}{2}\diff t\\
            &=\mu S_t\diff t+\sigma S_t\diff W_t
        \end{aligned}\]
    \end{proof}

    \problem
    \begin{subproblem}[(\alph*)]
        \item
        \begin{proof}
            We calculate $\diff(1/Y_t)$ first, then achieve the final goal by
            product rule.
            We have from Taylor's expansion that
            \[\diff\left(\frac{1}{Y_t}\right)
            =-\frac{\diff Y_t}{Y_t^2}+\frac{(\diff Y_t)^2}{Y_t^3}
            -\frac{(\diff Y_t)^3}{Y_t^4}+\cdots\]
            but the high order terms is 0 since $Y_t$ is an It\^o diffusion, i.e.,
            \[(\diff Y_t)^k=(\diff Y_t)^{k-2}(\diff Y_t)^2=\sigma^2(\diff Y_t)^{k-2}\diff t=0
            ,k\geq 3\]
            Therefore,
            \[\diff\left(\frac{1}{Y_t}\right)
            =-\frac{\diff Y_t}{Y_t^2}+\frac{(\diff Y_t)^2}{Y_t^3}\]
            It follows by product rule that
            \[\begin{aligned}
                \diff\left(\frac{X_t}{Y_t}\right)
                &=X_t\diff\left(\frac{1}{Y_t}\right)+\frac{\diff X_t}{Y_t}
                +\diff X_t\diff\left(\frac{1}{Y_t}\right)\\
                &=-\frac{X_t\diff Y_t}{Y_t^2}+\frac{X_t(\diff Y_t)^2}{Y_t^3}
                +\frac{\diff X_t}{Y_t}
                -\frac{\diff X_t\diff Y_t}{Y_t^2}
                +\frac{\diff X_t(\diff Y_t)^2}{Y_t^3}\\
                &=\frac{Y_t\diff X_t-X_t\diff Y_t-\diff X_t\diff Y_t}%
                  {Y_t^2}+\frac{X_t}{Y_t^3}(\diff Y_t)^2
            \end{aligned}\]
            as $\diff X_t(\diff Y_t)^2=0$ for the same reason.
        \end{proof}

        \item
        \begin{proof}
            We have from (bivariate) It\^o's lemma that
            \[\begin{aligned}
                \diff\left(\frac{X_t}{Y_t}\right)&=
                \frac{\diff X_t}{Y_t}-\frac{X_t}{Y_t^2}\diff Y_t
                +\frac{1}{2}\left(
                    -\frac{2}{Y_t^2}\diff X_t\diff Y_t
                    +\frac{2X_t}{Y_t^3}(\diff Y_t)^2
                \right)\\
                &=\frac{Y_t\diff X_t-X_t\diff Y_t-\diff X_t\diff Y_t}%
                  {Y_t^2}+\frac{X_t}{Y_t^3}(\diff Y_t)^2
            \end{aligned}\]
        \end{proof}

        Therefore,
        \[\diff\left(\frac{X_t}{f(t)}\right)
        =\frac{f(t)\diff X_t-X_t\diff f(t)}{(f(t))^2}\]
        as $f(t)$ is determinstic which implies that
        \[\diff X_t\diff f(t)=(\diff f(t))^2=0\]
    \end{subproblem}

    \problem
    \begin{subproblem}[(\alph*)]
        \item
        Since $X_t^{(1)},X_t^{(2)}$ are two It\^o diffusions,
        then we have that, for $i=1,2$,
        \[\left(\diff W_t^{(i)}\right)^2=\mu_i^2(\diff t)^2
        +\sigma_i^2\left(\diff W_t^{(i)}\right)^2
        +2\mu_i\sigma_i\diff t\diff W_t^{(i)}
        =\sigma_i^2\diff t\]
        as $W_t^{(1)},W_t^{(2)}$ are independent, where
        $\mu_i,\sigma_i$ is short for
        \[\mu_i\left(t,W_t^{(i)}\right),
        \sigma_i\left(t,W_t^{(i)}\right)\]
        accordingly.

        And we have from multi-dimensional Taylor's expansion that
        \[\begin{aligned}
            \diff f\left(t,X_t^{(1)},X_t^{(2)}\right)=
            \begin{aligned}[t]
            &\frac{\partial f}{\partial t}\diff t
            +\frac{\partial f}{\partial x_1}\diff X_t^{(1)}
            +\frac{\partial f}{\partial x_2}\diff X_t^{(2)}\\
            &+\frac{1}{2}\left(
                \frac{\partial^2 f}{\partial x_1^2}\left(\diff X_t^{(1)}\right)^2
                +\frac{\partial^2 f}{\partial x_2^2}\left(\diff X_t^{(2)}\right)^2
            \right)
            \end{aligned}
        \end{aligned}\]
        where $\frac{\partial f}{\partial x_1}$ is short for
        \[\left.\frac{f(t,x_1,x_2)}{\partial x_1}
        \right|_{(t,x_1,x_2)=\left(t,X_t^{(1)},X_t^{(2)}\right)}\]
        and so on.
        Note that in the equation above we throw away terms that contain,
        \[\left(\diff X_t^{(1)}\right)^p\left(\diff X_t^{(2)}\right)^q,
        \left(\diff X_t^{(i)}\right)^k(k\geq 3)\]
        as they all equal to 0 due to the independence and high order
        terms being zero, i.e.,
        \[\left(\diff X_t^{(i)}\right)^k
        =\left(\diff X_t^{(i)}\right)^{k-2}\cdot\sigma_i^2\diff t=0,
        k\geq 3\]
        Hence 2-dimensional It\^o's lemma can be obtained as
        \begin{equation}
            \label{eq:2-d Ito lemma}
            \begin{aligned}
            \diff f\left(t,X_t^{(1)},X_t^{(2)}\right)&=
            \begin{aligned}[t]
            &\frac{\partial f}{\partial t}\diff t
            +\frac{\partial f}{\partial x_1}\diff X_t^{(1)}
            +\frac{\partial f}{\partial x_2}\diff X_t^{(2)}\\
            &+\frac{1}{2}\left(
                \frac{\partial^2 f}{\partial x_1^2}\sigma_1^2\diff t
                +\frac{\partial^2 f}{\partial x_2^2}\sigma_2^2\diff t
            \right)
            \end{aligned}\\
            &=\begin{aligned}[t]
            &\left(
                \frac{\partial f}{\partial t}+     
                \frac{1}{2}\left(
                \frac{\partial^2 f}{\partial x_1^2}\sigma_1^2
                +\frac{\partial^2 f}{\partial x_n^2}\sigma_n^2
                \right)
            \right)\diff t\\
            &+\frac{\partial f}{\partial x_1}\diff X_t^{(1)}
            +\frac{\partial f}{\partial x_2}\diff X_t^{(2)}
            \end{aligned}
            \end{aligned}
        \end{equation}

        \item
        The generalization is rather straight-forward since
        the critical condition independence still holds and,
        \[\diff W_t^{i}\diff W_t^{(j)}=
        \begin{cases}
            \sigma_j^2\diff t,&i=j\\
            0,&i\neq j
        \end{cases}\]
        Therefore \cref{eq:2-d Ito lemma} can be simply generalized
        to $n$-dimension,
        \begin{equation*}
            \begin{aligned}
            \diff f\left(t,X_t^{(1)},\ldots,X_t^{(n)}\right)&=
            \begin{aligned}[t]
            &\frac{\partial f}{\partial t}\diff t
            +\frac{\partial f}{\partial x_1}\diff X_t^{(1)}
            +\cdots
            +\frac{\partial f}{\partial x_n}\diff X_t^{(n)}\\
            &+\frac{1}{2}\left(
                \frac{\partial^2 f}{\partial x_1^2}\sigma_1^2\diff t
                +\ldots
                +\frac{\partial^2 f}{\partial x_n^2}\sigma_n^2\diff t
            \right)
            \end{aligned}\\
            &=\left(\frac{\partial f}{\partial t}
            +\frac{1}{2}\boldsymbol\sigma^\trans A\boldsymbol\sigma\right)\diff t
            +\nabla f\cdot\diff\boldsymbol X_t
            \end{aligned}
        \end{equation*}
        where gradient operator $\nabla$ is defined as\sidenote{
            Here the notation $\frac{\partial f}{\partial x_1}$ is
            similar to the 2-dimension case, as well as other notations
            like $\mu_1,\sigma_1$.
        }
        \[\nabla f:=\left(\frac{\partial f}{\partial x_1},\ldots,\frac{\partial f}{\partial x_n}\right)\]
        and
        \[\begin{aligned}
            \diff\boldsymbol X_t&:=
            \left(\diff X_t^{(1)},\ldots,\diff X_t^{(n)}\right)\\
            \boldsymbol\sigma&:=
            \left(\sigma_1,\ldots,\sigma_n\right)^\trans\\
            A&:=\mathrm{diag}\left\{\frac{\partial^2 f}{\partial x_1^2},
           \frac{\partial^2 f}{\partial x_2^2},\ldots,
           \frac{\partial^2 f}{\partial x_n^2}\right\}
        \end{aligned}\]
        
        \item
        Here we claim that
        \[\diff W_t^{(i)}\diff W_t^{(j)}=\rho_{ij}\diff t\]
        to see this,
        we assume that for any $i,j$, the pair$(W_t^{(i)},W_t^{(j)})$
        has a joint normal distribution\sidenote{
            Two normally distributed random variables need not be jointly bivariate normal.
            But here we need this condition to achieve our claim.
        }. Take $(W_t^{(1)},W_t^{(2)})$ for example,
        then there exists a matrix $A\in\mathbb R^{2\times l}$ s.t.
        \[\boldsymbol W_t:=(W_t^{(1)},W_t^{(2)})^\trans
        =A\boldsymbol Z_t\]
        where $\boldsymbol Z_t$ is a random vector composed of independent normally
        distributed processes, i.e.,
        \[\boldsymbol Z_t:=(Z_t^{(1)},Z_t^{(2)},\ldots,Z_t^{(l)})^\trans\]
        and $Z_t^{(i)}\overset{\text{i.i.d.}}\sim\mathcal N(0,t),i=1,2,\ldots,l$.
        To keep things simple, we put the details in appendix: \nameref{app:jn}.

        It follows that
        \[\diff X_t^{(i)}\diff X_t^{(j)}
        =\left(\mu_i\diff t+\sigma_i\diff W_t^{(i)}\right)
        \left(\mu_j\diff t+\sigma_j\diff W_t^{(j)}\right)
        =\rho_{ij}\sigma_i\sigma_j\diff t\]

        So any other high order terms like
        \[\diff X_t^{(i)}\diff X_t^{(j)}\diff X_t^{(k)}\text{ or }
        \left(\diff X_t^{(i)}\right)^2\diff X_t^{(j)}\]
        is 0. Then we can have from multivariate Taylor's expansion
        that
        \[\begin{aligned}
            \diff f\left(t,X_t^{(1)},\ldots,X_t^{(n)}\right)&=
            \begin{aligned}[t]
            &\frac{\partial f}{\partial t}\diff t
            +\frac{\partial f}{\partial x_1}\diff X_t^{(1)}
            +\cdots
            +\frac{\partial f}{\partial x_n}\diff X_t^{(n)}\\
            &+\frac{1}{2}\sum_{i,j}\frac{\partial^2 f}{\partial x_i\partial x_j}
            \diff X_t^{(i)}\diff X_t^{(j)}\\
            \end{aligned}\\
            &=\left(\frac{\partial f}{\partial t}
            +\frac{1}{2}\sum_{i,j}\rho_{ij}\sigma_i\sigma_j
            \frac{\partial^2 f}{\partial x_i\partial x_j}
            \right)\diff t
            +\sum_{i=1}^n\frac{\partial f}{\partial x_n}\diff X_t^{(n)}\\
            &=\left(\frac{\partial f}{\partial t}
            +\frac{1}{2}\boldsymbol\sigma^\trans(\mathcal Lf)\boldsymbol\sigma\right)
            \diff t
            +\nabla f\cdot\diff\boldsymbol X_t
        \end{aligned}\]
        where
        \[\mathcal Lf:=\left(\rho_{ij}\sigma_i\sigma_j
            \frac{\partial^2 f}{\partial x_i\partial x_j}\right)_{n\times n}\]
        and other notations is similar to above.

    \end{subproblem}

    \problem
    By It\^o's lemma, we have that
    \[\diff (tW_t^2)=W_t^2\diff t+2tW_t\diff W_t+\frac{2t}{2}\diff t
    =(W_t^2+t)\diff t+2tW_t\diff W_t\]

    \problem
    \begin{proof}
        Denote
        \[\diff X_t=\mu(t,W_t)\diff t+\sigma(t,W_t)\diff W_t\]
        where $W_t$ is a Wiener process, as $X_t$ is an It\^o diffusion.
        Therefore,
        \[\diff f(t)\diff X_t=\mu(t,W_t)\diff t\diff f(t)
        +\sigma(t,W_t)\diff W_t\diff f(t)=0\]
        as $f(t)$ is determinstic which implies that
        \[\diff t\diff f(t)=\diff W_t\diff f(t)=0\]
    \end{proof}

    \problem
    \newcommand{\img}{\mathrm i}
    \begin{proof}
        We have from Euler's formula that,
        \[\begin{aligned}
            \int_0^T\e^{\frac{t}{2}+\img W_t}\diff W_t
            =\int_0^T\e^{\frac{t}{2}}\cos W_t\diff W_t
            +\img\int_0^T\e^{\frac{t}{2}}\sin W_t\diff W_t
        \end{aligned}\]
        and
        \[\begin{aligned}
            \img(1-\e^{\frac{T}{2}+\img W_T})
        &=\e^{\frac{T}{2}}\sin W_T+\img(1-\e^{\frac{T}{2}}\cos W_T)\\
        &=\int_0^T\diff\left(\e^{\frac{t}{2}}\sin W_t\right)
        +\img\int_0^T\diff\left(1-\e^{\frac{t}{2}}\cos W_t\right)
        \end{aligned}\]
        Therefore, it is equivalent to show that
        \[\begin{aligned}
            \diff\left(\e^{\frac{t}{2}}\sin W_t\right)
            &=\e^{\frac{t}{2}}\cos W_t\diff W_t\\
            \diff\left(1-\e^{\frac{t}{2}}\cos W_t\right)
            &=\e^{\frac{t}{2}}\sin W_t\diff W_t
        \end{aligned}\]

        By It\^o's lemma, we have that
        \[\begin{aligned}
            \diff\left(\e^{\frac{t}{2}}\sin W_t\right)
            &=\frac{1}{2}\e^{\frac{t}{2}}\sin W_t\diff t
            +\e^{\frac{t}{2}}\cos W_t\diff W_t
            -\frac{1}{2}\e^{\frac{t}{2}}\sin W_t\diff t\\
            &=\e^{\frac{t}{2}}\cos W_t\diff W_t\\
            \diff\left(1-\e^{\frac{t}{2}}\cos W_t\right)
            &=-\frac{1}{2}\e^{\frac{t}{2}}\cos W_t
            +\e^{\frac{t}{2}}\sin W_t\diff W_t
            +\frac{1}{2}\e^{\frac{t}{2}}\cos W_t\diff t\\
            &=\e^{\frac{t}{2}}\sin W_t\diff W_t
        \end{aligned}\]
        which is what we need exactly.
    \end{proof}

    \problem
    \begin{proof}
        Denote
        \[f(t):=1,g(W_t):=\arctan W_t\]
        thus it easy to see that
        \[\begin{aligned}
            g'(W_t)&=\frac{1}{1+W_t^2}\\
            g''(W_t)&=-\frac{2W_t}{(1+W_t^2)^2}\\
            f'(t)&=0
        \end{aligned}\]
        then we have from integration by parts that
        \[\begin{aligned}
            \int_0^T\frac{1}{1+W_t^2}\diff W_t
            &=\left.\arctan W_t\right|_0^T
            -\frac{1}{2}\int_0^T-\frac{2W_t}{(1+W_t^2)^2}\diff t\\
            &=\arctan W_T+\int_0^T\frac{W_t}{(1+W_t^2)^2}\diff t
        \end{aligned}\]
    \end{proof}

    Therefore, in sight of zero mean property of It\^o integral and
    Fubini theorem,
    \[E(\arctan W_T)=-E\left(\int_0^T\frac{W_t}{(1+W_t^2)^2}\diff t\right)
    =-\int_0^TE\left(\frac{W_t}{(1+W_t^2)^2}\right)\diff t\]

    \problem
    \begin{proof}
        Denote
        \[g(W_t):=(W_t-1)\e^{W_t}\]
        thus
        \[g'(W_t)=W_t\e^{W_t},g''(W_t)=(W_t+1)\e^{W_t}\]
        hence we have from integration by parts that
        \begin{equation}
            \label{eq:WeW}
            \begin{aligned}
            \int_0^TW_t\e^{W_t}\diff W_t&=
            \left.g(W_t)\right|_0^T
            -\frac{1}{2}\int_0^Tg''(W_t)\diff t\\
            &=\left.(W_t-1)\e^{W_t}\right|_0^T
            -\frac{1}{2}\int_0^T(W_t+1)\e^{W_t}\diff t\\
            &=W_T\e^{W_T}+1-\e^{W_T}
            -\frac{1}{2}\int_0^T(W_t+1)\e^{W_t}\diff t\\
            \end{aligned}
        \end{equation}
        as the determinstic part is 1 of which the derivative is 0.
    \end{proof}

    Denote
    \[h(t):=E(g(W_t))\]
    then we have from \cref{eq:WeW} that
    \[\left.h(t)\right|_0^T
    -\frac{1}{2}E\left(\int_0^T(W_t+1)\e^{W_t}\diff t\right)=0\]
    as the expectation of LHS which is an It\^o integral is 0.
    Thus, by Fubini theorem,
    \[h(t)|_0^T=\frac{1}{2}\int_0^TE\left((W_t+1)\e^{W_t}\right)\diff t
    =\frac{1}{2}\int_0^T\left(2E(\e^{W_t})+h(t)\right)\diff t\]
    i.e.,
    \[\int_0^T\diff h(t)=\int_0^T\left(\frac{h(t)}{2}
    +E(\e^{W_t})\right)\diff t\]
    which is equivalent to an ODE
    \[h'(t)=\frac{h(t)}{2}+E(\e^{W_t})\]

    And we have the fact that
    \[E(\e^{uW_t})=\e^{\frac{u^2t}2}\]
    hence we obtain the solution
    \[h(t)=(t-1)\e^{\frac{t}{2}}\]
    as $h(0)=E\left((W_0-1)\e^{W_0}\right)=-1$.
    Therefore,
    \[E(W_t\e^{W_t})=h(t)+\e^{\frac{t}{2}}=t\e^{\frac{t}{2}}\]
    thus
    \def\cov{\mathrm{Cov}}
    \def\var{\mathrm{Var}}
    \def\corr{\mathrm{Corr}}
    \[\cov(W_t,\e^{W_t})=E(W_t\e^{W_t})-E(W_t)\cdot E(\e^{W_t})
    =t\e^{\frac{t}{2}}\]
    and
    \[\corr(W_t,\e^{W_t})=\frac{\cov(W_t,\e^{W_t})}%
    {\sqrt{\var(W_t)\cdot\var(\e^{W_t})}}
    =\sqrt{\frac{t^2\e^t}{t\e^t(\e^t-1)}}
    =\sqrt{\frac{t}{\e^t-1}}\]
    as
    \[\var(\e^{W_t})=E(\e^{2W_t})-(E(\e^{W_t}))^2
    =\e^{2t}-\e^t=
    \e^t(\e^t-1)\]
    It follows that
    \[\begin{aligned}
        \corr(W_t,\e^{W_t})&\to 1\quad(t\to 0^+)\\
        \corr(W_t,\e^{W_t})&\to 0\quad(t\to \infty)
    \end{aligned}\]

    \problem
    \begin{proof}
        We have from integration by parts that
        \[\begin{aligned}
            \int_0^T\frac{2W_t}{1+W_t^2}\diff W_t
            &=\left.\ln(1+W_t^2)\right|_0^T
            -\int_0^T\frac{1+W_t^2-2W_t^2}{(1+W_t^2)^2}\diff t\\
            &=\ln(1+W_T^2)-\int_0^T\frac{1-W_t^2}{(1+W_t^2)^2}\diff t
        \end{aligned}\]
        Therefore,
        \[E(\ln(1+W_T^2))=E\left(
            \int_0^T\frac{1-W^2_t}{(1+W_t^2)^2}\diff t
        \right)\]
        as the expectation of It\^o integral is 0.
        And note that
        \[\begin{aligned}
            \frac{1-W_t^2}{(1+W_t^2)^2}\leq\frac{1+W_t^2}{(1+W_t^2)^2}
            =\frac{1}{1+W_t^2}\leq 1
        \end{aligned}\]
        and
        \[\frac{1-W_t^2}{(1+W_t^2)^2}+\frac{1}{8}
        =\frac{8(1-W_t^2)+(1+W_t^2)^2}{8(1+W_t^2)^2}
        =\frac{(W_t^2-3)^2}{8(1+W_t^2)^2}\geq 0\]
        hence
        \[-\frac{T}{8}=-E\left(\int_0^T\frac{1}{8}\diff t\right)
        \leq E(\ln(1+W_T^2))
        \leq E\left(\int_0^T\diff t\right)=T\]
    \end{proof}
    There is no contradiction -- they both are correct, while the
    bound yielded by Jensen's inequality is finer, as
    \[\ln(1+T)\leq T\]

    \problem
    Denote
    \[\begin{aligned}
        f(t)&=t^{-\frac{1}{2}}\\
        g(t,W_t)&=\e^{-\frac{W_t^2}{2t}}
    \end{aligned}\]
    thus
    \[\begin{aligned}
        \frac{\partial g(t,W_t)}{\partial W_t}
        &=-\frac{W_t\e^{-\frac{W_t^2}{2t}}}{t}\\
        \frac{\partial^2 g(t,W_t)}{\partial W_t^2}
        &=\frac{1}{t}\left(\frac{W_t^2}{t}-1\right)\e^{-\frac{W_t^2}{2t}}
    \end{aligned}\]
    then we have from integration by parts that
    \[\begin{aligned}
        \int_a^bt^{-\frac{3}{2}}W_t\e^{-\frac{W_t^2}{2t}}\diff W_t
        &=-\int_a^bf(t)\frac{\partial g(t,W_t)}{\partial W_t}\diff W_t\\
        &=\begin{aligned}[t]
        &-\left.f(t)g(t,W_t)\right|_a^b
         +\int_a^b\frac{\partial}{\partial t}\left(f(t)g(t,W_t)\right)\diff t\\
        &+\frac{1}{2}\int_a^bf(t)\frac{\partial^2g(t,W_t)}{\partial W_t^2}\diff t
        \end{aligned}
    \end{aligned}\]
    Also note the fact that
    \[\frac{\partial}{\partial t}\left(f(t)g(t,W_t)\right)
    =\left(\frac{W_t^2}{2}t^{-\frac{5}{2}}-\frac{t^{-\frac{3}{2}}}{2}\right)
    \e^{-\frac{W_t^2}{2t}}\]
    and
    \[f(t)\frac{\partial^2g(t,W_t)}{\partial W_t^2}
    =\left(t^{-\frac{5}{2}}W_t^2-t^{-\frac{3}{2}}\right)\e^{-\frac{W_t^2}{2t}}\]
    i.e.,
    \[\frac{\partial}{\partial t}\left(f(t)g(t,W_t)\right)
    +\frac{f(t)}{2}\frac{\partial^2g(t,W_t)}{\partial W_t^2}
    =t^{-\frac{5}{2}}(W_t^2-t)\e^{-\frac{W_t^2}{2t}}\]
    therefore,
    \[\begin{aligned}
        \int_a^bt^{-\frac{3}{2}}W_t\e^{-\frac{W_t^2}{2t}}\diff W_t
        &=-\left.f(t)g(t,W_t)\right|_a^b
        +\int_a^bt^{-\frac{5}{2}}(W_t^2-t)\e^{-\frac{W_t^2}{2t}}\diff t\\
        &=\frac{\e^{-\frac{W_a^2}{2a}}}{\sqrt a}
        -\frac{\e^{-\frac{W_b^2}{2b}}}{\sqrt b}
        +\int_a^bt^{-\frac{5}{2}}(W_t^2-t)\e^{-\frac{W_t^2}{2t}}\diff t\\
    \end{aligned}\]

    \appendix
    \section{Proof About Joint Normal Distribution}
    \label{app:jn}
    \begin{proof}
        Take $(W_t^{(1)},W_t^{(j)})$ for example.
        Denote $A=(\alpha_1,\alpha_2)^\trans$,
        it follows that
        \[W_t^{(1)}=\alpha_1^\trans\boldsymbol Z_t,
        W_t^{(2)}=\alpha_2^\trans\boldsymbol Z_t\]
        hence
        \[\diff W_t^{(1)}\diff W_t^{(2)}
        =\alpha_1^\trans\diff\boldsymbol Z_t
        \alpha_2^\trans\diff\boldsymbol Z_t
        =\alpha_1^\trans(\diff\boldsymbol Z_t\diff\boldsymbol Z_t^\trans)\alpha_2
        =\alpha_1^\trans\alpha_2\diff t
        \]
        as
        \[\diff\boldsymbol Z_t\diff\boldsymbol Z_t^\trans
        =\left(\diff Z_t^{(i)}\diff Z_t^{(j)}\right)_{l\times l}=I\diff t\]
        where $I$ represents the unit matrix.
        On the other hand,
        \[\cov\left(W_t^{(1)},W_t^{(2)}\right)
        =\cov(\alpha_1^\trans\boldsymbol Z_t,\alpha_2^\trans\boldsymbol Z_t)
        =\alpha_1^\trans\alpha_2 t\]
        due to the independence of components of $\boldsymbol Z_t$,
        thus the coefficient
        \[\rho_{12}=\alpha_1^\trans\alpha_2\]
        It follows that
        \[\diff W_t^{(1)}\diff W_t^{(2)}=\rho_{12}\diff t\]
        Therefore in general,
        \[\diff W_t^{(i)}\diff W_t^{(j)}=\rho_{ij}\diff t\]
    \end{proof}
\end{document}