\problem
\begin{question}
    Let $X$ be a random variable.  

    (i) prove that for any positive number $\lambda$
    \begin{equation}\label{3}
    P(\omega; X(\omega) \geq\lambda)\leq \frac{E(e^{t X})}{e^{\lambda t}}, \forall t>0
    \end{equation}
    and
    \begin{equation}\label{4}
    P(\omega; X(\omega) \leq\lambda)\geq \frac{E(e^{t X})}{e^{\lambda t}}, \forall t>0.
    \end{equation}

    (ii)  let $X$ be be the sum of $n$ random variables $X_i$, $i=1,2,...,n$.  Prove that
    \[P(\omega; X(\omega) \geq\lambda)\leq \frac{E(\prod_i^n e^{t X_i})}{e^{\lambda t}}, \forall t<0.\]

\end{question}

    \begin{subproblem}
        \item
        \begin{proof}
            For $E(\e^{tX})$ we have that
            \begin{align*}
                E(\e^{tX})&\geq\int_{X(\omega)\geq\lambda}\e^{tX}\diff P\\
                &\geq\e^{\lambda t}\int_{X(\omega)\geq\lambda}\diff P\\
                &=\e^{\lambda t}P(\omega;X(\omega)\geq\lambda)
            \end{align*}
            Then we obtain
            \begin{equation}
                \label{eq:Chernoff bound}
                P(\omega;X(\omega)\geq\lambda)\leq\frac{E(\e^{tX})}{\e^{\lambda t}}
            \end{equation}

            Similarly we have\sidenote{It seems like there is something wrong in
            the questions. This inequality can not hold for an arbitrary random varibale
            $X$
            \[P(X\leq\lambda)\leq\frac{E(\e^{tX})}{\e^{\lambda t}}
            \leq P(X\geq\lambda)\]
            
            So I just proved another inequality which is probably the one you
            really mean to.}

            \begin{align*}
                E(\e^{-tX})&\geq\int_{X(\omega)\leq\lambda}\e^{-tX}\diff P\\
                &\geq\e^{-\lambda t}\int_{X(\omega)\leq\lambda}\diff P\\
                &=\e^{-\lambda t}P(\omega;X(\omega)\leq\lambda)
            \end{align*}
            and so,
            \[P(\omega;X(\omega)\leq\lambda)\leq\e^{\lambda t}E(\e^{-tX})\]
        \end{proof}

        \item
        \begin{proof}
            Since $\prod_i^n\e^{tX_i}=\e^{t\sum_i^n X_i}=\e^{tX}$,
            it falls to prove \cref{eq:Chernoff bound} at $t<0$.

            \emph{But I have no idea about how to achieve this. :(}
        \end{proof}
    \end{subproblem}

\problem
\begin{question}
    (\ref{3}) and (\ref{4}) are called Chernoff bounds
    and $m(\lambda):=E(e^{t X})$ is called the \emph{moment generating function}
    (mgf) of $X$.  These estimates sometimes can be very useful in various
    applications if applied properly.  For example, let $X$ be a normal distributed
    random variable $X\sim N(\mu,\sigma^2)$.  Prove that for any $\lambda>\mu$
    \begin{equation}\label{5}
    P(X\geq \lambda)\leq e^{-\frac{(\lambda-\mu)^2}{2\sigma^2}}.
    \end{equation}

    Hint: First show that the mgf of $X$ is $m(t)=e^{\mu t+\frac{1}{2}\sigma^2t^2}$;
    then use the Chernoff bounds to obtain an upper bound for $P(X\geq \lambda)$,
    finally find the minimum of the quadratic function to prove (\ref{5}).
\end{question}

    \begin{proof}
        Since $X\sim N(\mu,\sigma^2)$,
        denote $Y=(X-\mu)/\sigma\sim N(0,1)$, therefore
        the mgf of $X$ is
        \begin{align*}
            E\e^{tx}&=E\e^{t(\mu+\sigma Y)}\\
                    &=\e^{t\mu}\int_{-\infty}^\infty\e^{t\sigma x}
                      \cdot\frac{\e^{-\frac{x^2}2}}{\sqrt{2\pi}}\diff x\\
                    &=\e^{t\mu}\cdot\e^{\frac{t^2\sigma^2}{2}}\int_{-\infty}^{\infty}
                      \frac{1}{\sqrt{2\pi}}\e^{-\frac{(x-t\sigma)^2}{2}}
                      \diff x\\
                    &=\e^{t\mu+\frac{t^2\sigma^2}{2}}
        \end{align*}
        Hence
        \begin{align*}
            \frac{E\e^{tx}}{\e^{\lambda t}}&=\e^{t(\mu-\lambda)+\frac{t^2\sigma^2}{2}}\\
            &=\e^{\frac{\sigma^2}{2}\left(t+\frac{\mu-\lambda}{\sigma^2}\right)^2}
              \cdot\e^{-\frac{(\mu-\lambda)^2}{2\sigma^2}}\\
            &\leq\e^{-\frac{(\mu-\lambda)^2}{2\sigma^2}}
        \end{align*}

        Applying Chernoff bounds we obtain
        \begin{align*}
            P(w,X(w)\geq\lambda)&\leq\frac{E\e^{tx}}{\e^{\lambda t}}
            \leq\e^{-\frac{(\mu-\lambda)^2}{2\sigma^2}}
        \end{align*}
    \end{proof}

\problem
\begin{question}
    Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$.
    Prove that for any real number $\lambda$
    \[P\Big(\omega; \frac{|X(\omega)-\mu|}{\sigma}\geq \lambda\Big)\leq \frac{1}{\lambda^2}.\]
    Remark: similar to the example I mentioned in lecture, only the case $\lambda>1$ is useful.
    In this case, the estimate shows that the probability that value of $X$ lies $\lambda \sigma$
    (units) away from its mean is bounded by $1/k^2$.
\end{question}

    \begin{proof}
        Note that $P(|X(\omega)-\mu|/\sigma\geq\lambda)
        =P(|X(\omega)-\mu|^2/\sigma^2\geq\lambda^2)$,
        we start from the definition of $E|X(\omega)-\mu|^2$,
        \begin{align*}
            E|X(\omega)-\mu|^2
            &\geq\int_{|X-\mu|\geq\lambda\sigma}|X-\mu|^2\diff P\\
            &\geq\int_{|X-\mu|\geq\lambda\sigma}\lambda^2\sigma^2\diff P\\
            &=\lambda^2\sigma^2 P(\omega;|X-\mu|\geq \lambda\sigma)
        \end{align*}

        Also note that $\mathrm{Var}(X):=E|X-EX|^2$,
        with $EX=\mu,\mathrm{Var}(X)=\sigma^2$ we have that
        \[E|X(w)-\mu|^2=\sigma^2\]
        It follows that
        \[P\left(\omega;\frac{|X(\omega)-\mu|}{\sigma}\geq\lambda\right)
        \leq\frac{1}{\lambda^2}\]
    \end{proof}

\problem\label{pb:a.s. implies in prob}
\begin{question}
    Let $\{X_n\}_{n=1}^\infty$ be a sequence of random variables. We now discuss the
    relationships between convergence almost surely and convergence in probability in
    the following problems.  We proved in the lecture that if $X_n\rightarrow X$ in
    probability, then there exists a subsequence $\{X_{n_j}\}$ that converges almost surely.
    First of all, we want to show that convergence almost surely implies convergence in probability.
    The proof is a little bit tricky and we divide it into the following steps.  Suppose that a.s.--$X_n\rightarrow X$,
    then the set ($\sigma$--field) that $\lim X_n\neq X$ has a probability zero, i.e.,
    for any $\epsilon>0$
    \[P(\omega;\lim_{n\rightarrow \infty} |X_n(\omega)-X(\omega)|\geq\epsilon)=0\]
    or \[P(\omega;\lim_{n\rightarrow \infty} |X_n(\omega)-X(\omega)|\leq\epsilon)=1,\]
    and we want to show that
    \[\lim_{i\rightarrow \infty }P(\omega;|X_n(\omega)-X(\omega)|\geq\epsilon)=0.\]
    Here we shall adopt the first definition.

    Let $\mathcal O=\{\omega;\lim X_n(\omega)\neq X(\omega)\}$, then $P(\mathcal O)=0$.
    Remark: $\mathcal O$ is not necessarily empty but its probability/measure is zero
    (think about the lengthy of a point but any singleton is not empty).

    step 1  For each $\epsilon>0$, let \[A_n:=\cup_{m\geq n}\{\omega;|X_m(\omega)-X(\omega)|\geq\epsilon\}.\]
    Prove that $\{A_n\}$ is decreasing in the sense that $A_j\subset A_i$ if $j\geq i$.

    step 2  According to probability theory (set theory), it is known that $\cap_{n\geq1} A_n$ exists and we denote
    \[A_\infty:=\cap_{n\geq1} A_n\]
    hence $P(A_n)\rightarrow P(A_\infty)$.  Let $\omega_0$ be any point in $\mathcal O$.  Show that $\omega_0$ does
    not belong to $A_n$ if $n$ is sufficiently large.

    step 3  Now we know that $\omega_0$ does belong to $A_\infty$.  Use this to show that
    $P(\omega;|X_n(\omega)-X(\omega)|\geq\epsilon)\rightarrow 0$
\end{question}

    \begin{proof}
        Denote $\mathcal A_m^\varepsilon:=\{\omega;|X_m(\omega)-X|\geq\varepsilon\}$,
        then $A_n=\bigcap_{m\geq n}\mathcal A_m^\varepsilon$.

        To prove the decreasing, consider any $\omega\in A_j$,
        there exists some $k\geq j$ such that $\omega\in\mathcal A_k^\varepsilon$.
        Since $i\leq j\leq k$, we have that $\omega\in\bigcup_{m\geq j}\mathcal A_m^\varepsilon$,
        i.e., $\omega\in A_i$ too, which is equivalent to $A_j\subset A_i$. Hence
        $A_n$ is decreasing.

        For any $\omega\in\mathcal O$, there exists some $\varepsilon$ s.t.
        for any $n>0$ there exists some $m\geq n$ s.t. $|X(\omega)-X|\geq\varepsilon$.
        It follows that for a given $\varepsilon$, and any $n$ there always
        exists $m\geq n$ s.t. $\omega\in\mathcal A_m^\varepsilon$, i.e.,
        this holds if $m$ ($n$) is suffienciently large.
        Alternatively this can be stated as
        \[\mathcal O=\bigcup_{\varepsilon>0}\bigcap_{n=1}^\infty
        \bigcup_{m\geq n}\mathcal A_m^\varepsilon
        =\bigcup_{\varepsilon>0}\mathcal A_m^\varepsilon\text{ i.o.}\]

        Since $X_n\xrightarrow{\mathrm{s.t.}}X$, we have $P(\mathcal O)=0$.
        Then we obtain $P(\mathcal A_m^\varepsilon\text{ i.o.})\leq
        P(\mathcal{O})=0$, i.e., $P(\mathcal A_m^\varepsilon\text{ i.o.})=0$.
        
        Also note that $\mathcal A_m^\varepsilon\text{ i.o.}=A_\infty$ since
        $A_n$ is monotoned. Then $\mathcal A_n^\varepsilon
        \subset A_n$ yields
        \[\lim_{n\to\infty}P(\mathcal A_n^\varepsilon)
        \leq\lim_{n\to\infty}P(A_n)=P(A_\infty)=0\]
        Thus we obtain that $X_n\xrightarrow{\mathrm{s.t.}}X$ as
        $n\to\infty$.
    \end{proof}

\problem
\begin{question}
    However, the opposite of the statement in Problem \ref{pb:a.s. implies in prob}
    is not true:  consider the following
    famous counter-example:

    Let $X_n$ be a random variable such that $P(X_n=1)=\frac{1}{n}$
    and $P(X_n=0)=1-\frac{1}{n}$.  Prove that $X_n\rightarrow X\equiv 0$
    in probability, but not almost surely.  Hint: The first is easy. 
    To show the latter, define as above
    \[A_n:=\cup_{m\geq n} \mathcal A^\epsilon_{m}:=\cup_{m\geq n}  \{w;|X_m(\omega)-X(\omega)|\geq \epsilon\}.\]
    Use the definition of convergence in probablity and Borel--Cantelli Lemma
    to show that $P(\mathcal A^\epsilon_{m}~i.o.)=0$.
    Then show that $X_n\not \rightarrow X$ almost surely.

    Remark: As we have proved in class, there is a subsequence of $X_n$
    such that $X_{n_j}\rightarrow X$ a.s.
\end{question}
    \subsection{Convergence In Probability}
    \begin{proof}
        For any $\delta\in(0,1]$, we have that $P(\omega;|X_n|\geq\delta)
        =1/n\to 0$ as $n\to\infty$. And for $\delta>1$, $P(\omega;|X_n|\geq\delta)
        =0$ which is trival. Therefore $X_n\xrightarrow{\mathrm{s.t.}}0$ as
        $n\to\infty$.
    \end{proof}

    \subsection{But Not a.s.}
    \begin{proof}
        Since $\sum P(X_n(\omega)=1)=\infty$, and $\{X_n=1\}$ is
        independent\sidenote{In my opinion the condition in question
        is not sufficient, so I add the independency.},
        then the B.C. second lemma yields 
        \[P\left(\limsup_{n\to\infty}\{\omega;X_n(\omega)=1\}\right)=1\]

        And for any $0<\varepsilon\leq 1$, we have that
        \[\{\omega;X_n(\omega)=1\}\subset\mathcal A_n^\varepsilon\]
        thus
        \[P\left(\limsup_{n\to\infty}\mathcal A_n^\varepsilon\right)\geq
        P\left(\limsup_{n\to\infty}\{\omega;X_n(\omega)=1\}\right)=1>0\]
        It follows that \sidenote{The set on which the sequence
        does not converge to 0 has probability 1 actually.}
        $X_n\not\to 0$.
    \end{proof}
