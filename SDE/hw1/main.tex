\documentclass{homework}

\title{Homework 1}

\begin{document}
    \maketitle

    \section{Problem 1}
    \begin{subproblem}
        \item
        \begin{proof}
            For $E(\e^{tX})$ we have that
            \begin{align*}
                E(\e^{tX})&\geq\int_{X(\omega)\geq\lambda}\e^{tX}\diff P\\
                &\geq\e^{\lambda t}\int_{X(\omega)\geq\lambda}\diff P\\
                &=\e^{\lambda t}P(\omega;X(\omega)\geq\lambda)
            \end{align*}
            Then we obtain
            \begin{equation}
                \label{eq:Chernoff bound}
                P(\omega;X(\omega)\geq\lambda)\leq\frac{E(\e^{tX})}{\e^{\lambda t}}
            \end{equation}

            Similarly we have\sidenote{It seems like there is something wrong in
            the questions. This inequality can not hold for an arbitrary random varibale
            $X$
            \[P(X\leq\lambda)\leq\frac{E(\e^{tX})}{\e^{\lambda t}}
            \leq P(X\geq\lambda)\]
            
            So I just proved another inequality which is probably the one you
            really mean to.}

            \begin{align*}
                E(\e^{-tX})&\geq\int_{X(\omega)\leq\lambda}\e^{-tX}\diff P\\
                &\geq\e^{-\lambda t}\int_{X(\omega)\leq\lambda}\diff P\\
                &=\e^{-\lambda t}P(\omega;X(\omega)\leq\lambda)
            \end{align*}
            and so,
            \[P(\omega;X(\omega)\leq\lambda)\leq\e^{\lambda t}E(\e^{-tX})\]
        \end{proof}

        \item
        \begin{proof}
            Since $\prod_i^n\e^{tX_i}=\e^{t\sum_i^n X_i}=\e^{tX}$,
            it falls to prove \cref{eq:Chernoff bound} at $t<0$.

            \emph{But I have no idea about how to achieve this. :(}
        \end{proof}
    \end{subproblem}

    \section{Problem 2}
    \begin{proof}
        Since $X\sim N(\mu,\sigma^2)$,
        denote $Y=(X-\mu)/\sigma\sim N(0,1)$, therefore
        the mgf of $X$ is
        \begin{align*}
            E\e^{tx}&=E\e^{t(\mu+\sigma Y)}\\
                    &=\e^{t\mu}\int_{-\infty}^\infty\e^{t\sigma x}
                      \cdot\frac{\e^{-\frac{x^2}2}}{\sqrt{2\pi}}\diff x\\
                    &=\e^{t\mu}\cdot\e^{\frac{t^2\sigma^2}{2}}\int_{-\infty}^{\infty}
                      \frac{1}{\sqrt{2\pi}}\e^{-\frac{(x-t\sigma)^2}{2}}
                      \diff x\\
                    &=\e^{t\mu+\frac{t^2\sigma^2}{2}}
        \end{align*}
        Hence
        \begin{align*}
            \frac{E\e^{tx}}{\e^{\lambda t}}&=\e^{t(\mu-\lambda)+\frac{t^2\sigma^2}{2}}\\
            &=\e^{\frac{\sigma^2}{2}\left(t+\frac{\mu-\lambda}{\sigma^2}\right)^2}
              \cdot\e^{-\frac{(\mu-\lambda)^2}{2\sigma^2}}\\
            &\leq\e^{-\frac{(\mu-\lambda)^2}{2\sigma^2}}
        \end{align*}

        Applying Chernoff bounds we obtain
        \begin{align*}
            P(w,X(w)\geq\lambda)&\leq\frac{E\e^{tx}}{\e^{\lambda t}}
            \leq\e^{-\frac{(\mu-\lambda)^2}{2\sigma^2}}
        \end{align*}
    \end{proof}

    \section{Problem 3}
    \begin{proof}
        Note that $P(|X(\omega)-\mu|/\sigma\geq\lambda)
        =P(|X(\omega)-\mu|^2/\sigma^2\geq\lambda^2)$,
        we start from the definition of $E|X(\omega)-\mu|^2$,
        \begin{align*}
            E|X(\omega)-\mu|^2
            &\geq\int_{|X-\mu|\geq\lambda\sigma}|X-\mu|^2\diff P\\
            &\geq\int_{|X-\mu|\geq\lambda\sigma}\lambda^2\sigma^2\diff P\\
            &=\lambda^2\sigma^2 P(\omega;|X-\mu|\geq \lambda\sigma)
        \end{align*}

        Also note that $\mathrm{Var}(X):=E|X-EX|^2$,
        with $EX=\mu,\mathrm{Var}(X)=\sigma^2$ we have that
        \[E|X(w)-\mu|^2=\sigma^2\]
        It follows that
        \[P\left(\omega;\frac{|X(\omega)-\mu|}{\sigma}\geq\lambda\right)
        \leq\frac{1}{\lambda^2}\]
    \end{proof}

    \section{Problem 4}
    \begin{proof}
        Denote $\mathcal A_m^\varepsilon:=\{\omega;|X_m(\omega)-X|\geq\varepsilon\}$,
        then $A_n=\bigcap_{m\geq n}\mathcal A_m^\varepsilon$.

        To prove the decreasing, consider any $\omega\in A_j$,
        there exists some $k\geq j$ such that $\omega\in\mathcal A_k^\varepsilon$.
        Since $i\leq j\leq k$, we have that $\omega\in\bigcup_{m\geq j}\mathcal A_m^\varepsilon$,
        i.e., $\omega\in A_i$ too, which is equivalent to $A_j\subset A_i$. Hence
        $A_n$ is decreasing.

        For any $\omega\in\mathcal O$, there exists some $\varepsilon$ s.t.
        for any $n>0$ there exists some $m\geq n$ s.t. $|X(\omega)-X|\geq\varepsilon$.
        It follows that for a given $\varepsilon$, and any $n$ there always
        exists $m\geq n$ s.t. $\omega\in\mathcal A_m^\varepsilon$, i.e.,
        this holds if $m$ ($n$) is suffienciently large.
        Alternatively this can be stated as
        \[\mathcal O=\bigcup_{\varepsilon>0}\bigcap_{n=1}^\infty
        \bigcup_{m\geq n}\mathcal A_m^\varepsilon
        =\bigcup_{\varepsilon>0}\mathcal A_m^\varepsilon\text{ i.o.}\]

        Since $X_n\xrightarrow{\mathrm{s.t.}}X$, we have $P(\mathcal O)=0$.
        Then we obtain $P(\mathcal A_m^\varepsilon\text{ i.o.})\leq
        P(\mathcal{O})=0$, i.e., $P(\mathcal A_m^\varepsilon\text{ i.o.})=0$.
        
        Also note that $\mathcal A_m^\varepsilon\text{ i.o.}=A_\infty$ since
        $A_n$ is monotoned. Then $\mathcal A_n^\varepsilon
        \subset A_n$ yields
        \[\lim_{n\to\infty}P(\mathcal A_n^\varepsilon)
        \leq\lim_{n\to\infty}P(A_n)=P(A_\infty)=0\]
        Thus we obtain that $X_n\xrightarrow{\mathrm{s.t.}}X$ as
        $n\to\infty$.
    \end{proof}

    \section{Problem 5}
    \subsection{Convergence In Probability}
    \begin{proof}
        For any $\delta\in(0,1]$, we have that $P(\omega;|X_n|\geq\delta)
        =1/n\to 0$ as $n\to\infty$. And for $\delta>1$, $P(\omega;|X_n|\geq\delta)
        =0$ which is trival. Therefore $X_n\xrightarrow{\mathrm{s.t.}}0$ as
        $n\to\infty$.
    \end{proof}

    \subsection{But Not a.s.}
    \begin{proof}
        Since $\sum P(X_n(\omega)=1)=\infty$, and $\{X_n=1\}$ is
        independent\sidenote{In my opinion the condition in question
        is not sufficient, so I add the independency.},
        then the B.C. second lemma yields 
        \[P\left(\limsup_{n\to\infty}\{\omega;X_n(\omega)=1\}\right)=1\]

        And for any $0<\varepsilon\leq 1$, we have that
        \[\{\omega;X_n(\omega)\}\subset\mathcal A_n^\varepsilon\]
        thus
        \[P\left(\limsup_{n\to\infty}\mathcal A_n^\varepsilon\right)\geq
        P\left(\limsup_{n\to\infty}\{\omega;X_n(\omega)=1\}\right)=1>0\]
        It follows that \sidenote{The set on which the sequence
        does not converge to 0 has probability 1 actually.}
        $X_n\not\to 0$.
    \end{proof}
\end{document}
