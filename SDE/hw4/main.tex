\documentclass{homework}

\title{Homework 4}

\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}

\begin{document}
    \maketitle

    \problem
    Since
    \[\begin{aligned}
        \var(X_t)&=E(X_t^2)-(E(X_t))^2\\
        &=E\left\{\left(\int_0^tW_s\diff s\right)
        \left(\int_0^tW_r\diff r\right)\right\}\\
        &=E\left(\int_0^t\int_0^tW_sW_r\diff s\diff r\right)\\
        &=\int_\Omega\left(\iint_{[0,t]^2}W_sW_r\diff s\diff r\right)\diff P
    \end{aligned}\]
    then we apply Fubini theorem again,
    \[\var(X_t)=\iint_{[0,t]^2}
    \left(\int_\Omega W_sW_r\diff P\right)\diff s\diff r\]
    Without loss of generality, assume that $s<r$, then we obtain
    by inpendent increments of Brownian motion,
    \[\begin{aligned}
        \int_\Omega W_sW_r\diff P&=E(W_sW_r)\\
        &=E(W_s(W_r-W_s+W_s))\\
        &=E(W_s(W_r-W_s))+E(W_s^2)\\
        &=s
    \end{aligned}\]
    thus in general,
    \[\int_\Omega W_sW_r\diff P=\min\{s,r\}\]
    It follows that
    \[\begin{aligned}
        \var(X_t)&=\iint_{[0,t]^2}\min\{s,r\}\diff s\diff r\\
        &=\left(\iint_{\{(s,r)\in[0,t]^2;s\leq r\}}
                +\iint_{\{(s,r)\in[0,t]^2;s\leq r\}}\right)
        \min\{s,r\}\diff s\diff r\\
    \end{aligned}\]
    and by symmetry, we obtain the variance as
    \[\begin{aligned}
        \var(X_t)&=2\iint_{\{(s,r)\in[0,t]^2;s\leq r\}}s\diff s\diff r\\
        &=\int_0^t\diff r\int_0^r2s\diff s\\
        &=\frac{t^3}{3}
    \end{aligned}\]

    \problem
    % TODO Problem 2

    \problem
    By Fubini theorem, calculating $E(Y_t)$ falls to
    calculate $\int_0^tE(\e^{W_s})\diff s$ as
    \[\begin{aligned}
    E(Y_t)&=\int_\Omega\left(\int_0^t\e^{W_s}\diff s\right)\diff P\\
    &=\int_0^t\left(\int_\Omega\e^{W_s}\diff P\right)\diff s\\
    &=\int_0^tE(\e^{W_s})\diff s \\
    \end{aligned}\]
    And to the convenience of following calculation, consider moment
    generating function of $W_s$,
    \begin{equation}
        \label{eq:mgf}
        \begin{aligned}
        E(\e^{uW_s})&=\int_{-\infty}^\infty\e^{ux}\cdot
        \frac{\e^{-\frac{x^2}{2s}}}{\sqrt{2\pi s}}\diff x\\
        &=\int_{-\infty}^\infty\e^{\frac{u^2s}{2}}
        \cdot\frac{\e^{-\frac{(x-us)^2}{2s}}}{\sqrt{2\pi s}}\diff x\\
        &=\int_{-\infty}^\infty\e^{\frac{u^2s}{2}}
        \cdot\frac{\e^{-\frac{\xi^2}{2s}}}{\sqrt{2\pi s}}\diff\xi\\
        &=\e^{\frac{u^2s}{2}}
        \end{aligned}
    \end{equation}
    where we used substitution of $\xi=x-us$.

    Therefore,
    \[\begin{aligned}
        E(\e^{W_s})&=\e^{\frac{s}{2}}\\
        E(\e^{2W_s})&=\e^{2s}\\
    \end{aligned}\]
    Thus,
    \[\begin{aligned}
        E(Y_t)&=\int_0^tE(\e^{W_s})\diff s\\
        &=\int_0^t\e^{\frac{s}{2}}\diff s\\
        &=2(\e^{\frac{t}{2}}-1)
    \end{aligned}\]
    and similarly,
    \[\begin{aligned}
        E(Y_t^2)&=\int_0^tE(\e^{2W_s})\diff s\\
        &=\int_0^t\e^{2s}\diff s\\
        &=\frac{\e^{2t}-1}{2}
    \end{aligned}\]
    hence the variance,
    \[\begin{aligned}
        \var(Y_t)&=E(Y_t^2)-(E(Y_t))^2\\
        &=\frac{\e^{2t}-1}{2}-4(\e^{\frac{t}{2}}-1)^2\\
        &=\frac{\e^{2t}}{2}-4\e^t+8\e^{\frac{t}{2}}-\frac{9}{2}
    \end{aligned}\]

    \problem

    \problem
    \begin{subproblem}
        \item
        Without loss of generality, assume that $s<t$.
        Since $EX_t=0$, then we have that
        \[\begin{aligned}
            \cov(X_t,X_s)&=E(X_tX_s)\\
        \end{aligned}\]
        Let $\mathcal F_\cdot=(\mathcal F_t)_{t>0}$ be
        the natural filtration of Brownian motion
        $W_t$. Consider that
        \begin{equation}
            \label{eq:p5 condi exp}
            E(X_t(W_s-sW_1)|\mathcal F_s)
            =E(W_sX_t|\mathcal F_s)
            -s\cdot E(W_1X_t|\mathcal F_s)
        \end{equation}
        The first term
        \[E(W_sX_t|\mathcal F_s)=W_s\cdot E(X_t|\mathcal F_s)
        =W_s\cdot E(W_t-tW_1|\mathcal F_s)=(1-t)W_s^2\]
        And the last term
        \begin{equation*}
            \begin{aligned}
                E(W_1X_t|\mathcal F_s)
                =E(W_1(W_t-tW_1)|\mathcal F_s)
                =E(W_1W_t|\mathcal F_s)-tE(W_1^2|\mathcal F_s)
            \end{aligned}
        \end{equation*}
        Since inpendent increments of $W_t$ implying that
        \[E(W_1W_t)=E(W_t(W_1-W_t+W_t))=E(W_t)\cdot E(W_1-W_t)+E(W_t^2)=t\]
        then the expectation of last term of \cref{eq:p5 condi exp}
        is obtained as
        \[E(E(W_1X_t|\mathcal F_s))=E(W_1W_t)-tE(W_1^2)=t-t=0\]
        Therefore by tower rule,
        \[\begin{aligned}
            E(X_tX_s)&=E(E(W_sX_t|\mathcal F_s))-s\cdot E(E(W_1X_t|\mathcal F_s))\\
            &=(1-t)E(W_s^2)\\
            &=s(1-t)
        \end{aligned}\]
        so in general,
        \begin{equation}
            \label{eq:p5 cov}
            \cov(X_t,X_s)=\min\{t,s\}-st
        \end{equation}

        \item
        Since \cref{eq:p5 cov} yields that
        \[\var(X_t)=\cov(X_t,X_t)=t(1-t)\]
        and by linearity of normal distribution, we have that
        \[X_t\sim \mathcal N(0,t(1-t))\]
        thus
        \[\left(\frac{X_t}{\sqrt{t(1-t)}}\right)^2
        =\frac{X_t^2}{t(1-t)}\sim\chi^2(1)\]
        hence
        \[\begin{aligned}
            E\left(\frac{X_t^2}{t(1-t)}\right)&=1\\
            \var\left(\frac{X_t^2}{t(1-t)}\right)&=2\\
        \end{aligned}\]
        i.e.,
        \[\begin{aligned}
            E(Y_t)&=E(X_t^2)=t(1-t)\\
            \var(Y_t)&=\var(X_t^2)=2t^2(1-t)^2
        \end{aligned}\]
    \end{subproblem}
    
    \problem
    \begin{subproblem}
        \item
        By Jensen's inequality, we have that
        \[E(|X_n-X|^2)\geq (E(|X_n-X|))^2\geq |E(X_n-X)|^2\]
        It follows that 
        \[\lim_{n\to\infty}E(|X_n-X|^2)=0\]
        yields
        \[\begin{aligned}
            \lim_{n\to\infty}E(X_n-X)&=0\\
        \end{aligned}\]
        i.e.,
        \[\lim_{n\to\infty}E(X_n)=E(X)\]
        And by Minkowski inequality,
        \[\begin{aligned}
            \sqrt{E(|X_n|^2)}&\leq\sqrt{E(|X_n-X|^2)}+\sqrt{E(|X|^2)}\\
            \sqrt{E(|X|^2)}&\leq\sqrt{E(|X-X_n|^2)}+\sqrt{E(|X_n|^2)}\\
        \end{aligned}\]
        i.e.,
        \[\left|\sqrt{E(|X_n|^2)-E(|X|^2)}\right|
        \leq\sqrt{E(|X_n-X|^2)}\]
        so m.s. $X_n\to X$ implies
        \[E(|X_n|^2)\to E(|X|^2)\]

        Finally since
        \[\begin{aligned}
            \var(X_n)&=E(X_n^2)-(E(X_n))^2\\
            \var(X)&=E(X^2)-(E(X))^2
        \end{aligned}\]
        we have that
        \[\var(X_n)\to\var(X)\quad(n\to\infty)\]

        \item
        Note
        \[E(|X_n-X|^2)=E(X_n^2)-2E(X_nX)+E(X^2)\]
        implies that
        \[\lim_{n\to\infty}E(X_nX)
        =\frac{1}{2}\lim_{n\to\infty}\left(E(X_n^2)+E(X^2)+E(|X_n-X|^2)\right)
        =E(X^2)=0\]
        as
        \[\lim_{n\to\infty}E(X_n^2)=E(X^2)
        \text{ and }
        \lim_{n\to\infty}E(|X_n^2-X^2|)=0\]
        therefore
        \[\begin{aligned}
            \cov(X_n,X)&=E(X_nX)-E(X_n)\cdot E(X)\\
            &\to E(X^2)-(E(X))^2=\cov(X,X)\quad(n\to\infty)
        \end{aligned}\]
    \end{subproblem}

    \problem
    \begin{subproblem}
        \item
        \begin{proof}
            For any $\sigma>0$, let
            $A_\sigma=\{\omega\in\Omega;|X_n(\omega)-X(\omega)|\geq\sigma\}$,
            then
            \[\begin{aligned}
                E(|X_n-X|^2)&=\int_\Omega|X_n-X|^2\diff P\\
                &\geq\int_{A_\sigma}|X_n-X|^2\diff P\\
                &\geq \sigma^2\int_{A_\sigma}\diff P\\
                &=\sigma^2P(A_\sigma)
            \end{aligned}\]
            It follows that
            \[\lim_{n\to\infty}P(A_\sigma)
            =\frac{1}{\sigma^2}\lim_{n\to\infty}E(|X_n-X|^2)
            =0\]
            as m.s. $X_n\to X$. Therefore, $X_n\to X$ in probability.
        \end{proof}

        \item
        Consider R.V. $X_n$ s.t. $P(X_n=0)=1-1/n^2$ and $P(X_n=n)=1/n^2$.
        And it is evident that
        \[\begin{aligned}
            E(|X_n-X|)&=\frac{1}{n}\to 0\\
            E(|X_n-X|^2)&=1\not\to 0
        \end{aligned}\quad(n\to\infty)\]
        where $X=0$ a.e.

        \item
        \begin{proof}
            For any $p>0$, the notation $A_\sigma$ is the same
            as the one in problem (i), and similarly,
            \[\begin{aligned}
                E(|X_n-X|^p)&=\int_\Omega|X_n-X|^p\diff P\\
                &\geq\int_{A_\sigma}|X_n-X|^p\diff P\\
                &\geq \sigma^p\int_{A_\sigma}\diff P\\
                &=\sigma^pP(A_\sigma)
            \end{aligned}\]
            Then $X_n\to X$ in $p$-th norm implies convergence in
            probability in the same manner as
            \[\lim_{n\to\infty}P(A_\sigma)
            =\frac{1}{\sigma^p}\lim_{n\to\infty}E(|X_n-X|^p)
            =0\]
        \end{proof}
    \end{subproblem}
\end{document}