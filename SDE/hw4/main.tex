\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}

    \problem
    \begin{question}
        Let $X_t=\int_0^t W_s ds$.  We have showed in class that
        $X_t\sim N(0,\frac{t^3}{3})$ (except the rigourous convergence
        of the random variable to $X_t$).  Indeed, to find $E(W_t)$
        alternatively, it is easy to see that
        \[E(X_t)=E\Big(\int_0^t W_s ds\Big)
        =\int_\Omega \int_0^t W_s dsdP
        \overset{Fubini}{=}\int_0^t \int_\Omega W_s dPds
        =\int_0^t E(W_s)ds=\int_0^t0ds=0,\]
        because here $t$ is a constant and integration is a
        linear operation.  Similarly, we can also use straightforward
        calculations to evaluate Var$(X_t)$ as follows
        \begin{align*}
            Var(X_t)= &E(X^2_t)-E^2(X_t)  \\
            =&E\Big\{\Big(\int_0^t W_sds\Big)\Big(\int_0^t W_rdr\Big)\Big\} \\
            \overset{\text{Fubini}}{=}&E \Big(\int_0^t\int_0^t W_s W_rdsdr\Big)... \\
        \end{align*}
        Now you can start from here to find Var$(X_t)$. We would like
        to remark that by straightforward calculations, one may find all
        kinds of statistics of $X_t$, such as mean, variance, kurtosis etc,
        however we can not tell if it is normally distributed as we showed in class.
    \end{question}
    Since
    \[\begin{aligned}
        \var(X_t)&=E(X_t^2)-(E(X_t))^2\\
        &=E\left\{\left(\int_0^tW_s\diff s\right)
        \left(\int_0^tW_r\diff r\right)\right\}\\
        &=E\left(\int_0^t\int_0^tW_sW_r\diff s\diff r\right)\\
        &=\int_\Omega\left(\iint_{[0,t]^2}W_sW_r\diff s\diff r\right)\diff P
    \end{aligned}\]
    then we apply Fubini theorem again,
    \[\var(X_t)=\iint_{[0,t]^2}
    \left(\int_\Omega W_sW_r\diff P\right)\diff s\diff r\]
    Without loss of generality, assume that $s<r$, then we obtain
    by independent increments of Brownian motion,
    \[\begin{aligned}
        \int_\Omega W_sW_r\diff P&=E(W_sW_r)\\
        &=E(W_s(W_r-W_s+W_s))\\
        &=E(W_s(W_r-W_s))+E(W_s^2)\\
        &=s
    \end{aligned}\]
    thus in general,
    \[\int_\Omega W_sW_r\diff P=\min\{s,r\}\]
    It follows that
    \[\begin{aligned}
        \var(X_t)&=\iint_{[0,t]^2}\min\{s,r\}\diff s\diff r\\
        &=\left(\iint_{\{(s,r)\in[0,t]^2;s\leq r\}}
                +\iint_{\{(s,r)\in[0,t]^2;s\leq r\}}\right)
        \min\{s,r\}\diff s\diff r\\
    \end{aligned}\]
    and by symmetry, we obtain the variance as
    \[\begin{aligned}
        \var(X_t)&=2\iint_{\{(s,r)\in[0,t]^2;s\leq r\}}s\diff s\diff r\\
        &=\int_0^t\diff r\int_0^r2s\diff s\\
        &=\frac{t^3}{3}
    \end{aligned}\]

    \problem
    \begin{question}
        Let us revisit the integrated Brownian motion by choosing a different sample point, say
        \[\int_0^t W_sds=m.s.-\lim_{N\rightarrow \infty} \sum^N_{i=1}W_{t_i}\Delta t,\]
        under equidistant partition.  What is the limit now? What is the limit
        \[\lim_{N\rightarrow \infty} \sum^N_{i=1}W_{t^*_i}\Delta t\]
        if we choose any sample point $t_i^*\in(t_i,t_{i+1})$,
        or $t^*_i=\lambda t_i+(1-\lambda)t_{i+1}$, $\lambda \in (0,1)$. 
        Does the limit depend on the choice of the sample point (compared to what I said in class)?
    \end{question}
    No matter what the partition is\sidenote{I have no idea about
    the convergence in mean square, since the limit itself is not
    friendly to me, and I just thinked about the
    limit distribution, under a not so specific partition.}
    , we can always write
    \[\begin{aligned}
        W_{t^*_1}&=W_{t^*_1},\\
        W_{t^*_2}&=W_{t^*_2}-W_{t^*_1}+W_{t^*_1}\\
        &\cdots\\
        W_{t^*_N}&=W_{t^*_N}-W_{t^*_{N-1}}+\cdots+W_{t^*_1}
    \end{aligned}\]
    hence
    \[\sum_{i=1}^NW_{t^*_i}\Delta t=\sum_{i=1}^NiZ_{n-i}\]
    where
    \[Z_i=\begin{cases}
        W_{t^*_1},&i=1\\
        W_{t^*_i}-W_{t^*_{i-1}},&i>1\\
    \end{cases}\]
    thus $Z_i$ is mutually independent.
    That being said
    \[\lim_{N\to\infty}\sum_{i=1}^N W_{t_i^*}\Delta t\]
    is normally distributed, so all we care is the expectation
    and variance. While the expectation is 0 apparently,
    \[\var\left(\sum_{i=1}^N W_{t_i^*}\Delta t\right)
    =\Delta t^2\sum_{i=1}^Ni^2\var(Z_{n-i})
    =\Delta t^2\sum_{i=1}^Ni^2\Delta t_i\]
    where $\Delta t_i=t^*_i-t^*_i$.
    Since $x^2$ is Riemann-integrable, then for any point $t_i^*$
    we choose, the limit is the same, the same as what we obtained
    in class, i.e., $t^3/3$, where we chosed the end points under
    equidistant partition.

    \problem
    \begin{question}
        Let $Y_t=\int_0^t e^{W_s}ds$ be the integral of a GBM.
        Find $E(Y_t)$ and Var$(Y_t)$.
    \end{question}
    By Fubini theorem, calculating $E(Y_t)$ falls to
    calculate $\int_0^tE(\e^{W_s})\diff s$ as
    \[\begin{aligned}
    E(Y_t)&=\int_\Omega\left(\int_0^t\e^{W_s}\diff s\right)\diff P\\
    &=\int_0^t\left(\int_\Omega\e^{W_s}\diff P\right)\diff s\\
    &=\int_0^tE(\e^{W_s})\diff s \\
    \end{aligned}\]
    And to the convenience of following calculation, consider moment
    generating function of $W_s$,
    \begin{equation}
        \label{eq:mgf h4}
        \begin{aligned}
        E(\e^{uW_s})&=\int_{-\infty}^\infty\e^{ux}\cdot
        \frac{\e^{-\frac{x^2}{2s}}}{\sqrt{2\pi s}}\diff x\\
        &=\int_{-\infty}^\infty\e^{\frac{u^2s}{2}}
        \cdot\frac{\e^{-\frac{(x-us)^2}{2s}}}{\sqrt{2\pi s}}\diff x\\
        &=\int_{-\infty}^\infty\e^{\frac{u^2s}{2}}
        \cdot\frac{\e^{-\frac{\xi^2}{2s}}}{\sqrt{2\pi s}}\diff\xi\\
        &=\e^{\frac{u^2s}{2}}
        \end{aligned}
    \end{equation}
    where we used substitution of $\xi=x-us$.

    Therefore,
    \[\begin{aligned}
        E(\e^{W_s})&=\e^{\frac{s}{2}}\\
        E(\e^{2W_s})&=\e^{2s}\\
    \end{aligned}\]
    Thus,
    \[\begin{aligned}
        E(Y_t)&=\int_0^tE(\e^{W_s})\diff s\\
        &=\int_0^t\e^{\frac{s}{2}}\diff s\\
        &=2(\e^{\frac{t}{2}}-1)
    \end{aligned}\]
    and similarly,
    \[\begin{aligned}
        E(Y_t^2)&=\int_0^tE(\e^{2W_s})\diff s\\
        &=\int_0^t\e^{2s}\diff s\\
        &=\frac{\e^{2t}-1}{2}
    \end{aligned}\]
    hence the variance,
    \[\begin{aligned}
        \var(Y_t)&=E(Y_t^2)-(E(Y_t))^2\\
        &=\frac{\e^{2t}-1}{2}-4(\e^{\frac{t}{2}}-1)^2\\
        &=\frac{\e^{2t}}{2}-4\e^t+8\e^{\frac{t}{2}}-\frac{9}{2}
    \end{aligned}\]

    \problem
    \begin{question}
        Consider the following stochastic process
        \[Y_t=e^{\int_0^t W_sds}.\]
        Find $E(Y_t)$, Var$(Y_t)$ and Cov$(Y_s,Y_t)$. 
        Hint:  We know that $\int_0^t W_sds\sim N(0,\frac{t^3}{3})$.
    \end{question}
    Denote
    \[X_t=\int_0^tW_\xi\diff\xi\]
    then
    \[X_t\sim\mathcal N\left(0,\frac{t^3}{3}\right)\]
    By the integration by parts formula\sidenote{In my opinion,
    only knowing the distribution of
    $X_t$ is not sufficient to calculate $\cov(X_t,X_s)$. So
    I turn to the definition of $X_t$, and did some deduction
    formally -- not so rigorously,
    e.g. the notation $\diff W_\xi$ is not well
    defined, and I just considered it as an increment like the
    discrete version in the Riemann sum.},
    \[\begin{aligned}
        X_t&=\left.\xi W_\xi\right|^t_0-\int_0^t\xi\diff W_\xi\\
        &=tW_t-\int_0^t\xi\diff W_\xi\\
        &=t\int_0^t\diff W_\xi-\int_0^t\xi\diff W_\xi\\
        &=\int_0^t(t-\xi)\diff W_\xi
    \end{aligned}\]
    That being said, $X_t$ is a sumation of independent increments
    $\diff W_\xi$ with $\xi\in[0,t]$. So we can see that $X_t$
    also has independent increments as
    $X_{t+s}-X_t=\int_t^{t+s}W_\xi\diff\xi$
    is only the
    sumation of $\diff W_\xi$ between $[t,t+s]$.

    Without loss of generality, assume $s<t$,
    therefore,
    \[\var(X_t)=\var(X_t-X_s)+\var(X_s)\]
    which yields
    \[\var(X_t-X_s)=\var(X_t)-\var(X_s)=\frac{t^3-s^3}{3}\]
    Since we know that $X_t,X_s$ are normally distributioned,
    with zero expectation, then
    \[X_t-X_s\sim\mathcal N\left(0,\frac{t^3-s^3}{3}\right)\]
    So in the light of the mgf stated in \cref{eq:mgf h4}, we have that
    \[E(\e^{X_t-X_s})=\e^{\frac{1}{2}\cdot\frac{t^3-s^3}{3}}
    =\e^{\frac{t^3-s^3}{6}}\]
    and
    \[E(\e^{kX_s})=\e^{\frac{k^2}{2}\cdot\frac{s^3}{3}}
    =\e^{\frac{k^2s^3}{6}}\]
    thus
    \[E(Y_t)=E(\e^{X_t})=\e^{\frac{t^3}{6}}\]
    And with independent increments,
    \[E(Y_tY_s)=E(\e^{X_t-X_s+2X_s})
    =E(\e^{X_t-X_s})\cdot E(\e^{2X_s})
    =\e^{\frac{t^3+3s^3}{6}}\]
    So
    \[\begin{aligned}
        \cov(Y_t,Y_s)&=E(Y_tY_s)-E(Y_t)\cdot E(Y_s)\\
        &=\e^{\frac{t^3+3s^3}{6}}-\e^{\frac{t^3}{6}}\cdot\e^\frac{s^3}{6}\\
        % &=\e^{\frac{t^3}{6}}\left(\e^{-\frac{s^3}{2}}-\e^{\frac{s^3}{6}}\right)
        &=\e^{\frac{t^3+s^3}{6}}\left(\e^{\frac{s^3}{3}}-1\right)
    \end{aligned}\]
    and in general,
    \[\cov(Y_t,Y_s)=\e^{\frac{t^3+s^3}{6}}
    \left(\e^{\frac{\min\{t,s\}^3}{3}}-1\right)\]
    then
    \[\var(Y_t)=\cov(Y_t,Y_t)
    =\e^{\frac{t^3}{3}}\left(\e^{\frac{t^3}{3}}-1\right)\]

    \problem
    \begin{question}
        Let $X_t=W_t-tW_1$, $t\in[0,1]$, be a Brownian Bridge.
        (i)  Find Cov$(X_s,X_t)$, $s,t\in[0,1]$;
        (ii)  Let $Y_t=X^2_t$.  Find $E(Y_t)$ and Var$(Y_t)$.
    \end{question}
    \begin{subproblem}
        \item
        Without loss of generality, assume that $s<t$.
        Since $EX_t=0$, then we have that
        \[\begin{aligned}
            \cov(X_t,X_s)&=E(X_tX_s)\\
        \end{aligned}\]
        Let $\mathcal F_\cdot=(\mathcal F_t)_{t>0}$ be
        the natural filtration of Brownian motion
        $W_t$. Consider that
        \begin{equation}
            \label{eq:p5 condi exp}
            E(X_t(W_s-sW_1)|\mathcal F_s)
            =E(W_sX_t|\mathcal F_s)
            -s\cdot E(W_1X_t|\mathcal F_s)
        \end{equation}
        The first term
        \[E(W_sX_t|\mathcal F_s)=W_s\cdot E(X_t|\mathcal F_s)
        =W_s\cdot E(W_t-tW_1|\mathcal F_s)=(1-t)W_s^2\]
        And the last term
        \begin{equation*}
            \begin{aligned}
                E(W_1X_t|\mathcal F_s)
                =E(W_1(W_t-tW_1)|\mathcal F_s)
                =E(W_1W_t|\mathcal F_s)-tE(W_1^2|\mathcal F_s)
            \end{aligned}
        \end{equation*}
        Since independent increments of $W_t$ implying that
        \[E(W_1W_t)=E(W_t(W_1-W_t+W_t))=E(W_t)\cdot E(W_1-W_t)+E(W_t^2)=t\]
        then the expectation of last term of \cref{eq:p5 condi exp}
        is obtained as
        \[E(E(W_1X_t|\mathcal F_s))=E(W_1W_t)-tE(W_1^2)=t-t=0\]
        Therefore by tower rule,
        \[\begin{aligned}
            E(X_tX_s)&=E(E(W_sX_t|\mathcal F_s))-s\cdot E(E(W_1X_t|\mathcal F_s))\\
            &=(1-t)E(W_s^2)\\
            &=s(1-t)
        \end{aligned}\]
        so in general,
        \begin{equation}
            \label{eq:p5 cov}
            \cov(X_t,X_s)=\min\{t,s\}-st
        \end{equation}

        \item
        Since \cref{eq:p5 cov} yields that
        \[\var(X_t)=\cov(X_t,X_t)=t(1-t)\]
        and by linearity of normal distribution, we have that
        \[X_t\sim \mathcal N(0,t(1-t))\]
        thus
        \[\left(\frac{X_t}{\sqrt{t(1-t)}}\right)^2
        =\frac{X_t^2}{t(1-t)}\sim\chi^2(1)\]
        hence
        \[\begin{aligned}
            E\left(\frac{X_t^2}{t(1-t)}\right)&=1\\
            \var\left(\frac{X_t^2}{t(1-t)}\right)&=2\\
        \end{aligned}\]
        i.e.,
        \[\begin{aligned}
            E(Y_t)&=E(X_t^2)=t(1-t)\\
            \var(Y_t)&=\var(X_t^2)=2t^2(1-t)^2
        \end{aligned}\]
    \end{subproblem}
    
    \problem
    \begin{question}
        Suppose that m.s.-$X_n\rightarrow X$ as $n\rightarrow \infty$
        and $X$ is square integrable.  (i)  Show that
        Var$(X_n)\rightarrow$Var$(X)$ as $n\rightarrow \infty$.
        (ii) Show that Cov$(X_n,X)\rightarrow$Cov$(X,X)$.
        Hint:  You may need to first prove $X_n\rightarrow X$ in mean by Jensen's inequality.
    \end{question}
    \begin{subproblem}
        \item
        By Jensen's inequality, we have that
        \[E(|X_n-X|^2)\geq (E(|X_n-X|))^2\geq |E(X_n-X)|^2\]
        It follows that 
        \[\lim_{n\to\infty}E(|X_n-X|^2)=0\]
        yields
        \[\begin{aligned}
            \lim_{n\to\infty}E(X_n-X)&=0\\
        \end{aligned}\]
        i.e.,
        \[\lim_{n\to\infty}E(X_n)=E(X)\]
        And by Minkowski inequality,
        \[\begin{aligned}
            \sqrt{E(|X_n|^2)}&\leq\sqrt{E(|X_n-X|^2)}+\sqrt{E(|X|^2)}\\
            \sqrt{E(|X|^2)}&\leq\sqrt{E(|X-X_n|^2)}+\sqrt{E(|X_n|^2)}\\
        \end{aligned}\]
        i.e.,
        \[\left|\sqrt{E(|X_n|^2)}-\sqrt{E(|X|^2)}\right|
        \leq\sqrt{E(|X_n-X|^2)}\]
        so m.s. $X_n\to X$ implies
        \[\sqrt{E(|X_n|^2)}\to\sqrt{E(|X|^2)}\]
        as well as
        \[E(|X_n|^2)\to E(|X|^2)\]
        because of the continuity of $f(x)=x^2$.

        Finally since
        \[\begin{aligned}
            \var(X_n)&=E(X_n^2)-(E(X_n))^2\\
            \var(X)&=E(X^2)-(E(X))^2
        \end{aligned}\]
        we have that
        \[\var(X_n)\to\var(X)\quad(n\to\infty)\]

        \item
        Note
        \[E(|X_n-X|^2)=E(X_n^2)-2E(X_nX)+E(X^2)\]
        implies that
        \[\lim_{n\to\infty}E(X_nX)
        =\frac{1}{2}\lim_{n\to\infty}\left(E(X_n^2)+E(X^2)+E(|X_n-X|^2)\right)
        =E(X^2)=0\]
        as
        \[\lim_{n\to\infty}E(X_n^2)=E(X^2)
        \text{ and }
        \lim_{n\to\infty}E(|X_n^2-X^2|)=0\]
        therefore
        \[\begin{aligned}
            \cov(X_n,X)&=E(X_nX)-E(X_n)\cdot E(X)\\
            &\to E(X^2)-(E(X))^2=\cov(X,X)\quad(n\to\infty)
        \end{aligned}\]
    \end{subproblem}

    \problem
    \begin{question}
        The following problems are to provide a list of important relationships
        between the different notions of convergence of random variables.
        You can prove them by definitions.

        \noindent(i)  If m.s.-$X_n\rightarrow X$, then s.t. $X_n\rightarrow X$.
        (convergence in mean/strong convergence implies convergence in probability)

        \noindent(ii)  We know that convergence in mean square implies convergence
        in mean.  However the oppositive is not true.  Give a counter--example,
        either discrete or continuous R.V.

        \noindent(iii)  Show that in (i), convergence in mean is enough,
        i.e., if $X_n\rightarrow X$ in mean, then s.t. $X_n\rightarrow X$.
        Indeed you can even show that $X_n\rightarrow X$ in $p$--th norm
        for any $p>0$ is sufficient.

        \noindent(iv) It is known (from our previous homework, or you go to show)
        that $X_t=e^{W_t-\frac{t}{2}}$ is a martingale and $E(|X_t|)=1$.
        Then the \emph{Doob's Martingale Convergence Theorem} implies that
        there exists $L$ such that a.c.--$X_t\rightarrow L$.  Of course it natural
        to guess that $L=1$ because each $|X_t|$ has mean $1$ and this guess
        is indeed correct, i.e., a.c.--$X_t\rightarrow 1$.  Show that $X_t$
        does not converge to $1$ in mean square.  This serves as an example
        of convergence with probability 1/almost surely but not in mean square.

        \noindent(v).  There are some very famous examples which show that
        convergence in mean square does not imply the convergence with probability
        1/almost surely (when you say something is famous or celebrated usually
        it is nontrivial to construct one).  You can go to read this part yourself.
        Now together with (iv), the story you can take home is that convergence
        in quadratic mean does not imply, and is not implied by, convergence almost surely.

        \noindent(vi)  We also want to mention that convergence in probability
        does not imply Convergence in $p$--norm/Strong convergence.
        One simple counter--example is the discrete R.V. $\{X_n\}_{n\geq1}$
        with distribution function
        \[P(\omega;X_n(\omega)=x)=
        \left\{
        \begin{array}{ll}
        1-\frac{1}{n},&x=0  \\
        \frac{1}{n},&x=n,\\
        0,&\text{otherwise}
        \end{array}
        \right.\]
        Prove by definition that $X_n\rightarrow 0$ in probability, but not in mean square.

        \noindent(vii)  For the same $\{X_n\}$ above.  Prove that a.c.--$X_n\rightarrow 0$.
        Therefore this also serves as a counter--example that convergence almost
        surely does not imply convergence in mean square.

        \noindent(viii)  Show that if $X_n \rightarrow X$ in probability, then in distribution.
        For the sake of simplicity, let us assume that both $F_n(x)$ and $F(x)$ are continuous
        functions.  Hint: Show that for any
        $\epsilon>0$, $F_n(x)\leq F(x+\epsilon)+P(\omega;|X_n(\omega)-X(\omega)|\geq \epsilon)$
        and $F(x-\epsilon)\leq F_n(x)+P(\omega;|X_n(\omega)-X(\omega)|\geq \epsilon)$.
        Now send the limit $n\rightarrow \infty$.

        \noindent(ix) However convergence in distribution does not imply convergence
        in probability.  The simple counter example is: let $X\sim N(0,1)$ and $X_n=-X$
        for $n=1,2,....$  It is easy to see that $X_n$ and $X$ has the same distribution
        function because $P(-X\leq x)=P(X\geq -x)=P(X\leq x)$ (you can draw the pdf to see
        this).  Prove that $X_n$ does not converge to $X$ in probability.
    \end{question}
    \begin{subproblem}
        \item
        \begin{proof}
            For any $\sigma>0$, let
            $A_\sigma=\{\omega\in\Omega;|X_n(\omega)-X(\omega)|\geq\sigma\}$,
            then
            \[\begin{aligned}
                E(|X_n-X|^2)&=\int_\Omega|X_n-X|^2\diff P\\
                &\geq\int_{A_\sigma}|X_n-X|^2\diff P\\
                &\geq \sigma^2\int_{A_\sigma}\diff P\\
                &=\sigma^2P(A_\sigma)
            \end{aligned}\]
            It follows that
            \[\lim_{n\to\infty}P(A_\sigma)
            =\frac{1}{\sigma^2}\lim_{n\to\infty}E(|X_n-X|^2)
            =0\]
            as m.s. $X_n\to X$. Therefore, $X_n\to X$ in probability.
        \end{proof}

        \item
        Consider R.V. $X_n$ s.t. $P(X_n=0)=1-1/n^2$ and $P(X_n=n)=1/n^2$.
        And it is evident that
        \[\begin{aligned}
            E(|X_n-X|)&=n\cdot\frac{1}{n^2}=\frac{1}{n}\to 0\\
            E(|X_n-X|^2)&=n^2\cdot\frac{1}{n^2}=1\not\to 0
        \end{aligned}\quad(n\to\infty)\]
        where $X=0$ a.e.

        \item
        \begin{proof}
            For any $p>0$, the notation $A_\sigma$ is the same
            as the one in problem (i), and similarly,
            \[\begin{aligned}
                E(|X_n-X|^p)&=\int_\Omega|X_n-X|^p\diff P\\
                &\geq\int_{A_\sigma}|X_n-X|^p\diff P\\
                &\geq \sigma^p\int_{A_\sigma}\diff P\\
                &=\sigma^pP(A_\sigma)
            \end{aligned}\]
            Then $X_n\to X$ in $p$-th norm implies convergence in
            probability in the same manner as
            \[\lim_{n\to\infty}P(A_\sigma)
            =\frac{1}{\sigma^p}\lim_{n\to\infty}E(|X_n-X|^p)
            =0\]
        \end{proof}

        \item
        \begin{proof}
            Since
            \[\begin{aligned}
                E(|X_t-1|^2)&=E(X_t^2)-2E(X_t)+1\\
                &=\e^{-t}\cdot E(\e^{2W_t})
                  -2\e^{-\frac t2}\cdot E(\e^{W_t})+1
            \end{aligned}\]
            and by the mgf \cref{eq:mgf h4} we know that
            \[\begin{aligned}
                E(\e^{2W_t})&=\e^{2t}\\
                E(\e^{W_t})&=\e^{\frac t2}\\
            \end{aligned}\]
            then we have that
            \[\begin{aligned}
                E(|X_t-1|^2)&=\e^{-t}\cdot\e^{2t}
                -2\e^{-\frac t2}\cdot\e^{\frac t2}+1\\
                &=\e^t-1\not\to 0\quad(t\to\infty)
            \end{aligned}\]
            i.e., $X_t\not\to 1$ m.s.
        \end{proof}

        \item
        Consider $X_n$ s.t.
        \[P(X_n=n)=\frac{1}{n^2}\text{ and }P(X_n=0)=1-\frac{1}{n^2}\]
        then for any $\varepsilon>0$,
        \[P(\omega;|X_n(\omega)|\geq\varepsilon)=P(\omega;X_n(\omega)=n)
        =\frac{1}{n^2}\]
        hence
        \[\sum_{n=1}^\infty P(\omega;|X_n(\omega)|\geq\varepsilon)<\infty\]
        and by Borel-Cantelli lemma, we have that
        \[P\left(\limsup_{n\to\infty}\{\omega;|X_n(\omega)|\geq\varepsilon\}\right)
        =0\]
        i.e.,
        \[P\left(\bigcap_{N=1}^\infty\bigcup_{n=N}^\infty
        \{\omega;|X_n(\omega)\geq\varepsilon\}\right)=0\]
        that being said, the set consisting of $\omega$ s.t.
        for any integer $N$,
        there exists some $n$ making $|X_n(w)|\geq\varepsilon$,
        has zero measure, i.e., $X_n\to 0$ a.c.
        And it is easy to see that
        \[E(|X_n|^2)=n^2\cdot\frac{1}{n^2}=1\not\to 0\]
        i.e., $X_n\not\to 0$ m.s. This serves as a counter example
        for convergence a.c. not implying m.s.

        Consider\sidenote{In fact it is the example in the last
        problem of HW1} mutually independent $X_n$ s.t.
        \[P(X_n=0)=1-\frac{1}{n}\text{ and }
        P(X_n=1)=\frac{1}{n}\]
        We can see that
        \[E(X_n^2)=\frac{1}{n}\to 0\]
        i.e., $X_n\to 0$ m.s.
        And denote $A_n=\{\omega\in\Omega;|X_n|geq 1\}$,
        then
        \[P(A_n)=P(X_n=1)=\frac{1}{n}\]
        thus
        \[\sum_{n=1}^\infty P(A_n)=\infty\]
        By Borel-Cantelli lemma (the second form),
        \[P\left(\limsup_{n\to\infty}A_n\right)=1\]
        that being said\sidenote{The set on which the
        sequence does not converge to 0 has probability 1 actually},
        a.c. $X_n\not\to 0$. And this serves as a counter example
        for m.s. not implying a.c.

        \item
        \begin{proof}
            For any $\sigma>0$, the only non-zero-measure part
            of $X_n$ is $X_n=n$, thus
            \[\begin{aligned}
                P(|X_n|\geq\sigma)=P(X_n=n)=\frac{1}{n}\to 0
                \quad(n\to\infty)
            \end{aligned}\]
            therefore $X_n\to 0$ in probability.

            And apparently,
            \[E(|X_n|^2)=n^2\cdot\frac{1}{n}=n\not\to 0\]
            i.e., $X_n\not\to 0$ m.s.
        \end{proof}

        \item
        {
            \it
            It seems that there may be something wrong or I missed something,
            since I can
            prove that $X_n\not\to 0$ a.c under a specific condition
            (i.e., the independence of $X_n$).

            For any $\varepsilon>0$, denote
            \[A_n=\{\omega\in\Omega;|X_n(\omega)|\geq\varepsilon\}\]
            and we can see that
            \[P(A_n)=P(X_n=n)=\frac{1}{n}\]
            thus
            \[\sum_{n=1}^\infty P(A_n)=\infty\]
            So if $X_n$ is mutually independent, then events $A_n$
            is independent mutually as well. By Borel-Cantelli lemma
            (the second form\sidenote{By the way, I remember the condition
            to the second form is disjoint $A_n$ as you mentioned in class,
            but I searched on the Internet, they said it is the independence.}), 
            \[P\left(\limsup_{n\to\infty}A_n\right)=1\]
            that being said, a.c. $X_n\not\to 0$.
        }


        \item
        \begin{proof}
            For any $\epsilon>0$ and $x\in\mathbb R,\omega\in\Omega$,
            if
            \[X(\omega)>x+\epsilon\text{ and }
            |X_n(\omega)-X(\omega)|<\epsilon\]
            then
            \[X_n(\omega)>X(\omega)-\epsilon>x\]
            thus
            \[\{\omega;X(\omega)>x+\epsilon\}\cap
            \{\omega;|X_n(\omega)-X(\omega)|<\epsilon\}
            \subset \{\omega;X_n(\omega)>x\}\]
            i.e.,
            \[\{\omega;X_n(\omega)\leq x\}
            \subset\{\omega;X(\omega)\leq x+\epsilon\}
            \cup\{\omega;|X_n(\omega)-X(\omega)|\geq\epsilon\}\]
            hence
            \[P(X_n\leq x)\leq P(X_n\leq x+\epsilon)+P(|X_n-X|\geq\epsilon)\]
            and if we change the position of $X_n$ and $X$, and substitute
            $x-\epsilon$ for $x$ due to the arbitrary of $x$, then we obatin,
            \[P(X\leq x-\epsilon)\leq P(X_n\leq x)+P(|X_n-X|\geq\epsilon)\]
            It follows that
            \[F(x-\epsilon)-P(|X_n-X|\geq\epsilon)
            \leq F_n(x)\leq
            F(x+\epsilon)+P(|X_n-X|\geq\epsilon)\]
            Then by sending $n\to\infty$, since $X_n\to X$ in probability,
            we have that
            \[F(x-\epsilon)\leq\lim_{n\to\infty}F_n(x)\leq F(x+\epsilon)\]
            again send $\epsilon\to 0^+$, we obtain
            \[\lim_{n\to\infty}F_n(x)=F(x)\]
            thanks to the continuity of $F(x)$. So $X_n\to X$ in distribution.
        \end{proof}

        \item
        \begin{proof}
            Since for any $\sigma>0$,
            \[P(|X_n-X|\geq\sigma)=P(2|X|\geq\sigma)\]
            which is a non-zero constant, it is evident that
            $X_n\not\to X$ in probability.
        \end{proof}
    \end{subproblem}

    \problem
    \begin{question}
        For any $a,b\in\mathbb R$, we consider equidistant of $(a,b)$
        $\pi:\{t_k; a=t_0<t_1<...<t_{n-1}<t_n=b\}$ and $t_{k+1}-t_k=\Delta t_k$.
        Show that in mean square
        \[\lim_{n\rightarrow\infty}\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_{i}})^2=b-a.\]
        Does the partition has to be equidistant? Provide a counter--example or prove why not.
    \end{question}
    \begin{proof}
        Denote that
        \[X_n:=\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})^2\]
        then by independent increments of  Brownian motion, we have that
        \[\var(X_n)=\sum_{i=0}^{n-1}\var((W_{t_{i+1}}-W_{t_i})^2)\]
        And we know that
        \[W_{t_{i+1}}-W_{t_i}\sim\mathcal N(0,\Delta t_i)\]
        thus
        \[\left(\frac{W_{t_{i+1}}-W_{t_i}}{\sqrt{\Delta t_i}}\right)^2
        \sim\chi^2(1)\]
        therefore,
        \[\begin{aligned}
            \var((W_{t_{i+1}}-W_{t_i})^2)&=2(\Delta t_i)^2\\
            E((W_{t_{i+1}}-W_{t_i})^2)&=\Delta t_i
        \end{aligned}\]
        and since partition is equidistant, i.e., 
        $\Delta t_i=(b-a)/n$, then we have
        \begin{equation}
            \label{eq:p8 variance}
            \var(X_t)=2\sum_{i=0}^{n-1}(\Delta t_i)^2
            =2n\cdot\left(\frac{b-a}{n}\right)^2
            =\frac{2(b-a)^2}{n}
        \end{equation}
        and
        \[E(X_t)=\sum_{i=0}^{n-1}\Delta t_i=b-a\]
        Therefore,
        \begin{equation}
            \label{eq:p8 m.s.}
            \begin{aligned}
            E(|X_t-(b-a)|^2)&=\var(X_t-(b-a))+(E(X_t-(b-a)))^2\\
            &=\var(X_t)+(E(X_t)-(b-a))^2\\
            &=\frac{2(b-a)^2}{n}\to 0\quad(n\to\infty)
            \end{aligned}
        \end{equation}
        i.e., in mean square
        \[\lim_{n\to\infty}\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})^2=b-a\]
    \end{proof}

    And if the partition is not equidistant, then denote
    \[\Delta t=\sup_{i}\Delta t_i\]
    then \cref{eq:p8 variance} is stated as
    \[\var(X_t)=2\sum_{i=0}^{n-1}(\Delta t_i)^2
    \leq 2\sum_{i=0}^{n-1}\Delta t_i\cdot\Delta t
    =2(b-a)\Delta t\]
    and similarly, like \cref{eq:p8 m.s.},
    \[E(|X_t-(b-a)|^2)=\var(X_t)\leq 2(b-a)\Delta t\]
    So for any partition whether equidistant or not,
    if $\Delta t=\sup_i\Delta t_i\to 0$,
    then $X_t\to b-a$ m.s. as well.

    \problem
    \begin{question}
        Consider the equidistant partition $0=t_0<t_1<...<t_{n-1}<t_n=T$.  Show that
        \[m.s.-\lim_{n\rightarrow \infty}\sum_{k=0}^{n-1}(W_{t_{k+1}}-W_{t_k})(t_{k+1}-t_k)=0.\]
        Does the partition has to be equidistant? Provide a counter--example or prove why not.
    \end{question}
    \begin{proof}
        Denote $\Delta t_k=t_{k+1}-t_k$, and
        \[X_n=\sum_{k=0}^{n-1}(W_{t_{k+1}}-W_{t_k})(t_{k+1}-t_k)\]
        then by independent increments,
        \[\var(X_n)
        =\sum_{k=0}^{n-1}(\Delta t_k)^2\var(W_{t_{k+1}}-W_{t_k})
        =\sum_{k=0}^{n-1}(\Delta t_k)^2\cdot\Delta t_k
        =\sum_{k=0}^{n-1}(\Delta t_k)^3\]
        and 
        \[E(X_n)=\sum_{k=0}^{n-1}\Delta t\cdot E(W_{t_{k+1}}-W_{t_k})
        =0\]
        Hence in the same manner
        \[E(|X_n|^2)=\var(X_n)+(E(X_n))^2=\sum_{k=0}^{n-1}(\Delta t_k)^3
        \leq\sum_{k=0}^{n-1}(\Delta t)^2\Delta t_k
        =(\Delta t)^2 T\]
        where $\Delta t=\sup_{k}\Delta t_k$.
        It follows that
        for any partition whether equidistant or not,
        if $\Delta t=\sup_i\Delta t_i\to 0$,
        then $X_t\to 0$ m.s.
    \end{proof}

    \problem
    \begin{question}
        Assume that $X\sim N(0,\sigma^2)$.  Show that \text{Var}$(X^2)=2\sigma^4$
        by straightforward calculations.
    \end{question}
    \newcommand{\epower}{\e^{-\frac{x^2}{2\sigma^2}}}
    \newcommand{\const}{\sqrt{2\pi\sigma^2}}
    \begin{proof}
        Since
        \[\var(X^2)=E(X^4)-(E(X^2))^2\]
        of which $E(X^2)=\sigma^2$ is known to us,
        then consider
        \[\begin{aligned}
            E(X^4)&=\int_{\Omega}X^4\diff P\\
            &=\int_{-\infty}^\infty x^4\cdot
            \frac{\epower}{\const}\diff x\\
            &=\frac{2}{\const}\int_0^\infty x^4\epower\diff x
        \end{aligned}\]
        Note that we used the symmetry in the last equation above.

        And we know that
        \[\diff \epower=-\frac{x}{\sigma^2}\epower\diff x\]
        thus the integral above becomes
        \[\begin{aligned}
            E(X^4)&=-\frac{2\sigma^2}{\const}\int_0^\infty x^3\diff\epower\\
            &=-\frac{2\sigma^2}{\const}\left(\left.x^3\epower\right|^\infty_0
            -\int_0^\infty\epower\diff x^3\right)
        \end{aligned}\]
        and the first term in the brace is self-cancelled as
        \[x^3\epower=0\]
        hence
        \[\begin{aligned}
            E(X^4)&=\frac{2\sigma^2}{\const}\int_0^\infty 3x^2\epower\diff x\\
            &=3\sigma^2\int_{-\infty}^\infty x^2\cdot\frac{\epower}{\const}\diff x
        \end{aligned}\]
        of which the integral is $E(X^2)$ exactly, therefore
        \[E(X^4)=3\sigma^2\cdot \sigma^2=3\sigma^4\]
        Then
        \[\var(X^2)=E(X^4)-(E(X^2))^2=3\sigma^4-(\sigma^2)^2=2\sigma^4\]
    \end{proof}

    \problem
    \begin{question}
        Prove that
        \[m.s \lim_{n\rightarrow \infty}(X_n+Y_n)=m.s \lim_{n\rightarrow \infty}X_n+m.s \lim_{n\rightarrow \infty}Y_n.\]
        Hint:  First prove this for the case when $\lim_{n\rightarrow \infty}X_n=\lim_{n\rightarrow \infty}Y_n=0$;
        here you may need to show that $E(X_nY_n)\rightarrow 0$.  Now for this general case, apply this results to $\tilde X_n=X_n-\lim X_n$ and $\tilde Y_n=Y_n-\lim Y_n$.
    \end{question}
    \begin{proof}
        Let's show the case when m.s. $X_n\to 0,Y_n\to 0$ at first.
        Since
        \[E(|X_n+Y_n|^2)=E(|X_n|^2)+E(|Y_n|^2)+2E(X_nY_n)\]
        we need to show that $E(X_nY_n)\to 0$ m.s.
        And in the light of Cauchy-Schwarz inequality, note that
        for any $\lambda\in\mathbb R$,
        \[E(|X_n+\lambda Y_n|^2)\geq 0\]
        i.e.,
        \[\lambda^2E(Y_n^2)+2\lambda E(X_nY_n)+E(X_n^2)\geq 0\]
        It follows that
        \[\Delta=(2E(X_nY_n))^2-4E(X_n^2)\cdot E(Y_n^2)\leq 0\]
        in term of the quadratic of $\lambda$. Hence
        \[|E(X_nY_n)|\leq\sqrt{E(X_n^2)\cdot E(Y_n^2)}\to 0\]
        as m.s. $X_n,Y_n\to 0$.

        Therefore,
        \[E(|X_n+Y_n|^2)=E(|X_n|^2)+E(|Y_n|^2)+2E(X_nY_n)\to 0\]
        i.e., m.s. $X_n+Y_n\to 0$.

        Now consider any $X_n\to X$ m.s. and $Y_n\to Y$ m.s.,
        denote $\tilde X_n=X_n-X$ and $\tilde Y_n=Y_n-Y$.
        And it is easy to verify that
        \[E(|\tilde X_n|^2)=E(|X_n-X|^2)\to 0
        \text{ and }
        E(|\tilde Y_n|^2)=E(|Y_n-Y|^2)\to 0 \]
        i.e., m.s. $\tilde X_n,\tilde Y_n\to 0$
        Then apply the result we obtained above, we have that
        \[E(|\tilde X_n+\tilde Y_n|^2)\to 0\]
        i.e.,
        \[E(|X_n+Y_n-(X+Y)|^2)\to 0\]
        that being said,
        \[\text{m.s.}\lim_{n\to\infty}(X_n+Y_n)
        =\text{m.s.}\lim_{n\to\infty}X_n
        +\text{m.s.}\lim_{n\to\infty}Y_n\]
    \end{proof}

    \problem
    \begin{question}
        \noindent(i)  Prove the squeeze theorem:  Let $X_n$, $Y_n$ and $Z_n$ be sequences
        of random variables on $(\Omega,\mathcal F,P)$ such that
        \[X_n\leq Y_n\leq Z_n, \text{a.s},\forall n\geq 1.\]
        If $X_n$ and $Y_n$ converge to $L$ in mean square, then $Z_n$
        converges to $L$ in mean square.  Remark:  the same conclusion holds
        for different convergence manner.

        \noindent(ii).  Show that \[m.s.--\lim_{t\rightarrow \infty}\frac{W_t\sin W_t}{t}=0\]
    \end{question}
    \begin{subproblem}
        \item
        \begin{proof}
            Denote $\tilde X_n=X_n-L,\tilde Y_n=Y_n-L$
            and $\tilde Z_n=Z_n-L$. Then we have that for a.c.
            $\omega\in\Omega$,
            \[\tilde X_n\leq\tilde Y_n\leq\tilde Z_n\]
            thus
            \[\tilde Y_n\leq\tilde Z_n\text{ and }
            -\tilde Y_n\leq-\tilde X_n\]
            hence
            \[|\tilde Y_n|\leq\max\{\tilde Z_n,-\tilde X_n\}\]

            And note that for maximum we have the equivalent form,
            \[\max\{\tilde Z_n,-\tilde X_n\}
            =\frac{\tilde Z_n-\tilde X_n+|\tilde Z_n+\tilde X_n|}{2}
            \leq\frac{|\tilde Z_n-\tilde X_n|+|\tilde Z_n+\tilde X_n|}{2}\]
            hence a.c. in $\Omega$,
            \[\tilde Y^2_n\leq
            \max\{\tilde Z_n,-\tilde X_n\}^2
            \leq\frac{\tilde Z_n^2+\tilde X_n^2+2|\tilde Z_n^2-\tilde X_n^2|}{4}
            \leq\frac{3(\tilde Z_n^2+\tilde X_n^2)}{4}\]
            It follows that
            \[E(\tilde Y_n^2)
            \leq E(\max\{\tilde Z_n,-\tilde X_n\}^2)
            \leq\frac{3}{4}E(\tilde Z_n^2+\tilde X_n^2)
            \to 0\quad(n\to\infty)\]
            as m.s. $\tilde X_n,\tilde Z_n\to 0$. Therefore,
            m.s. $\tilde Y_n\to 0$, i.e., $Y_n\to L$.
        \end{proof}

        \item
        \begin{proof}
            Since
            \[(\sin W_t)^2\leq 1\]
            then it is obvious to see that
            \[\left(\frac{W_t\sin W_t}{t}\right)^2\leq\frac{W_t^2}{t^2}\]
            hence
            \[E\left(\left|\frac{W_t\sin W_t}{t}\right|^2\right)
            \leq\frac{E(W_t^2)}{t^2}=\frac{1}{t}\to 0\]
            as $t\to\infty$, i.e.,
            \[\text{m.s.}\lim_{t\to\infty}\frac{W_t\sin W_t}{t}=0\]
        \end{proof}
    \end{subproblem}