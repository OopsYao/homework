\documentclass{homework}

\title{Homework 4}

\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}

\begin{document}
    \maketitle

    \problem
    Since
    \[\begin{aligned}
        \var(X_t)&=E(X_t^2)-(E(X_t))^2\\
        &=E\left\{\left(\int_0^tW_s\diff s\right)
        \left(\int_0^tW_r\diff r\right)\right\}\\
        &=E\left(\int_0^t\int_0^tW_sW_r\diff s\diff r\right)\\
        &=\int_\Omega\left(\iint_{[0,t]^2}W_sW_r\diff s\diff r\right)\diff P
    \end{aligned}\]
    then we apply Fubini theorem again,
    \[\var(X_t)=\iint_{[0,t]^2}
    \left(\int_\Omega W_sW_r\diff P\right)\diff s\diff r\]
    Without loss of generality, assume that $s<r$, then we obtain
    by independent increments of Brownian motion,
    \[\begin{aligned}
        \int_\Omega W_sW_r\diff P&=E(W_sW_r)\\
        &=E(W_s(W_r-W_s+W_s))\\
        &=E(W_s(W_r-W_s))+E(W_s^2)\\
        &=s
    \end{aligned}\]
    thus in general,
    \[\int_\Omega W_sW_r\diff P=\min\{s,r\}\]
    It follows that
    \[\begin{aligned}
        \var(X_t)&=\iint_{[0,t]^2}\min\{s,r\}\diff s\diff r\\
        &=\left(\iint_{\{(s,r)\in[0,t]^2;s\leq r\}}
                +\iint_{\{(s,r)\in[0,t]^2;s\leq r\}}\right)
        \min\{s,r\}\diff s\diff r\\
    \end{aligned}\]
    and by symmetry, we obtain the variance as
    \[\begin{aligned}
        \var(X_t)&=2\iint_{\{(s,r)\in[0,t]^2;s\leq r\}}s\diff s\diff r\\
        &=\int_0^t\diff r\int_0^r2s\diff s\\
        &=\frac{t^3}{3}
    \end{aligned}\]

    \problem
    % TODO Problem 2

    \problem
    By Fubini theorem, calculating $E(Y_t)$ falls to
    calculate $\int_0^tE(\e^{W_s})\diff s$ as
    \[\begin{aligned}
    E(Y_t)&=\int_\Omega\left(\int_0^t\e^{W_s}\diff s\right)\diff P\\
    &=\int_0^t\left(\int_\Omega\e^{W_s}\diff P\right)\diff s\\
    &=\int_0^tE(\e^{W_s})\diff s \\
    \end{aligned}\]
    And to the convenience of following calculation, consider moment
    generating function of $W_s$,
    \begin{equation}
        \label{eq:mgf}
        \begin{aligned}
        E(\e^{uW_s})&=\int_{-\infty}^\infty\e^{ux}\cdot
        \frac{\e^{-\frac{x^2}{2s}}}{\sqrt{2\pi s}}\diff x\\
        &=\int_{-\infty}^\infty\e^{\frac{u^2s}{2}}
        \cdot\frac{\e^{-\frac{(x-us)^2}{2s}}}{\sqrt{2\pi s}}\diff x\\
        &=\int_{-\infty}^\infty\e^{\frac{u^2s}{2}}
        \cdot\frac{\e^{-\frac{\xi^2}{2s}}}{\sqrt{2\pi s}}\diff\xi\\
        &=\e^{\frac{u^2s}{2}}
        \end{aligned}
    \end{equation}
    where we used substitution of $\xi=x-us$.

    Therefore,
    \[\begin{aligned}
        E(\e^{W_s})&=\e^{\frac{s}{2}}\\
        E(\e^{2W_s})&=\e^{2s}\\
    \end{aligned}\]
    Thus,
    \[\begin{aligned}
        E(Y_t)&=\int_0^tE(\e^{W_s})\diff s\\
        &=\int_0^t\e^{\frac{s}{2}}\diff s\\
        &=2(\e^{\frac{t}{2}}-1)
    \end{aligned}\]
    and similarly,
    \[\begin{aligned}
        E(Y_t^2)&=\int_0^tE(\e^{2W_s})\diff s\\
        &=\int_0^t\e^{2s}\diff s\\
        &=\frac{\e^{2t}-1}{2}
    \end{aligned}\]
    hence the variance,
    \[\begin{aligned}
        \var(Y_t)&=E(Y_t^2)-(E(Y_t))^2\\
        &=\frac{\e^{2t}-1}{2}-4(\e^{\frac{t}{2}}-1)^2\\
        &=\frac{\e^{2t}}{2}-4\e^t+8\e^{\frac{t}{2}}-\frac{9}{2}
    \end{aligned}\]

    \problem

    \problem
    \begin{subproblem}
        \item
        Without loss of generality, assume that $s<t$.
        Since $EX_t=0$, then we have that
        \[\begin{aligned}
            \cov(X_t,X_s)&=E(X_tX_s)\\
        \end{aligned}\]
        Let $\mathcal F_\cdot=(\mathcal F_t)_{t>0}$ be
        the natural filtration of Brownian motion
        $W_t$. Consider that
        \begin{equation}
            \label{eq:p5 condi exp}
            E(X_t(W_s-sW_1)|\mathcal F_s)
            =E(W_sX_t|\mathcal F_s)
            -s\cdot E(W_1X_t|\mathcal F_s)
        \end{equation}
        The first term
        \[E(W_sX_t|\mathcal F_s)=W_s\cdot E(X_t|\mathcal F_s)
        =W_s\cdot E(W_t-tW_1|\mathcal F_s)=(1-t)W_s^2\]
        And the last term
        \begin{equation*}
            \begin{aligned}
                E(W_1X_t|\mathcal F_s)
                =E(W_1(W_t-tW_1)|\mathcal F_s)
                =E(W_1W_t|\mathcal F_s)-tE(W_1^2|\mathcal F_s)
            \end{aligned}
        \end{equation*}
        Since independent increments of $W_t$ implying that
        \[E(W_1W_t)=E(W_t(W_1-W_t+W_t))=E(W_t)\cdot E(W_1-W_t)+E(W_t^2)=t\]
        then the expectation of last term of \cref{eq:p5 condi exp}
        is obtained as
        \[E(E(W_1X_t|\mathcal F_s))=E(W_1W_t)-tE(W_1^2)=t-t=0\]
        Therefore by tower rule,
        \[\begin{aligned}
            E(X_tX_s)&=E(E(W_sX_t|\mathcal F_s))-s\cdot E(E(W_1X_t|\mathcal F_s))\\
            &=(1-t)E(W_s^2)\\
            &=s(1-t)
        \end{aligned}\]
        so in general,
        \begin{equation}
            \label{eq:p5 cov}
            \cov(X_t,X_s)=\min\{t,s\}-st
        \end{equation}

        \item
        Since \cref{eq:p5 cov} yields that
        \[\var(X_t)=\cov(X_t,X_t)=t(1-t)\]
        and by linearity of normal distribution, we have that
        \[X_t\sim \mathcal N(0,t(1-t))\]
        thus
        \[\left(\frac{X_t}{\sqrt{t(1-t)}}\right)^2
        =\frac{X_t^2}{t(1-t)}\sim\chi^2(1)\]
        hence
        \[\begin{aligned}
            E\left(\frac{X_t^2}{t(1-t)}\right)&=1\\
            \var\left(\frac{X_t^2}{t(1-t)}\right)&=2\\
        \end{aligned}\]
        i.e.,
        \[\begin{aligned}
            E(Y_t)&=E(X_t^2)=t(1-t)\\
            \var(Y_t)&=\var(X_t^2)=2t^2(1-t)^2
        \end{aligned}\]
    \end{subproblem}
    
    \problem
    \begin{subproblem}
        \item
        By Jensen's inequality, we have that
        \[E(|X_n-X|^2)\geq (E(|X_n-X|))^2\geq |E(X_n-X)|^2\]
        It follows that 
        \[\lim_{n\to\infty}E(|X_n-X|^2)=0\]
        yields
        \[\begin{aligned}
            \lim_{n\to\infty}E(X_n-X)&=0\\
        \end{aligned}\]
        i.e.,
        \[\lim_{n\to\infty}E(X_n)=E(X)\]
        And by Minkowski inequality,
        \[\begin{aligned}
            \sqrt{E(|X_n|^2)}&\leq\sqrt{E(|X_n-X|^2)}+\sqrt{E(|X|^2)}\\
            \sqrt{E(|X|^2)}&\leq\sqrt{E(|X-X_n|^2)}+\sqrt{E(|X_n|^2)}\\
        \end{aligned}\]
        i.e.,
        \[\left|\sqrt{E(|X_n|^2)}-\sqrt{E(|X|^2)}\right|
        \leq\sqrt{E(|X_n-X|^2)}\]
        so m.s. $X_n\to X$ implies
        \[\sqrt{E(|X_n|^2)}\to\sqrt{E(|X|^2)}\]
        as well as
        \[E(|X_n|^2)\to E(|X|^2)\]
        because of the continuity of $f(x)=x^2$.

        Finally since
        \[\begin{aligned}
            \var(X_n)&=E(X_n^2)-(E(X_n))^2\\
            \var(X)&=E(X^2)-(E(X))^2
        \end{aligned}\]
        we have that
        \[\var(X_n)\to\var(X)\quad(n\to\infty)\]

        \item
        Note
        \[E(|X_n-X|^2)=E(X_n^2)-2E(X_nX)+E(X^2)\]
        implies that
        \[\lim_{n\to\infty}E(X_nX)
        =\frac{1}{2}\lim_{n\to\infty}\left(E(X_n^2)+E(X^2)+E(|X_n-X|^2)\right)
        =E(X^2)=0\]
        as
        \[\lim_{n\to\infty}E(X_n^2)=E(X^2)
        \text{ and }
        \lim_{n\to\infty}E(|X_n^2-X^2|)=0\]
        therefore
        \[\begin{aligned}
            \cov(X_n,X)&=E(X_nX)-E(X_n)\cdot E(X)\\
            &\to E(X^2)-(E(X))^2=\cov(X,X)\quad(n\to\infty)
        \end{aligned}\]
    \end{subproblem}

    \problem
    \begin{subproblem}
        \item
        \begin{proof}
            For any $\sigma>0$, let
            $A_\sigma=\{\omega\in\Omega;|X_n(\omega)-X(\omega)|\geq\sigma\}$,
            then
            \[\begin{aligned}
                E(|X_n-X|^2)&=\int_\Omega|X_n-X|^2\diff P\\
                &\geq\int_{A_\sigma}|X_n-X|^2\diff P\\
                &\geq \sigma^2\int_{A_\sigma}\diff P\\
                &=\sigma^2P(A_\sigma)
            \end{aligned}\]
            It follows that
            \[\lim_{n\to\infty}P(A_\sigma)
            =\frac{1}{\sigma^2}\lim_{n\to\infty}E(|X_n-X|^2)
            =0\]
            as m.s. $X_n\to X$. Therefore, $X_n\to X$ in probability.
        \end{proof}

        \item
        Consider R.V. $X_n$ s.t. $P(X_n=0)=1-1/n^2$ and $P(X_n=n)=1/n^2$.
        And it is evident that
        \[\begin{aligned}
            E(|X_n-X|)&=n\cdot\frac{1}{n^2}=\frac{1}{n}\to 0\\
            E(|X_n-X|^2)&=n^2\cdot\frac{1}{n^2}=1\not\to 0
        \end{aligned}\quad(n\to\infty)\]
        where $X=0$ a.e.

        \item
        \begin{proof}
            For any $p>0$, the notation $A_\sigma$ is the same
            as the one in problem (i), and similarly,
            \[\begin{aligned}
                E(|X_n-X|^p)&=\int_\Omega|X_n-X|^p\diff P\\
                &\geq\int_{A_\sigma}|X_n-X|^p\diff P\\
                &\geq \sigma^p\int_{A_\sigma}\diff P\\
                &=\sigma^pP(A_\sigma)
            \end{aligned}\]
            Then $X_n\to X$ in $p$-th norm implies convergence in
            probability in the same manner as
            \[\lim_{n\to\infty}P(A_\sigma)
            =\frac{1}{\sigma^p}\lim_{n\to\infty}E(|X_n-X|^p)
            =0\]
        \end{proof}

        \item
        \begin{proof}
            Since
            \[\begin{aligned}
                E(|X_t-1|^2)&=E(X_t^2)-2E(X_t)+1\\
                &=\e^{-t}\cdot E(\e^{2W_t})
                  -2\e^{-\frac t2}\cdot E(\e^{W_t})+1
            \end{aligned}\]
            and by the mgf \cref{eq:mgf} we know that
            \[\begin{aligned}
                E(\e^{2W_t})&=\e^{2t}\\
                E(\e^{W_t})&=\e^{\frac t2}\\
            \end{aligned}\]
            then we have that
            \[\begin{aligned}
                E(|X_t-1|^2)&=\e^{-t}\cdot\e^{2t}
                -2\e^{-\frac t2}\cdot\e^{\frac t2}+1\\
                &=\e^t-1\not\to 0\quad(t\to\infty)
            \end{aligned}\]
            i.e., $X_t\not\to 1$ m.s.
        \end{proof}

        \item
        % TODO (v) Find 2 counter example between a.c. and m.s.

        \item
        \begin{proof}
            For any $\sigma>0$, the only non-zero-measure part
            of $X_n$ is $X_n=n$, thus
            \[\begin{aligned}
                P(|X_n|\geq\sigma)=P(X_n=n)=\frac{1}{n}\to 0
                \quad(n\to\infty)
            \end{aligned}\]
            therefore $X_n\to 0$ in probability.

            And apparently,
            \[E(|X_n|^2)=n^2\cdot\frac{1}{n}=n\not\to 0\]
            i.e., $X_n\not\to 0$ m.s.
        \end{proof}

        \item
        % TODO (vii) Prove Xn -> 0 a.c. above

        \item
        % TODO (viii) Prove convergence in probability => in distribution

        \item
        \begin{proof}
            Since for any $\sigma>0$,
            \[P(|X_n-X|\geq\sigma)=P(2|X|\geq\sigma)\]
            which is a non-zero constant, it is evident that
            $X_n\not\to X$ in probability.
        \end{proof}
    \end{subproblem}

    \problem
    \begin{proof}
        Denote that
        \[X_n:=\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})^2\]
        then by independent increments of  Brownian motion, we have that
        \[\var(X_n)=\sum_{i=0}^{n-1}\var((W_{t_{i+1}}-W_{t_i})^2)\]
        And we know that
        \[W_{t_{i+1}}-W_{t_i}\sim\mathcal N(0,\Delta t_i)\]
        thus
        \[\left(\frac{W_{t_{i+1}}-W_{t_i}}{\sqrt{\Delta t_i}}\right)^2
        \sim\chi^2(1)\]
        therefore,
        \[\begin{aligned}
            \var((W_{t_{i+1}}-W_{t_i})^2)&=2(\Delta t_i)^2\\
            E((W_{t_{i+1}}-W_{t_i})^2)&=\Delta t_i
        \end{aligned}\]
        and since partition is equidistant, i.e., 
        $\Delta t_i=(b-a)/n$, then we have
        \begin{equation}
            \label{eq:p8 variance}
            \var(X_t)=2\sum_{i=0}^{n-1}(\Delta t_i)^2
            =2n\cdot\left(\frac{b-a}{n}\right)^2
            =\frac{2(b-a)^2}{n}
        \end{equation}
        and
        \[E(X_t)=\sum_{i=0}^{n-1}\Delta t_i=b-a\]
        Therefore,
        \begin{equation}
            \label{eq:p8 m.s.}
            \begin{aligned}
            E(|X_t-(b-a)|^2)&=\var(X_t-(b-a))+(E(X_t-(b-a)))^2\\
            &=\var(X_t)+(E(X_t)-(b-a))^2\\
            &=\frac{2(b-a)^2}{n}\to 0\quad(n\to\infty)
            \end{aligned}
        \end{equation}
        i.e., in mean square
        \[\lim_{n\to\infty}\sum_{i=0}^{n-1}(W_{t_{i+1}}-W_{t_i})^2=b-a\]
    \end{proof}

    And if the partition is not equidistant, then denote
    \[\Delta t=\sup_{i}\Delta t_i\]
    then \cref{eq:p8 variance} is stated as
    \[\var(X_t)=2\sum_{i=0}^{n-1}(\Delta t_i)^2
    \leq 2\sum_{i=0}^{n-1}\Delta t_i\cdot\Delta t
    =2(b-a)\Delta t\]
    and similarly, like \cref{eq:p8 m.s.},
    \[E(|X_t-(b-a)|^2)=\var(X_t)\leq 2(b-a)\Delta t\]
    So for any partition whether equidistant or not,
    if $\Delta t=\sup_i\Delta t_i\to 0$,
    then $X_t\to b-a$ m.s. as well.

    \problem
    \begin{proof}
        Denote $\Delta t_k=t_{k+1}-t_k$, and
        \[X_n=\sum_{k=0}^{n-1}(W_{t_{k+1}}-W_{t_k})(t_{k+1}-t_k)\]
        then by independent increments,
        \[\var(X_n)
        =\sum_{k=0}^{n-1}(\Delta t_k)^2\var(W_{t_{k+1}}-W_{t_k})
        =\sum_{k=0}^{n-1}(\Delta t_k)^2\cdot\Delta t_k
        =\sum_{k=0}^{n-1}(\Delta t_k)^3\]
        and 
        \[E(X_n)=\sum_{k=0}^{n-1}\Delta t\cdot E(W_{t_{k+1}}-W_{t_k})
        =0\]
        Hence in the same manner
        \[E(|X_n|^2)=\var(X_n)+(E(X_n))^2=\sum_{k=0}^{n-1}(\Delta t_k)^3
        \leq\sum_{k=0}^{n-1}(\Delta t)^2\Delta t_k
        =(\Delta t)^2 T\]
        where $\Delta t=\sup_{k}\Delta t_k$.
        It follows that
        for any partition whether equidistant or not,
        if $\Delta t=\sup_i\Delta t_i\to 0$,
        then $X_t\to 0$ m.s.
    \end{proof}

    \problem
    \newcommand{\epower}{\e^{-\frac{x^2}{2\sigma^2}}}
    \newcommand{\const}{\sqrt{2\pi\sigma^2}}
    \begin{proof}
        Since
        \[\var(X^2)=E(X^4)-(E(X^2))^2\]
        of which $E(X^2)=\sigma^2$ is known to us,
        then consider
        \[\begin{aligned}
            E(X^4)&=\int_{\Omega}X^4\diff P\\
            &=\int_{-\infty}^\infty x^4\cdot
            \frac{\epower}{\const}\diff x\\
            &=\frac{2}{\const}\int_0^\infty x^4\epower\diff x
        \end{aligned}\]
        Note that we used the symmetry in the last equation above.

        And we know that
        \[\diff \epower=-\frac{x}{\sigma^2}\epower\diff x\]
        thus the integral above becomes
        \[\begin{aligned}
            E(X^4)&=-\frac{2\sigma^2}{\const}\int_0^\infty x^3\diff\epower\\
            &=-\frac{2\sigma^2}{\const}\left(\left.x^3\epower\right|^\infty_0
            -\int_0^\infty\epower\diff x^3\right)
        \end{aligned}\]
        and the first term in the brace is self-cancelled as
        \[x^3\epower=0\]
        hence
        \[\begin{aligned}
            E(X^4)&=\frac{2\sigma^2}{\const}\int_0^\infty 3x^2\epower\diff x\\
            &=3\sigma^2\int_{-\infty}^\infty x^2\cdot\frac{\epower}{\const}\diff x
        \end{aligned}\]
        of which the integral is $E(X^2)$ exactly, therefore
        \[E(X^4)=3\sigma^2\cdot \sigma^2=3\sigma^4\]
        Then
        \[\var(X^2)=E(X^4)-(E(X^2))^2=3\sigma^4-(\sigma^2)^2=2\sigma^4\]
    \end{proof}

    \problem
    \begin{proof}
        Let's show the case when m.s. $X_n\to 0,Y_n\to 0$ at first.
        Since
        \[E(|X_n+Y_n|^2)=E(|X_n|^2)+E(|Y_n|^2)+2E(X_nY_n)\]
        we need to show that $E(X_nY_n)\to 0$ m.s.
        And in the light of Cauchy-Schwarz inequality, note that
        for any $\lambda\in\mathbb R$,
        \[E(|X_n+\lambda Y_n|^2)\geq 0\]
        i.e.,
        \[\lambda^2E(Y_n^2)+2\lambda E(X_nY_n)+E(X_n^2)\geq 0\]
        It follows that
        \[\Delta=(2E(X_nY_n))^2-4E(X_n^2)\cdot E(Y_n^2)\leq 0\]
        in term of the quadratic of $\lambda$. Hence
        \[|E(X_nY_n)|\leq\sqrt{E(X_n^2)\cdot E(Y_n^2)}\to 0\]
        as m.s. $X_n,Y_n\to 0$.

        Therefore,
        \[E(|X_n+Y_n|^2)=E(|X_n|^2)+E(|Y_n|^2)+2E(X_nY_n)\to 0\]
        i.e., m.s. $X_n+Y_n\to 0$.

        Now consider any $X_n\to X$ m.s. and $Y_n\to Y$ m.s.,
        denote $\tilde X_n=X_n-X$ and $\tilde Y_n=Y_n-Y$.
        And it is easy to verify that
        \[E(|\tilde X_n|^2)=E(|X_n-X|^2)\to 0
        \text{ and }
        E(|\tilde Y_n|^2)=E(|Y_n-Y|^2)\to 0 \]
        i.e., m.s. $\tilde X_n,\tilde Y_n\to 0$
        Then apply the result we obtained above, we have that
        \[E(|\tilde X_n+\tilde Y_n|^2)\to 0\]
        i.e.,
        \[E(|X_n+Y_n-(X+Y)|^2)\to 0\]
        that being said,
        \[\text{m.s.}\lim_{n\to\infty}(X_n+Y_n)
        =\text{m.s.}\lim_{n\to\infty}X_n
        +\text{m.s.}\lim_{n\to\infty}Y_n\]
    \end{proof}

    \problem
    \begin{subproblem}
        \item
        % TODO Prove squeeze theorem

        \item
        \begin{proof}
            Since
            \[(\sin W_t)^2\leq 1\]
            then it is obvious to see that
            \[\left(\frac{W_t\sin W_t}{t}\right)^2\leq\frac{W_t^2}{t^2}\]
            hence
            \[E\left(\left|\frac{W_t\sin W_t}{t}\right|^2\right)
            \leq\frac{E(W_t^2)}{t^2}=\frac{1}{t}\to 0\]
            as $t\to\infty$, i.e.,
            \[\text{m.s.}\lim_{t\to\infty}\frac{W_t\sin W_t}{t}=0\]
        \end{proof}
    \end{subproblem}
\end{document}