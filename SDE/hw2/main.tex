\documentclass{homework}

\title{Homework 2}

\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Corr}{Corr}

\begin{document}
    \maketitle    

    \problem
    Without loss of generality, assume that $t\geq s$.
    Then by the law of distributivity,
    \begin{align*}
        \Cov(B_t,B_s)&=\Cov(B_t-B_s+B_s,B_s)\\
                     &=\Cov(B_t-B_s,B_s)+\Cov(B_s,B_s)
    \end{align*} 
    Since Brownian motion has independent increments,
    we have that
    \[\Cov(B_t-B_s,B_s)=0\]
    Therefore
    \[\Cov(B_t,B_s)=\Var(B_s)=s\]
    i.e.,
    \[\Cov(B_t,B_s)=\min\{t,s\}\]
    in general.

    Hence
    \[\Corr(B_t,B_s)=\frac{\Cov(B_t,B_s)}{\sqrt{\Var(B_t)\Var(B_s)}}
    =\frac{\min\{t,s\}}{\sqrt{ts}}\]

    \problem
    Note that $E\left(B_t^2\right)=t$, so by definition of
    covariance,
    \begin{equation}
        \label{eq:cov def}
        E\left((B_t^2-t)(B_s^2-s)\right)=\Cov(B_t^2,B_s^2)
    \end{equation}
    Also note that
    \begin{equation}
        \label{eq:cov formula}
        \Cov(B_t^2,B_s^2)=E(B_t^2B_s^2)-E(B_t^2)\cdot E(B_s^2)
    \end{equation}

    So the key to these problems is calculating $E(B_t^2B_s^2)$,
    which can be obtained by law of total expectation. Therefore,
    we can start from $E(B_t^2B_s^2|\mathcal F_s)$, and without
    loss of generality, we assume that $s\leq t$.

    We have that
    \begin{align*}
        E\left.\left(B_t^2B_s^2\right|\mathcal F_s\right)
        &=B_s^2E\left.\left(B_t^2\right|\mathcal F_s\right)\\
    \end{align*}
    and
    \begin{align*}
        E\left.\left(B_t^2\right|\mathcal F_s\right)
        &=E\left.\left((B_t-B_s+B_s)^2\right|\mathcal F_s\right)\\
        &=E\left((B_t-B_s)^2|\mathcal F_s\right)
            +2E\big(B_s(B_t-B_s)|\mathcal F_s\big)
            +E\left.\left(B_s^2\right|\mathcal F_s\right)
    \end{align*}
    Since Brownian motion has independent increments, we have
    that
    \begin{align*}
    E\left.\left((B_t-B_s)^2\right|\mathcal F_s\right)
    &=E\left((B_t-B_s)^2\right)=t-s\\
    E\left.\left(B_s(B_t-B_s)\right|\mathcal F_s\right)&=B_sE(B_t-B_s)=0\\
    E\left.\left(B_s^2\right|\mathcal F_s\right)&=B_s^2
    \end{align*}
    Hence
    \begin{align*}
        E\left.\left(B_t^2B_s^2\right|\mathcal F_s\right)
        &=B_s^2\cdot E\left.\left(B_t^2\right|\mathcal F_s\right)\\
        &=B_s^2\left(t-s+B_s^2\right)
    \end{align*} 

    Therefore
    \begin{align*}
    E\left(B_t^2B_s^2\right)&=E\left(E\left.\left(B_t^2B_s^2\right|\mathcal F_s\right)\right)\\
    &=E\left(B_s^2\left(t-s+B_s^2\right)\right)\\
    &=(t-s)E\left(B_s^2\right)+E\left(B_s^4\right)
    \end{align*}


    As for $E\left(B_s^4\right)$, since that $\left(B_s/\sqrt{s}\right)^2
    \sim\chi^2(1)$, we obtain,
    \[\Var(B_s^2)=2s^2\]
    thus
    \begin{align*}
        E\left(B_s^4\right)&=\Var\left(B_s^2\right)+\left(E\left(B_s^2\right)\right)^2=3s^2
    \end{align*}
    It follows that
    \begin{align*}
    E\left(B_t^2B_s^2\right)=s(t+2s)
    \end{align*}
    so in general by symmetry,
    \begin{equation}
        \label{eq:EBs2Bt2}
        E\left(B_t^2B_s^2\right)=ts+2\min\{t,s\}
    \end{equation}

    \begin{subproblem}
        \item\label{pb:2.1}
        From \cref{eq:cov def,eq:cov formula} we know that
        \[E\left((B_t^2-t)(B_s^2-s)\right)=ts+2\min\{t,s\}-ts=2\min\{t,s\}\]

        \item
        As in \cref{eq:EBs2Bt2}
        \[E\left(B_t^2B_s^2\right)=ts+2\min\{t,s\}\]

        \item
        Same as \ref{pb:2.1} as shown by \cref{eq:cov def},
        \[\Cov(B_t^2,B_s^2)=2\min\{t,s\}\]
    \end{subproblem}

    \problem
    \begin{proof}
        Since $2X_i-1$ represents the movements of every step,
        we have that
        \[Y_n=\Delta x\cdot\left(\sum_{i=1}2X_i-1\right)
        =\Delta x(2S_n-n)\]
        Then by Laplace-De Moivre Theorem,
        \[\frac{S_n-\frac{n}{2}}{\sqrt{\frac{n}{4}}}\to N(0,1)\]
        i.e.,
        \[\frac{Y_n}{\sqrt{n}\Delta x}\to N(0,1)\]
        Since $D=(\Delta x)^2/\Delta t$ and $t=n\Delta t$,
        we have that
        \[\frac{Y_n}{\sqrt{n}\Delta x}=\frac{Y_n}{\sqrt{Dt}}\]
        And $n\to\infty$ is equivalent $t\to\infty$, therefore
        \[\frac{Y_t}{\sqrt{Dt}}\to N(0,1)\]
        It follows that
        \begin{align*}
        \lim_{n\to\infty,t=n\Delta t}P(a\leq Y_t\leq b)
        &=\frac{1}{\sqrt{2\pi}}
        \int_{a/\sqrt{Dt}}^{b/\sqrt{Dt}}\e^{-\frac{\xi^2}{2}}\diff\xi\\
        &=\frac{1}{\sqrt{2\pi Dt}}
        \int_a^b\e^{-\frac{x^2}{2Dt}}\diff x
        \end{align*}
    \end{proof}

    \problem
    \begin{subproblem}[\roman*)]
        \item
        Let's start from the high dimension directly.
        Denote random variable
        \[Z_i:=\begin{cases}
            W_{t_1},&i=1\\
            W_{t_i}-W_{t_{i-1}},&i>1
        \end{cases}\]
        By the independent increments of Brownian motion, we
        obtain $Z_i,i=1,2,\ldots,N$ are mutually independent.
        Then the joint distribution of $(Z_1,Z_2,\ldots,Z_N)$
        is clear and simple -- we have the JPDF
        \begin{equation*}
            p(z_1,z_2,\ldots,z_N)=
            \frac{\e^{-\frac{1}{2}\left(
                \frac{z_1^2}{t_1}
                +\frac{z_2^2}{t_2-t_1}
                +\cdots+\frac{z_N^2}{t_N-t_{N-1}}
            \right)}}{\sqrt{(2\pi)^Nt_1(t_2-t_1)\cdots(t_N-t_{N-1})}}
        \end{equation*}

        For the event $\{\omega;a_i\leq W_{t_i}(\omega)\leq b_i\}$,
        it is equivalent to
        \[\left\{\omega;a_i\leq \sum_{k=1}^iZ_k(\omega)\leq b_i\right\}\]
        It follows that
        \begin{equation}
            \label{eq:p4 mult-integral}
            \begin{aligned}
            &P(\omega;a_i\leq W_{t_i}\leq b_i,i=1,2,\ldots,N)\\
            =&P\left(\omega;a_i\leq \sum_{k=1}^iZ_k(\omega)\leq b_i,i=1,2,\ldots,N\right)\\
            =&\int_{\left\{z\in\mathbb R^N;a_i\leq\sum_{k=1}^iz_i\leq b_i,i=1,2,\ldots,N\right\}}
              p(z_1,z_2,\ldots,z_N)
              \diff z_1\diff z_2\cdots\diff z_N
            \end{aligned}
        \end{equation}
        Then by variable subtitution of the following,
        \begin{align*}
            z_1&=w_1,\\
            z_i&=w_i-w_{i-1},i=2,3,\ldots,N\\
        \end{align*}
        we have the Jacobian determinant
        \[\mathcal J=
        \frac{\partial(z_1,z_2,\ldots,z_N)}{\partial(w_1,w_2,\ldots,w_N)}
        =\det\begin{pmatrix}
            1  &        &        &       \\
            -1 & 1      &        &       \\
               & \ddots & \ddots &       \\
               &        & -1     & 1     \\
        \end{pmatrix}=1\]
        thus \cref{eq:p4 mult-integral} becomes
        \begin{align*}
            &P(\omega;a_i\leq W_{t_i}\leq b_i,i=1,2,\ldots,N)\\
            =&\int_{\prod_{i=1}^N[a_i,b_i]}
              p(w_1,w_2-w_1,\ldots,w_N-w_{N-1})
              |\mathcal J|
              \diff w_1\diff w_2\cdots\diff w_N\\
            =&\int_{\prod_{i=1}^N[a_i,b_i]}
               \frac{\e^{-\frac{1}{2}\left(
                   \frac{w_1^2}{t_1} 
                   +\frac{(w_2-w_1)^2}{t_2-t_1}
                   +\cdots
                   +\frac{(w_N-w_{N-1})^2}{t_N-t_{N-1}}
               \right)}}{\sqrt{(2\pi)^Nt_1(t_2-t_1)\cdots(t_N-t_{N-1})}}
              \diff w_1\diff w_2\cdots\diff w_N\\
        \end{align*}

        \item
        As $N\to\infty$, there are infinitely many $W_{t_i}\in[a,b]$,
        which should be rarely happening. So in conclusion,
        \[\lim_{N\to\infty}P\left(
            \omega;
            a_i\leq W_{t_i}\leq b_i,
            \forall i=1,2,\ldots,N
        \right)=0\]
        
        \item
        As \sidenote{Since the work of calculation increases rapidly
        as $N$ increases, only a few $N$s is chosen.}
        \cref{fig:integral}.
        \begin{marginfigure}
            \centering
            \includegraphics[width=\columnwidth]{integral}
            \caption{Approximation as $N\to\infty$}
            \label{fig:integral}
        \end{marginfigure}
    \end{subproblem}

    \problem
    \subsection{$X_t$ Is Not a Martingale}
    \begin{proof}
        Just verify equation
        \[E(X_{t+s}|\mathcal F_t)=X_t\]
        does not hold for some $s\leq 0$.

        Consider the moments generating function of $W_t$
        \begin{equation}
            \label{eq:mgf}
            \begin{aligned}
            E\left(\e^{aW_t}\right)&=\int_{-\infty}^\infty
            \e^{ax}
            \frac{\e^{-\frac{x^2}{2t}}}{\sqrt{2\pi t}}
            \diff x\\
            &=\int_{-\infty}^\infty
              \e^{-\frac{(x-at)^2}{2t}+\frac{a^2t}{2}}\diff x\\
            &=\e^{\frac{a^2t}{2}}\int_{-\infty}^\infty
              \frac{\e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}}
              \diff\xi\\
            &=\e^{\frac{a^2t}{2}}
            \end{aligned}
        \end{equation}
        Thus
        \[\begin{aligned}
            E(X_{t+s}|\mathcal F_t)&=E(\e^{W_{t+s}-W_t}\cdot\e^{W_t}|\mathcal F_t)\\
            &=\e^{W_t}E(\e^{W_{t+s}-W_t}|\mathcal F_t)
        \end{aligned}\]
        Since Brownian motion has independent increments, we have that
        \[E\left(\left.\e^{W_{t+s}-W_t}\right|\mathcal F_t\right)
        =E\left(\e^{W_{t+s}-W_t}\right)
        =\e^{\frac{s}{2}}\]
        Hence we obtain,
        \[E(X_{t+s}|\mathcal F_t)=\e^{\frac{s}{2}}X_t\neq X_t\]
        for any $s>0$. So $\{X_t\}_{t\geq 0}$ is not a martingale.

    \end{proof}

    \subsection{$Y_t$ Is a Martingale}
    \begin{proof}
        Since $Y_t>0$, we have that $Y_t$ is integrable is equivalent
        to the existence of $E(Y_t)$, which will be proved during
        its calculation. And $W_t$ being adapted to $\mathcal F_t$ yields
        $Y_t=\e^{aW_t-\frac{a^2t}{2}}$ adapted too.

        So let's start from verifying the conditional expectation,
        \[\begin{aligned}
            E(Y_{t+s}|\mathcal F_t)
            &=E\left(\left.\e^{a(W_{t+s}-W_t)+aW_t-\frac{a^2(t+s)}{2}}\right|\mathcal F_t\right)\\
            &=\e^{aW_t-\frac{a^2(t+s)}{2}}E\left(\left.\e^{a(W_{t+s}-W_t)}\right|\mathcal F_t\right)
        \end{aligned}\]

        Similarly, by \cref{eq:mgf} we have that
        \[E\left(\left.\e^{a(W_{t+s}-W_t)}\right|\mathcal F_t\right)
        =E\left(\e^{a(W_{t+s}-W_t)}\right)
        =\e^{\frac{a^2s}{2}}\]
        It follows that
        \[E(Y_{t+s}|\mathcal F_t)=\e^{aW_t-\frac{a^2t}{2}}=Y_t\]
        Then $\{Y_t\}_{t\geq 0}$ is a martingale.
    \end{proof}

    \subsection{About $\Cov(X_s,X_t)$}
    By covariance formula
    \[\Cov(X,Y)=E(XY)-EX\cdot EY\]
    we only need to concentrate on $E(X_tX_s)$.

    Without loss of generality, we assume $t>s$.
    Consider conditional expectation
    \[\begin{aligned}
        E(X_tX_s|\mathcal F_s)&=\e^{2W_s}E\left(\left.\e^{W_t-W_s}\right|\mathcal F_s\right)\\
        &=\e^{2W_s}E\left(\e^{W_t-W_s}\right)\\
        &=\e^{2W_s+\frac{t-s}{2}}
    \end{aligned}\]
    Then by total expectation formula, together with
    \cref{eq:mgf}, we obtain,
    \[\begin{aligned}
        E(X_tX_s)&=E(E(X_tX_s|\mathcal F_s))\\
                 &=E\left(\e^{2W_s+\frac{t-s}{2}}\right)\\
                 &=\e^{\frac{t-s}{2}}\e^{\frac{2^2s}{2}}\\
                 &=\e^{\frac{t+3s}{2}}
    \end{aligned}\]
    It follows that
    \[\Cov(X_t,X_s)=\e^{\frac{t+3s}{2}}
    -\e^{\frac{t}{2}}\cdot\e^{\frac{s}{2}}
    =\e^{\frac{t+s}{2}}(\e^s-1)\]
    i.e., in general
    \[\Cov(X_t,X_s)=\e^{\frac{t+s}{2}}\left(\e^{\min\{t,s\}}-1\right)\]

    \problem
    \begin{subproblem}
        \item
        \begin{proof}
            Since $\phi$ is a convex function, Jensen's inequality
            also holds for conditional expectation,
            \begin{equation}
                \label{eq:Jensen}
                E(\phi(X_{t+s})|\mathcal F_t)
                \geq\phi(E(X_{t+s}|\mathcal F_t))
            \end{equation}
            So
            \[E(\phi(X_{t+s}|\mathcal F_t))\geq\phi(X_t)\]
            as $\{X_t\}_{t\geq 0}$ is a martingale w.r.t $\{\mathcal F_t\}$.

            While the integrability of $\phi(X_t)$ implies the existence of
            $E|\phi(X_t)|$, $\phi(X_t)$ is adapted as $X_t$ is adapted.
            Therefore, $\{\phi(X_t)\}_{t\geq 0}$ is a submartingale.
        \end{proof}

        \item
        \begin{proof}
            Since $\phi$ is increasing and $\{X_t\}_{t\geq 0}$ is a
            submartingale, we have that
            \[\phi(E(X_{t+s}|\mathcal F_t))\geq\phi(X_t)\]
            With \cref{eq:Jensen}, it yields
            \[\phi(E(X_{t+s}|\mathcal F_t))\geq\phi(X_t)\]
            Similarly, it follows that $\{\phi(X_t)\}_{t\geq 0}$
            is a submartingale.
        \end{proof}
    \end{subproblem}

    \problem
    \begin{proof}
        For any $s>0$ we have that
        \newcommand{\conde}[1]{E\left(\left. #1 \right|\mathcal F_t\right)}
        \[\begin{aligned}
            \conde{\e^{\sigma W_{t+s}}}
            &=\e^{\mu(t+s)+\sigma W_t}\conde{\e^{\sigma(W_{t+s}-W_t)}}\\
            &=\e^{\mu(t+s)+\sigma W_t}E\left(\e^{\sigma(W_{t+s}-W_t)}\right)
        \end{aligned}\]
        as Brownian motion has independent increments.

        By \cref{eq:mgf} we obtain the expectation as
        \[\begin{aligned}
            \conde{\e^{\sigma W_{t+s}}}
            &=\e^{\mu(t+s)+\sigma W_t}\cdot\e^{\frac{\sigma^2s}{2}}\\
            &=\e^{\left(\mu+\frac{\sigma^2}{2}\right)s}\cdot\e^{\sigma W_t+\mu t}\\
            &\geq\e^{\sigma W_t+\mu t}
        \end{aligned}\]
        since $\mu>0$.

        The moment generating function guarantees the integrability,
        and $\e^{\sigma W_t+\mu t}$ is adapted to $\{\mathcal F_t\}$.
        Therefore $X_t$ is a submartingale.
    \end{proof}
\end{document}