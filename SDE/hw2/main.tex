\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Corr}{\mathrm{Corr}}

    \problem
    \begin{question}
        Find Cov$(B_t,B_s)$ and  Corr$(B_t,B_s)$ for the Brownian motion
        $B_t$ and $\forall s,t\in\mathbb R$;
    \end{question}

    Without loss of generality, assume that $t\geq s$.
    Then by the law of distributivity,
    \begin{align*}
        \Cov(B_t,B_s)&=\Cov(B_t-B_s+B_s,B_s)\\
                     &=\Cov(B_t-B_s,B_s)+\Cov(B_s,B_s)
    \end{align*} 
    Since Brownian motion has independent increments,
    we have that
    \[\Cov(B_t-B_s,B_s)=0\]
    Therefore
    \[\Cov(B_t,B_s)=\Var(B_s)=s\]
    i.e.,
    \[\Cov(B_t,B_s)=\min\{t,s\}\]
    in general.

    Hence
    \[\Corr(B_t,B_s)=\frac{\Cov(B_t,B_s)}{\sqrt{\Var(B_t)\Var(B_s)}}
    =\frac{\min\{t,s\}}{\sqrt{ts}}\]

    \problem
    \begin{question}
        We already know that both $B_t$ and $B^2_t-t$ are martingales. 
        Now use the martingale's properties to find the followings for any $s,t\geq0$

        \noindent i)  Find E$((B^2_t-t)(B^2_s-s))$;

        \noindent ii)  Find E$(B^2_tB^2_s)$;

        \noindent iii)  Find Cov$(B^2_t,B^2_s)$.
    \end{question}

    Note that $E\left(B_t^2\right)=t$, so by definition of
    covariance,
    \begin{equation}
        \label{eq:cov def}
        E\left((B_t^2-t)(B_s^2-s)\right)=\Cov(B_t^2,B_s^2)
    \end{equation}
    Also note that
    \begin{equation}
        \label{eq:cov formula}
        \Cov(B_t^2,B_s^2)=E(B_t^2B_s^2)-E(B_t^2)\cdot E(B_s^2)
    \end{equation}

    So the key to these problems is calculating $E(B_t^2B_s^2)$,
    which can be obtained by law of total expectation. Therefore,
    we can start from $E(B_t^2B_s^2|B_s)$, and without
    loss of generality, we assume that $s\leq t$.

    We have that
    \begin{align*}
        E\left.\left(B_t^2B_s^2\right|B_s\right)
        &=B_s^2E\left.\left(B_t^2\right|B_s\right)\\
    \end{align*}
    and
    \begin{align*}
        E\left.\left(B_t^2\right|B_s\right)
        &=E\left.\left((B_t-B_s+B_s)^2\right|B_s\right)\\
        &=E\left((B_t-B_s)^2|B_s\right)
            +2E\big(B_s(B_t-B_s)|B_s\big)
            +E\left.\left(B_s^2\right|B_s\right)
    \end{align*}
    Since Brownian motion has independent increments, we have
    that
    \begin{align*}
    E\left.\left((B_t-B_s)^2\right|B_s\right)
    &=E\left((B_t-B_s)^2\right)=t-s\\
    E\left.\left(B_s(B_t-B_s)\right|B_s\right)&=B_sE(B_t-B_s)=0\\
    E\left.\left(B_s^2\right|B_s\right)&=B_s^2
    \end{align*}
    Hence
    \begin{align*}
        E\left.\left(B_t^2B_s^2\right|B_s\right)
        &=B_s^2\cdot E\left.\left(B_t^2\right|B_s\right)\\
        &=B_s^2\left(t-s+B_s^2\right)
    \end{align*} 

    Therefore
    \begin{align*}
    E\left(B_t^2B_s^2\right)&=E\left(E\left.\left(B_t^2B_s^2\right|B_s\right)\right)\\
    &=E\left(B_s^2\left(t-s+B_s^2\right)\right)\\
    &=(t-s)E\left(B_s^2\right)+E\left(B_s^4\right)
    \end{align*}


    As for $E\left(B_s^4\right)$, since that $\left(B_s/\sqrt{s}\right)^2
    \sim\chi^2(1)$, we obtain,
    \[\Var(B_s^2)=2s^2\]
    thus
    \begin{align*}
        E\left(B_s^4\right)&=\Var\left(B_s^2\right)+\left(E\left(B_s^2\right)\right)^2=3s^2
    \end{align*}
    It follows that
    \begin{align*}
    E\left(B_t^2B_s^2\right)=s(t+2s)
    \end{align*}
    so in general by symmetry,
    \begin{equation}
        \label{eq:EBs2Bt2}
        E\left(B_t^2B_s^2\right)=ts+2\min\{t^2,s^2\}
    \end{equation}

    \begin{subproblem}
        \item\label{pb:2.1}
        From \cref{eq:cov def,eq:cov formula} we know that
        \[E\left((B_t^2-t)(B_s^2-s)\right)=ts+2\min\{t^2,s^2\}-ts=2\min\{t^2,s^2\}\]

        \item
        As in \cref{eq:EBs2Bt2}
        \[E\left(B_t^2B_s^2\right)=ts+2\min\{t^2,s^2\}\]

        \item
        Same as \ref{pb:2.1} as shown by \cref{eq:cov def},
        \[\Cov(B_t^2,B_s^2)=2\min\{t^2,s^2\}\]
    \end{subproblem}

    \problem
    \begin{question}
        This problem is designated to give you a careful study of random walk
        transferred into Brownian motion (distribution).  Assume that a particle
        starting with the origin follows a random walk or jump on the 1D grid 
        \[...,-3\Delta x, -2\Delta x, -\Delta x, 0,\Delta x,2\Delta x,3\Delta x,...\]
        with a unit size $\Delta x$ over a unit time $\Delta t$ such that 
        the probability it moves to left and to the right equals 1.

        \begin{figure}[h!]
        \centering
        \includegraphics[width=\textwidth]{grid.png}\\
        \end{figure}
        
        Here the randomness is ascertained by specification of
        the 50\% probability of each jump to either direction. 
        The illustration above carries not only for the origin but also any location.

        % % TODO Fix figure
        % \begin{tikzpicture}
        % \draw (-5,0) -- (5,0) node[anchor=north west] {$x$};
        % \draw[thick,->] (-5,0) -- (5,0)
        %  node[pos=0.25]{0}  node[pos=0.5]{$x$} node[pos=0.9]{9/10};
        %   \draw[thick, ->] (0,0.2) arc (0:185:1);
        %   \draw[thick, ->] (0,0.2) arc (1:185:2);
        % \end{tikzpicture}

        Let us introduce the collection of a binary i.i.d. distribution
        $\{X_i(\omega)\}_{i=1}^n$ such that
        $P(\omega;X_i(\omega)=0)=P(\omega;X_i(\omega)=1)=\frac{1}{2}$.  Denote
        \[S_n:=\sum_{i=1}^n X_i.\]
        Then it is easy to see that $S_n$ counts how many steps
        the particle has moved/jumped to the right by time $t=n\Delta t$.
         Let $Y_t$ be the position of the particle at time $t$. 
         Prove that $\forall a\leq b\in\mathbb R$
        \[\lim_{n\rightarrow \infty, t=n\Delta t}P\big(a\leq Y_t\leq b\big)
        =\frac{1}{\sqrt{2\pi Dt}}\int_a^b e^{-\frac{x^2}{2Dt}}dx,\]
        where we denote $D:=\frac{(\Delta x)^2}{\Delta t}$. 
        Hint: first write $Y_t$ in terms of $S_n$, then use Laplace-De
        Moivre Theorem to prove this limit.  Now it is not hard to see
        that $Y_t$ the limit of $n\rightarrow\infty$ is N$(0,Dt)$ distributed,
        and this readily connects the discrete random walk with the continuous
        Brownian motion.
    \end{question}

    \begin{proof}
        Since $2X_i-1$ represents the movements of every step,
        we have that
        \[Y_n=\Delta x\cdot\left(\sum_{i=1}2X_i-1\right)
        =\Delta x(2S_n-n)\]
        Then by Laplace-De Moivre Theorem,
        \[\frac{S_n-\frac{n}{2}}{\sqrt{\frac{n}{4}}}\to N(0,1)\]
        i.e.,
        \[\frac{Y_n}{\sqrt{n}\Delta x}\to N(0,1)\]
        Since $D=(\Delta x)^2/\Delta t$ and $t=n\Delta t$,
        we have that
        \[\frac{Y_n}{\sqrt{n}\Delta x}=\frac{Y_n}{\sqrt{Dt}}\]
        And $n\to\infty$ is equivalent $t\to\infty$, therefore
        \[\frac{Y_t}{\sqrt{Dt}}\to N(0,1)\]
        It follows that
        \begin{align*}
        \lim_{n\to\infty,t=n\Delta t}P(a\leq Y_t\leq b)
        &=\frac{1}{\sqrt{2\pi}}
        \int_{a/\sqrt{Dt}}^{b/\sqrt{Dt}}\e^{-\frac{\xi^2}{2}}\diff\xi\\
        &=\frac{1}{\sqrt{2\pi Dt}}
        \int_a^b\e^{-\frac{x^2}{2Dt}}\diff x
        \end{align*}
    \end{proof}

    \problem
    \begin{question}
        We know that the probability of a Brownian motion particle $W_t$
        stays in interval $(a,b)$ at time $t$ is
        \[P(a\leq W_t\leq b)=\frac{1}{\sqrt{2\pi t}}\int_a^be^{-\frac{x^2}{2t}}dx.\]

        % \begin{enumerate}[label=\roman*),itemindent=\labelwidth+\labelsep,leftmargin=0pt]
            \noindent i)
            find the probability $P(a_1\leq W_{t_1}\leq b_1,a_2\leq W_{t_2}\leq b_2)$;
            then try to extend this to  $P(a_i\leq W_{t_i}\leq b_i,\text{for all~} i=1,2,..N)$
            for a collection of time grids $0<t_1,<t_2<...<t_n$. Hint: check what happens 
            if a Brownian motion does not starts at the origin.

            \noindent ii)
            suppose that $(a_i,b_i)\equiv (a,b)$ for each $i$ with $-\infty<a<b<\infty$.
            Use your intuition to guess what is the limit $\lim_{N\rightarrow \infty}
            P(a_i\leq W_{t_i}\leq b_i,\text{for all~} i=1,2,..N)$;

            \noindent iii)
            now set $(a_i,b_i)\equiv (-L,L)$ and $t_n=n\Delta t$.  You can use
            a computer to calculate the probability $P_N:=P(a_i\leq W_{t_i}\leq b_i,
            \text{for all~} i=1,2,..N)$ by evaluating the integral.  Let $L=1$ and 
            $\Delta t=1$, find/plot $P_N$ for $N=1,2,...$.  Do the same by choosing 
            your own $L$ and $\Delta t$.  The results should be matching your intuition.
        % \end{enumerate}
    \end{question}

    \begin{subproblem}[\roman*)]
        \item
        Let's start from the high dimension directly.
        Denote random variable
        \[Z_i:=\begin{cases}
            W_{t_1},&i=1\\
            W_{t_i}-W_{t_{i-1}},&i>1
        \end{cases}\]
        By the independent increments of Brownian motion, we
        obtain $Z_i,i=1,2,\ldots,N$ are mutually independent.
        Then the joint distribution of $(Z_1,Z_2,\ldots,Z_N)$
        is clear and simple -- we have the JPDF
        \begin{equation*}
            p(z_1,z_2,\ldots,z_N)=
            \frac{\e^{-\frac{1}{2}\left(
                \frac{z_1^2}{t_1}
                +\frac{z_2^2}{t_2-t_1}
                +\cdots+\frac{z_N^2}{t_N-t_{N-1}}
            \right)}}{\sqrt{(2\pi)^Nt_1(t_2-t_1)\cdots(t_N-t_{N-1})}}
        \end{equation*}

        For the event $\{\omega;a_i\leq W_{t_i}(\omega)\leq b_i\}$,
        it is equivalent to
        \[\left\{\omega;a_i\leq \sum_{k=1}^iZ_k(\omega)\leq b_i\right\}\]
        It follows that
        \begin{equation}
            \label{eq:p4 mult-integral}
            \begin{aligned}
            &P(\omega;a_i\leq W_{t_i}\leq b_i,i=1,2,\ldots,N)\\
            =&P\left(\omega;a_i\leq \sum_{k=1}^iZ_k(\omega)\leq b_i,i=1,2,\ldots,N\right)\\
            =&\int_{\left\{z\in\mathbb R^N;a_i\leq\sum_{k=1}^iz_i\leq b_i,i=1,2,\ldots,N\right\}}
              p(z_1,z_2,\ldots,z_N)
              \diff z_1\diff z_2\cdots\diff z_N
            \end{aligned}
        \end{equation}
        Then by variable subtitution of the following,
        \begin{align*}
            z_1&=w_1,\\
            z_i&=w_i-w_{i-1},i=2,3,\ldots,N\\
        \end{align*}
        we have the Jacobian determinant
        \[\mathcal J=
        \frac{\partial(z_1,z_2,\ldots,z_N)}{\partial(w_1,w_2,\ldots,w_N)}
        =\det\begin{pmatrix}
            1  &        &        &       \\
            -1 & 1      &        &       \\
               & \ddots & \ddots &       \\
               &        & -1     & 1     \\
        \end{pmatrix}=1\]
        thus \cref{eq:p4 mult-integral} becomes
        \begin{align*}
            &P(\omega;a_i\leq W_{t_i}\leq b_i,i=1,2,\ldots,N)\\
            =&\int_{\prod_{i=1}^N[a_i,b_i]}
              p(w_1,w_2-w_1,\ldots,w_N-w_{N-1})
              |\mathcal J|
              \diff w_1\diff w_2\cdots\diff w_N\\
            =&\int_{\prod_{i=1}^N[a_i,b_i]}
               \frac{\e^{-\frac{1}{2}\left(
                   \frac{w_1^2}{t_1} 
                   +\frac{(w_2-w_1)^2}{t_2-t_1}
                   +\cdots
                   +\frac{(w_N-w_{N-1})^2}{t_N-t_{N-1}}
               \right)}}{\sqrt{(2\pi)^Nt_1(t_2-t_1)\cdots(t_N-t_{N-1})}}
              \diff w_1\diff w_2\cdots\diff w_N\\
        \end{align*}

        \item
        As $N\to\infty$, there are infinitely many $W_{t_i}\in[a,b]$,
        which should be rarely happening. So in conclusion,
        \[\lim_{N\to\infty}P\left(
            \omega;
            a_i\leq W_{t_i}\leq b_i,
            \forall i=1,2,\ldots,N
        \right)=0\]
        
        \item
        As \sidenote{Since the work of calculation increases rapidly
        as $N$ increases, only a few $N$s is chosen.}
        \cref{fig:integral}.
        \begin{marginfigure}
            \centering
            \includegraphics[width=\columnwidth]{integral}
            \caption{Approximation as $N\to\infty$}
            \label{fig:integral}
        \end{marginfigure}
    \end{subproblem}

    \problem
    \begin{question}
        Consider $X_t:=e^{W_t}$.  This is called the Geometric
        Brownian Motion or GBM for short.  Show that $X_t$
        is not a martingale, however $e^{-\frac{t}{2}}X_t$
        is a martingale.  For the latter case, you can actually
        show in general that $Y_t=e^{aW_t-\frac{a^2t}{2}}$ is
        a martingale for any constant $a\in\mathbb R$.  Now use
        the martingale property to find Cov$(X_s,X_t)$. 
        Remark:  I want to emphasize that in almost all
        text books or lecture notes, you may find statement
        like \emph{a geometric Brownian motion is a martingale}
        with respect to... blabla...However there is
        no contradiction to the fact that $X_t$ is not 
        a martingale because it is $X_t=\exp\Big\{\Big(\mu-\frac{\sigma^2}{2}\Big)t
        +\sigma W_t\Big\}$ how they define a GBM.  And then their statement is
        that a GBM is a martingale when the drift parameter is 0, which is 
        exactly our general case above with $a=\sigma$.  It seems necessary 
        to mention that $X_t$ is log normally distributed, and I urge you to 
        evaluate its pdf yourself. 
    \end{question}

    \subsection{$X_t$ Is Not a Martingale}
    \begin{proof}
        Just verify equation
        \[E(X_{t+s}|\mathcal F_t)=X_t\]
        does not hold for some $s\leq 0$.

        Consider the moments generating function of $W_t$
        \begin{equation}
            \label{eq:mgf}
            \begin{aligned}
            E\left(\e^{aW_t}\right)&=\int_{-\infty}^\infty
            \e^{ax}
            \frac{\e^{-\frac{x^2}{2t}}}{\sqrt{2\pi t}}
            \diff x\\
            &=\int_{-\infty}^\infty
              \e^{-\frac{(x-at)^2}{2t}+\frac{a^2t}{2}}\diff x\\
            &=\e^{\frac{a^2t}{2}}\int_{-\infty}^\infty
              \frac{\e^{-\frac{\xi^2}{2}}}{\sqrt{2\pi}}
              \diff\xi\\
            &=\e^{\frac{a^2t}{2}}
            \end{aligned}
        \end{equation}
        Thus
        \[\begin{aligned}
            E(X_{t+s}|\mathcal F_t)&=E(\e^{W_{t+s}-W_t}\cdot\e^{W_t}|\mathcal F_t)\\
            &=\e^{W_t}E(\e^{W_{t+s}-W_t}|\mathcal F_t)
        \end{aligned}\]
        Since Brownian motion has independent increments, we have that
        \[E\left(\left.\e^{W_{t+s}-W_t}\right|\mathcal F_t\right)
        =E\left(\e^{W_{t+s}-W_t}\right)
        =\e^{\frac{s}{2}}\]
        Hence we obtain,
        \[E(X_{t+s}|\mathcal F_t)=\e^{\frac{s}{2}}X_t\neq X_t\]
        for any $s>0$. So $\{X_t\}_{t\geq 0}$ is not a martingale.

    \end{proof}

    \subsection{$Y_t$ Is a Martingale}
    \begin{proof}
        Since $Y_t>0$, we have that $Y_t$ is integrable is equivalent
        to the existence of $E(Y_t)$, which will be proved during
        its calculation. And $W_t$ being adapted to $\mathcal F_t$ yields
        $Y_t=\e^{aW_t-\frac{a^2t}{2}}$ adapted too,
        as $f(x)=\e^{ax-\frac{s^2t}{2}}$ is continuous.
        \sidenote{Since $f^{-1}(G)$ is open if $f$ is continuous and
        $G$ is open, that being said, $f(W_t)$ in open set implies
        $W_t$ in open set too, hence $W_t$ in $\mathcal F_s,s\leq t$.
        It follows that $f(W_t)$ is adapted to filtration $(\mathcal F_t)$}

        So let's start from verifying the conditional expectation,
        \[\begin{aligned}
            E(Y_{t+s}|\mathcal F_t)
            &=E\left(\left.\e^{a(W_{t+s}-W_t)+aW_t-\frac{a^2(t+s)}{2}}\right|\mathcal F_t\right)\\
            &=\e^{aW_t-\frac{a^2(t+s)}{2}}E\left(\left.\e^{a(W_{t+s}-W_t)}\right|\mathcal F_t\right)
        \end{aligned}\]

        Similarly, by \cref{eq:mgf} we have that
        \[E\left(\left.\e^{a(W_{t+s}-W_t)}\right|\mathcal F_t\right)
        =E\left(\e^{a(W_{t+s}-W_t)}\right)
        =\e^{\frac{a^2s}{2}}\]
        It follows that
        \[E(Y_{t+s}|\mathcal F_t)=\e^{aW_t-\frac{a^2t}{2}}=Y_t\]
        Then $\{Y_t\}_{t\geq 0}$ is a martingale.
    \end{proof}

    \subsection{About $\Cov(X_s,X_t)$}
    By covariance formula
    \[\Cov(X,Y)=E(XY)-EX\cdot EY\]
    we only need to concentrate on $E(X_tX_s)$.

    Without loss of generality, we assume $t>s$.
    Consider conditional expectation
    \[\begin{aligned}
        E(X_tX_s|W_s)&=\e^{2W_s}E\left(\left.\e^{W_t-W_s}\right|W_s\right)\\
        &=\e^{2W_s}E\left(\e^{W_t-W_s}\right)\\
        &=\e^{2W_s+\frac{t-s}{2}}
    \end{aligned}\]
    Then by total expectation formula, together with
    \cref{eq:mgf}, we obtain,
    \[\begin{aligned}
        E(X_tX_s)&=E(E(X_tX_s|W_s))\\
                 &=E\left(\e^{2W_s+\frac{t-s}{2}}\right)\\
                 &=\e^{\frac{t-s}{2}}\e^{\frac{2^2s}{2}}\\
                 &=\e^{\frac{t+3s}{2}}
    \end{aligned}\]
    It follows that
    \[\Cov(X_t,X_s)=\e^{\frac{t+3s}{2}}
    -\e^{\frac{t}{2}}\cdot\e^{\frac{s}{2}}
    =\e^{\frac{t+s}{2}}(\e^s-1)\]
    i.e., in general
    \[\Cov(X_t,X_s)=\e^{\frac{t+s}{2}}\left(\e^{\min\{t,s\}}-1\right)\]

    \problem
    \begin{question}
        Prove that:

        \noindent i) if $X_t$ is a martingale and $\phi$ a convex function 
        such that $\phi(X_t)$ is integrable, then $\phi(X_t)$ is a submartingale;

        \noindent ii) if $X_t$ is a submartingale and $\phi$ is an increasing convex 
        function such that $\phi(X_t)$ is integrable, then $\phi(X_t)$ is a submartingale.
    \end{question}

    \begin{subproblem}
        \item
        \begin{proof}
            Since $\phi$ is a convex function, Jensen's inequality
            also holds for conditional expectation,
            \begin{equation}
                \label{eq:Jensen}
                E(\phi(X_{t+s})|\mathcal F_t)
                \geq\phi(E(X_{t+s}|\mathcal F_t))
            \end{equation}
            So
            \[E(\phi(X_{t+s}|\mathcal F_t))\geq\phi(X_t)\]
            as $\{X_t\}_{t\geq 0}$ is a martingale w.r.t $\{\mathcal F_t\}$.

            While the integrability of $\phi(X_t)$ implies the existence of
            $E|\phi(X_t)|$, $\phi(X_t)$ is adapted as $X_t$ is adapted.
            Therefore, $\{\phi(X_t)\}_{t\geq 0}$ is a submartingale.
        \end{proof}

        \item
        \begin{proof}
            Since $\phi$ is increasing and $\{X_t\}_{t\geq 0}$ is a
            submartingale, we have that
            \[\phi(E(X_{t+s}|\mathcal F_t))\geq\phi(X_t)\]
            With \cref{eq:Jensen}, it yields
            \[\phi(E(X_{t+s}|\mathcal F_t))\geq\phi(X_t)\]
            Similarly, it follows that $\{\phi(X_t)\}_{t\geq 0}$
            is a submartingale.
        \end{proof}
    \end{subproblem}

    \problem
    \begin{question}
        Prove that $X_t=e^{\sigma W_t+\mu t}$ is a submartingale with $\mu>0$.
    \end{question}

    \begin{proof}
        For any $s>0$ we have that
        \newcommand{\conde}[1]{E\left(\left. #1 \right|\mathcal F_t\right)}
        \[\begin{aligned}
            \conde{\e^{\sigma W_{t+s}}}
            &=\e^{\mu(t+s)+\sigma W_t}\conde{\e^{\sigma(W_{t+s}-W_t)}}\\
            &=\e^{\mu(t+s)+\sigma W_t}E\left(\e^{\sigma(W_{t+s}-W_t)}\right)
        \end{aligned}\]
        as Brownian motion has independent increments.

        By \cref{eq:mgf} we obtain the expectation as
        \[\begin{aligned}
            \conde{\e^{\sigma W_{t+s}}}
            &=\e^{\mu(t+s)+\sigma W_t}\cdot\e^{\frac{\sigma^2s}{2}}\\
            &=\e^{\left(\mu+\frac{\sigma^2}{2}\right)s}\cdot\e^{\sigma W_t+\mu t}\\
            &\geq\e^{\sigma W_t+\mu t}
        \end{aligned}\]
        since $\mu>0$.

        The moment generating function guarantees the integrability,
        and $\e^{\sigma W_t+\mu t}$ is adapted to $\{\mathcal F_t\}$.
        Therefore $X_t$ is a submartingale.
    \end{proof}

    \problem
    \begin{question}
        Given a Brownian Motion, we are able to define or introduce many
        of its variants, based on the phenomenon that we want to model or 
        describe.  For example,
        \[X_t=W_t-tW_1, t\in[0,1]\]
        is called a Brownian Bridge, since $W_0=0$ and $W_1=1$ and $W_t$ is 
        like a bridge connecting these two points.

        \noindent i)  Show that $X_t$ is normally distributed and find its pdf;

        \noindent ii)  Find E$(X_t)$ and Var$(X_t)$;

        \noindent iii) Let $Y_t=X_t^2$.  Find E$(Y_t)$ and Var$(Y_t)$.
    \end{question}
    \begin{subproblem}
        \item
        \begin{proof}
            Since
            \[\begin{aligned}
                X_t&=W_t-t(W_1-W_t+W_t)\\
                   &=(1-t)W_t-t(W_1-W_t) 
            \end{aligned}\]
            we know that $X_t$ is the summation of two
            independent normally distributed random variables,
            as Brownian has independent increments. Also we know
            the linearity of normal distribution, hence
            \begin{equation*}
                \begin{aligned}
                X_t\sim N\left(0,(1-t)^2t+t^2(1-t)\right)
                \end{aligned}
            \end{equation*}
            i.e.,
            \begin{equation}
                \label{eq:normal dist}
                \begin{aligned}
                X_t\sim N\big(0,t(1-t)\big)
                \end{aligned}
            \end{equation}
            $X_t$ is normally distributed.
        \end{proof}

        Therefore, its pdf,
        \[p(x)=\frac{\e^{-\frac{x^2}{t(1-t)}}}{\sqrt{2\pi t(1-t)}}\]

        \item
        From \cref{eq:normal dist} we know that
        \begin{align*}
            E(X_t)&=0,\\
            \Var(X_t)&=t(1-t)
        \end{align*}

        \item
        Since
        \[\left(\frac{X_t}{\sqrt{t(1-t)}}\right)^2\sim\chi^2(1)\]
        we have that
        \begin{align*}
            E\left(\frac{Y_t}{t(1-t)}\right)&=1,\\
            \Var\left(\frac{Y_t}{t(1-t)}\right)&=2
        \end{align*}
        thus
        \begin{align*}
            E(Y_t)&=t(1-t),\\
            \Var(Y_t)&=2t^2(1-t)^2
        \end{align*}
    \end{subproblem}